{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e1710dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e34c01",
   "metadata": {},
   "source": [
    "# Generate the data splits\n",
    "\n",
    "In an ideal world, we would recieve the data pre-partitioned into train, validation, and test. However, this is not the case. Ideally, I would perform k-fold cross validation in order to ensure my model is robust, but I'm opting to go with the naive strategy here for the sake of time and reduced complexity.\n",
    "\n",
    "To start, lets choose our split percents. I think I will train on 80%, and validate on 20%. I'm opting not to create a test set here because I'm going to naively assume that the validation set I create will be representative of the data the model will be tested on/the model will see during deployment. This naive split will allow us to select a model, and then we can analyze the kNN retrievals on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6905bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/zack/datasets/geological_similarity'\n",
    "OUTPUT_DIR = '/home/zack/datasets/geological_similarity/'\n",
    "TRAIN_PERCENT = 0.8\n",
    "# If we are regenerating the dataset, make sure we only include our categories\n",
    "CLASSES = ['rhyolite', 'quartzite', 'marble', 'andesite', 'schist', 'gneiss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8718bb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set will be generated with 23998 samples. Validation set will be generated with 6000 samples.\n"
     ]
    }
   ],
   "source": [
    "train_file_list = []\n",
    "val_file_list = []\n",
    "for subdir in os.listdir(DATA_DIR):\n",
    "    if subdir not in CLASSES: continue\n",
    "    subdir_path = os.path.join(DATA_DIR, subdir)\n",
    "    # Make sure we don't open any weird hidden files\n",
    "    if not os.path.isdir(subdir_path) or subdir[0] == '.': continue\n",
    "    files = np.array([os.path.join(subdir_path, x) for x in os.listdir(subdir_path)])\n",
    "    # Randomly shuffle the file list (we don't know if the files are alphabetical/by order of difficulty)\n",
    "    np.random.shuffle(files)\n",
    "    # Compute the number of files from this subdir that will be placed in train\n",
    "    num_train_samples = int(len(files) * TRAIN_PERCENT)\n",
    "    # Generate the splits\n",
    "    train, val = list(files[:num_train_samples]), list(files[num_train_samples:])\n",
    "    train_file_list += train\n",
    "    val_file_list += val\n",
    "print(f'Training set will be generated with {len(train_file_list)} samples. Validation set will be generated with {len(val_file_list)} samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbddf7d4",
   "metadata": {},
   "source": [
    "It looks like there are only 29998 files in the dataset, so we are missing two images. This won't be an issue, but its worth pointing out. This was validated through the command line as well.\n",
    "\n",
    "\n",
    "```\n",
    "$ find /home/zack/datasets/geological_similarity -type f -name \"*.jpg\" | wc -l \n",
    "29998\n",
    "```\n",
    "\n",
    "Now that we have our splits, lets create a train and val subfolder that we will symlink all of our data to. This will require the least amount of coding for a Torch dataset, as it will meet the spec for an ImageFolder dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f54bb590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def symlink_file_list(file_list, data_dir, output_dir):\n",
    "    for file in tqdm(file_list, desc='Creating symlinks for split'):\n",
    "        output_path = file.replace(data_dir, output_dir)\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        os.symlink(file, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3ced2458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating symlinks for split: 100%|████████████████████████████████████████████████████████████████████████████████████| 23998/23998 [00:00<00:00, 41130.83it/s]\n",
      "Creating symlinks for split: 100%|██████████████████████████████████████████████████████████████████████████████████████| 6000/6000 [00:00<00:00, 41547.16it/s]\n"
     ]
    }
   ],
   "source": [
    "train_output_dir = os.path.join(OUTPUT_DIR, 'train')\n",
    "val_output_dir = os.path.join(OUTPUT_DIR, 'val')\n",
    "\n",
    "if os.path.exists(train_output_dir): shutil.rmtree(train_output_dir)\n",
    "if os.path.exists(val_output_dir):  shutil.rmtree(val_output_dir)\n",
    "\n",
    "symlink_file_list(train_file_list, DATA_DIR, train_output_dir)\n",
    "symlink_file_list(val_file_list, DATA_DIR, val_output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54492163",
   "metadata": {},
   "source": [
    "# Compute normalization statistics\n",
    "\n",
    "Having zero-mean, unit standard deviation data assists the learning process and speeds up convergence. In order to facilitate this, we need to compute the mean and standard deviation of the training dataset. Lets do that now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "15280a15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with shape torch.Size([18814432, 3])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch\n",
    "\n",
    "\n",
    "ds = ImageFolder(train_output_dir, transform=ToTensor())\n",
    "ds = torch.hstack([x.unsqueeze(1) for (x, y) in ds]).transpose(0, 1)\n",
    "ds = ds.view(ds.shape[0], 3, -1).transpose(1, 2).view(-1, 3)\n",
    "print(f'Dataset loaded with shape {ds.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8bbf025a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset mean: tensor([0.5083, 0.5198, 0.5196])\n",
      "Dataset std: tensor([0.1851, 0.1997, 0.2196])\n"
     ]
    }
   ],
   "source": [
    "print(f'Dataset mean: {ds.mean(dim=0)}')\n",
    "print(f'Dataset std: {ds.std(dim=0)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764a1234",
   "metadata": {},
   "source": [
    "# Continue to supervised_baseline notebook\n",
    "\n",
    "Excellent! Lets continue to train our model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
