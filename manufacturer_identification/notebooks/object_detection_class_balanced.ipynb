{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zack/anaconda3/envs/riverside/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.model_zoo import get_config_file, get_checkpoint_url\n",
    "from detectron2.engine import DefaultTrainer, hooks\n",
    "from detectron2.evaluation import COCOEvaluator\n",
    "from detectron2.modeling import GeneralizedRCNNWithTTA\n",
    "from detectron2.data.datasets.coco import register_coco_instances\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# The challenge not addressed by the baseline\n",
    "\n",
    "While we achieved what most would consider \"good\" performance in the baseline, there is something peculiar about the by-class performance. Some classes perform exceedingly well, but others (like the Supermarine) perform poorly in comparison. While a lot of this could be explained by poor data for a given class, which often requires either better annotations, more representative samples, or synthetic data to resolve. However, another obvious answer is the class distribution. Most real world problems are not class balanced (at least not the ones we actually care about). Datasets change and problems evolve at a rapid pace, introducing new categories to our target sets (known as an open-set problem). Given data is often scarce for new targets when they are first observed, this leads to a long-tailed class distribution (e.g. many samples for some classes, few samples for others). Without appropriate regularization techniques, classes in the tail of this distribution (few samples) often suffer in performance as the random sampling presents the head classes to the model more frequently. Lets see what the training dataset distribution looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most annotations in any given scene: 1\n"
     ]
    }
   ],
   "source": [
    "with open('/home/zack/datasets/manufacturer_identification/data/coco/train.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "category_id_to_name_map = {x['id']: x['name'] for x in train_data['categories']}\n",
    "category_counts = {x : 0 for x in category_id_to_name_map.values()}\n",
    "annotations_per_image_id = {x['id'] : 0 for x in train_data['images']}\n",
    "for annotation in train_data['annotations']:\n",
    "    cat_name = category_id_to_name_map[annotation['category_id']]\n",
    "    category_counts[cat_name] += 1\n",
    "    annotations_per_image_id[annotation['image_id']] += 1\n",
    "    \n",
    "category_counts = dict(sorted(category_counts.items(), key=lambda item: item[1]))\n",
    "print(f'Most annotations in any given scene: {max(annotations_per_image_id.values())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.barh(y=list(category_counts.keys()), width=category_counts.values())\n",
    "plt.title('Class Distribution')\n",
    "plt.ylabel('Class Label')\n",
    "plt.xlabel('Sample Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, the class distibtruion follows a long-tailed distribution which will effect our trained model. One way to overcome this is class-balanced sampling, so lets try that. To do that, we can compute the inverse class weights by inverting the sample counts, which can become our new sampling weights. Rare classes will be weighted more heavily than common classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## LVIS Repeat Factor Sampler\n",
    "\n",
    "Detectron2 provides a built-in `RepeatFactorTrainingSampler` that samples images base on the frequency of items in the dataset. However, this sampler contains added complexity to account for multiple objects in a single scene- something we conveniently don't have to deal with. So, instead of using the `RepeatFactorTrainingSampler` and having to worry about the stochastic rounding, lets write our own simpler `WeightedRandomSampler`. \n",
    "\n",
    "First, lets walk through how we will compute the weights of the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get a count for all of the categories\n",
    "sample_counts = np.array(list(category_counts.values()))\n",
    "# Compute the inverse frequency of all the categories (More frequent categories => Lower value)\n",
    "inverse_sample_counts = 1 / sample_counts\n",
    "# This is not necessary, but lets convert it into a probability distribution for visualization purposes\n",
    "inverse_class_prob = inverse_sample_counts / inverse_sample_counts.sum()\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "plt.barh(y=list(category_counts.keys()), width=inverse_class_prob)\n",
    "plt.title('Inverse Class Probability')\n",
    "plt.ylabel('Class Label')\n",
    "plt.xlabel('Class Probability')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom WeightedRandomSampler\n",
    "\n",
    "Now that we can invert the class probability distribution, lets implement it as a sampler to use during training. Let's implement a custom `WeightedRandomSampler` that computes the inverse class probabilities as shown above and scatters them to all of the samples in our training dataset. Then, each epoch, we can draw samples based on this \"weighting\", shuffle, then train on them.\n",
    "\n",
    "Ultimately, we hope to overcome the class imbalance and train a better predictor on **most** categories at the potential sacrifice of performance on the head categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data.samplers import RepeatFactorTrainingSampler\n",
    "from torch.utils.data.sampler import Sampler\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from detectron2.utils import comm\n",
    "import itertools\n",
    "\n",
    "\n",
    "class WeightedRandomSampler(Sampler):\n",
    "    \"\"\"\n",
    "    Similar to TrainingSampler, but a sample may appear more times than others based\n",
    "    on its categorical probability in the dataset. Since we don't want to do too much surgery, we\n",
    "    mimic the signature of RepeatFactorTrainingSampler, and will monkey patch this in.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, repeat_factors, *, shuffle=True, seed=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            repeat_factors (Tensor): a float vector, the repeat factor for each indice. When it's\n",
    "                full of ones, it is equivalent to ``TrainingSampler(len(repeat_factors), ...)``.\n",
    "            shuffle (bool): whether to shuffle the indices or not\n",
    "            seed (int): the initial seed of the shuffle. Must be the same\n",
    "                across all workers. If None, will use a random seed shared\n",
    "                among workers (require synchronization among all workers).\n",
    "        \"\"\"\n",
    "        self._shuffle = shuffle\n",
    "        if seed is None:\n",
    "            seed = comm.shared_random_seed()\n",
    "        self._seed = int(seed)\n",
    "\n",
    "        self._rank = comm.get_rank()\n",
    "        self._world_size = comm.get_world_size()\n",
    "\n",
    "        # Store the repeat factors for later use\n",
    "        self.repeat_factors = repeat_factors\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def repeat_factors_from_category_frequency(dataset_dicts, repeat_thresh):\n",
    "        \"\"\"\n",
    "        Compute the inverse category weights and scatter them across all samples in the dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset_dicts (list[dict]): annotations in Detectron2 dataset format.\n",
    "            repeat_thresh (float): This is not used\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor:\n",
    "                the i-th element is the repeat factor for the dataset image at index i.\n",
    "        \"\"\"\n",
    "        # 1. For each category c, compute the fraction of images that contain it: f(c)\n",
    "        category_freq = defaultdict(int)\n",
    "        for dataset_dict in dataset_dicts:  # For each image (without repeats)\n",
    "            cat_ids = {ann[\"category_id\"] for ann in dataset_dict[\"annotations\"]}\n",
    "            for cat_id in cat_ids:\n",
    "                category_freq[cat_id] += 1\n",
    "                \n",
    "                \n",
    "        inverse_category_freq = {k : 1 / v for k, v in category_freq.items()}\n",
    "        total_inverse_category_freq = sum(inverse_category_freq.values())\n",
    "        inverse_category_freq = {k : v / total_inverse_category_freq for k, v in inverse_category_freq.items()}\n",
    "        print(f'Weighting draws by {inverse_category_freq}')\n",
    "        # 3. For each image I, compute the image-level repeat factor:\n",
    "        #    r(I) = max_{c in I} r(c)\n",
    "        rep_factors = []\n",
    "        for dataset_dict in dataset_dicts:\n",
    "            cat_ids = {ann[\"category_id\"] for ann in dataset_dict[\"annotations\"]}\n",
    "            rep_factor = max({inverse_category_freq[cat_id] for cat_id in cat_ids})\n",
    "            rep_factors.append(rep_factor)\n",
    "\n",
    "        return torch.tensor(rep_factors, dtype=torch.float32)\n",
    "\n",
    "\n",
    "    def _get_epoch_indices(self, generator):\n",
    "        \"\"\"\n",
    "        Create a list of dataset indices (with repeats) to use for one epoch.\n",
    "\n",
    "        Args:\n",
    "            generator (torch.Generator): pseudo random number generator used for\n",
    "                stochastic rounding.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: list of dataset indices to use in one epoch. Each index\n",
    "                is repeated based on its calculated repeat factor.\n",
    "        \"\"\"\n",
    "        # Since repeat factors are fractional, we use stochastic rounding so\n",
    "        # that the target repeat factor is achieved in expectation over the\n",
    "        # course of training\n",
    "\n",
    "            \n",
    "        indices = torch.multinomial(self.repeat_factors, \n",
    "                                    len(self.repeat_factors), \n",
    "                                    True, \n",
    "                                    generator=None)\n",
    "        return indices.long()\n",
    "\n",
    "    def __iter__(self):\n",
    "        start = self._rank\n",
    "        yield from itertools.islice(self._infinite_indices(), start, None, self._world_size)\n",
    "\n",
    "    def _infinite_indices(self):\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(self._seed)\n",
    "        while True:\n",
    "            # Sample indices with repeats determined by stochastic rounding; each\n",
    "            # \"epoch\" may have a slightly different size due to the rounding.\n",
    "            indices = self._get_epoch_indices(g)\n",
    "            if self._shuffle:\n",
    "                randperm = torch.randperm(len(indices), generator=g)\n",
    "                yield from indices[randperm].tolist()\n",
    "            else:\n",
    "                yield from indices.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expose our Sampler to Detectron2\n",
    "\n",
    "Implementing the sampler was the easy part, now we need to expose it to the Detectron2 API so it can instantiate it at runtime. To do this, we simply need to overwrite some methods built-in to detectron2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data.catalog import DatasetCatalog, MetadataCatalog\n",
    "from detectron2.data.common import AspectRatioGroupedDataset, DatasetFromList, MapDataset, ToIterableDataset\n",
    "from detectron2.data.dataset_mapper import DatasetMapper\n",
    "from detectron2.data.build import get_detection_dataset_dicts, build_batch_data_loader\n",
    "from detectron2.config import configurable\n",
    "from detectron2.utils.logger import _log_api_usage\n",
    "import torch.utils.data as torchdata\n",
    "import logging\n",
    "\n",
    "def _train_loader_from_config(cfg, mapper=None, *, dataset=None, sampler=None):\n",
    "    if dataset is None:\n",
    "        dataset = get_detection_dataset_dicts(\n",
    "            cfg.DATASETS.TRAIN,\n",
    "            filter_empty=cfg.DATALOADER.FILTER_EMPTY_ANNOTATIONS,\n",
    "            min_keypoints=cfg.MODEL.ROI_KEYPOINT_HEAD.MIN_KEYPOINTS_PER_IMAGE\n",
    "            if cfg.MODEL.KEYPOINT_ON\n",
    "            else 0,\n",
    "            proposal_files=cfg.DATASETS.PROPOSAL_FILES_TRAIN if cfg.MODEL.LOAD_PROPOSALS else None,\n",
    "        )\n",
    "        _log_api_usage(\"dataset.\" + cfg.DATASETS.TRAIN[0])\n",
    "\n",
    "    if mapper is None:\n",
    "        mapper = DatasetMapper(cfg, True)\n",
    "\n",
    "    if sampler is None:\n",
    "        sampler_name = cfg.DATALOADER.SAMPLER_TRAIN\n",
    "        logger = logging.getLogger(__name__)\n",
    "        print(\"Using training sampler {}\".format(sampler_name))\n",
    "        logger.info(\"Using training sampler {}\".format(sampler_name))\n",
    "        if sampler_name == \"TrainingSampler\":\n",
    "            sampler = TrainingSampler(len(dataset))\n",
    "        elif sampler_name == \"RepeatFactorTrainingSampler\":\n",
    "            repeat_factors = RepeatFactorTrainingSampler.repeat_factors_from_category_frequency(\n",
    "                dataset, cfg.DATALOADER.REPEAT_THRESHOLD\n",
    "            )\n",
    "            sampler = RepeatFactorTrainingSampler(repeat_factors)\n",
    "        elif sampler_name == \"RandomSubsetTrainingSampler\":\n",
    "            sampler = RandomSubsetTrainingSampler(len(dataset), cfg.DATALOADER.RANDOM_SUBSET_RATIO)\n",
    "        elif sampler_name == \"WeightedRandomSampler\":\n",
    "            repeat_factors = WeightedRandomSampler.repeat_factors_from_category_frequency(\n",
    "                dataset, cfg.DATALOADER.REPEAT_THRESHOLD\n",
    "            )\n",
    "            sampler = WeightedRandomSampler(repeat_factors)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown training sampler: {}\".format(sampler_name))\n",
    "\n",
    "    return {\n",
    "        \"dataset\": dataset,\n",
    "        \"sampler\": sampler,\n",
    "        \"mapper\": mapper,\n",
    "        \"total_batch_size\": cfg.SOLVER.IMS_PER_BATCH,\n",
    "        \"aspect_ratio_grouping\": cfg.DATALOADER.ASPECT_RATIO_GROUPING,\n",
    "        \"num_workers\": cfg.DATALOADER.NUM_WORKERS,\n",
    "    }\n",
    "\n",
    "@configurable(from_config=_train_loader_from_config)\n",
    "def build_our_custom_detection_train_loader(\n",
    "    dataset,\n",
    "    *,\n",
    "    mapper,\n",
    "    sampler=None,\n",
    "    total_batch_size,\n",
    "    aspect_ratio_grouping=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Build a dataloader for object detection with some default features.\n",
    "    Args:\n",
    "        dataset (list or torch.utils.data.Dataset): a list of dataset dicts,\n",
    "            or a pytorch dataset (either map-style or iterable). It can be obtained\n",
    "            by using :func:`DatasetCatalog.get` or :func:`get_detection_dataset_dicts`.\n",
    "        mapper (callable): a callable which takes a sample (dict) from dataset and\n",
    "            returns the format to be consumed by the model.\n",
    "            When using cfg, the default choice is ``DatasetMapper(cfg, is_train=True)``.\n",
    "        sampler (torch.utils.data.sampler.Sampler or None): a sampler that produces\n",
    "            indices to be applied on ``dataset``.\n",
    "            If ``dataset`` is map-style, the default sampler is a :class:`TrainingSampler`,\n",
    "            which coordinates an infinite random shuffle sequence across all workers.\n",
    "            Sampler must be None if ``dataset`` is iterable.\n",
    "        total_batch_size (int): total batch size across all workers.\n",
    "        aspect_ratio_grouping (bool): whether to group images with similar\n",
    "            aspect ratio for efficiency. When enabled, it requires each\n",
    "            element in dataset be a dict with keys \"width\" and \"height\".\n",
    "        num_workers (int): number of parallel data loading workers\n",
    "        collate_fn: a function that determines how to do batching, same as the argument of\n",
    "            `torch.utils.data.DataLoader`. Defaults to do no collation and return a list of\n",
    "            data. No collation is OK for small batch size and simple data structures.\n",
    "            If your batch size is large and each sample contains too many small tensors,\n",
    "            it's more efficient to collate them in data loader.\n",
    "    Returns:\n",
    "        torch.utils.data.DataLoader:\n",
    "            a dataloader. Each output from it is a ``list[mapped_element]`` of length\n",
    "            ``total_batch_size / num_workers``, where ``mapped_element`` is produced\n",
    "            by the ``mapper``.\n",
    "    \"\"\"\n",
    "    if isinstance(dataset, list):\n",
    "        dataset = DatasetFromList(dataset, copy=False)\n",
    "    if mapper is not None:\n",
    "        dataset = MapDataset(dataset, mapper)\n",
    "\n",
    "    if isinstance(dataset, torchdata.IterableDataset):\n",
    "        assert sampler is None, \"sampler must be None if dataset is IterableDataset\"\n",
    "    else:\n",
    "        if sampler is None:\n",
    "            sampler = TrainingSampler(len(dataset))\n",
    "        assert isinstance(sampler, torchdata.Sampler), f\"Expect a Sampler but got {type(sampler)}\"\n",
    "    return build_batch_data_loader(\n",
    "        dataset,\n",
    "        sampler,\n",
    "        total_batch_size,\n",
    "        aspect_ratio_grouping=aspect_ratio_grouping,\n",
    "        num_workers=num_workers,\n",
    "        collate_fn=collate_fn,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, Detectron2 doesn't do exactly this, but a rough approximation of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from detectron2.data import build_detection_train_loader\n",
    "\n",
    "\n",
    "class Trainer(DefaultTrainer):\n",
    "    \"\"\"\n",
    "    We use the \"DefaultTrainer\" which contains pre-defined default logic for\n",
    "    standard training workflow. They may not work for you, especially if you\n",
    "    are working on a new research project. In that case you can write your\n",
    "    own training loop. You can use \"tools/plain_train_net.py\" as an example.\n",
    "    \"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            output_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\n",
    "        return [COCOEvaluator(dataset_name, output_dir=output_folder)]\n",
    "    \n",
    "    @classmethod\n",
    "    def test_with_TTA(cls, cfg, model):\n",
    "        logger = logging.getLogger(\"detectron2.trainer\")\n",
    "        # In the end of training, run an evaluation with TTA\n",
    "        # Only support some R-CNN models.\n",
    "        logger.info(\"Running inference with test-time augmentation ...\")\n",
    "        model = GeneralizedRCNNWithTTA(cfg, model)\n",
    "        evaluators = [\n",
    "            cls.build_evaluator(\n",
    "                cfg, name, output_folder=os.path.join(cfg.OUTPUT_DIR, \"inference_TTA\")\n",
    "            )\n",
    "            for name in cfg.DATASETS.TEST\n",
    "        ]\n",
    "        res = cls.test(cfg, model, evaluators)\n",
    "        res = OrderedDict({k + \"_TTA\": v for k, v in res.items()})\n",
    "        return res\n",
    "    \n",
    "    @classmethod\n",
    "    @configurable(from_config=_train_loader_from_config)\n",
    "    def build_train_loader(cls, cfg):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            iterable\n",
    "        It now calls :func:`detectron2.data.build_detection_train_loader`.\n",
    "        Overwrite it if you'd like a different data loader.\n",
    "        \"\"\"\n",
    "        return build_our_custom_detection_train_loader(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Construct the configuration\n",
    "\n",
    "Detectron2 provides a flat-file interface, so that's what we'll be setting up in the below cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cfg = get_cfg()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Initial model selection\n",
    "\n",
    "A good baseline will be utilizing a Faster-RCNN with an FPN. The Faster RCNN is a two-stage object detector that has set many benchmarks over the years and still provides robust performance in the era of enhanced single-stage detectors. The FPN provides multi-scale feature maps to enhance scale invariance of the resulting detector, which is less of a concern with this dataset since all of the data is hand held imagery and the objects are fairly uniform in scale (unlike in overhead imagery). We'll use a Resnet50 as the backbone because it provides decent feature extraction performance without being computationally or memory intensive. Lets load the configuration for the R50 Faster-RCNN FPN now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cfg.merge_from_file(get_config_file('COCO-Detection/faster_rcnn_R_50_FPN_1x.yaml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Setup our data configuration\n",
    "\n",
    "Below, we are simply defining the names of the datasets we wish to use. These datasets have not been registered yet, but we will do that in the cell after. I'm going to use 8 workers because I have a CPU with 12 cores and want to ensure I have enough processes loading data to saturate the PCIE/GPU without overtaxing my system and causing stuttering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cfg.DATASETS.TRAIN = ('manu_id_train',)\n",
    "cfg.DATASETS.TEST = ('manu_id_val',)\n",
    "cfg.DATALOADER.NUM_WORKERS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Invoke our WeightedRandomSampler\n",
    "\n",
    "To ensure Detectron2 uses our new sampler, we simpley define it in the config. Lets do that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.DATALOADER.SAMPLER_TRAIN = 'WeightedRandomSampler'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Register the dataset\n",
    "\n",
    "The cell below is where the dataset is registered by name (matching the names we defined above). To do this, we simply register the directory containing the imagery and the COCO json files we created previously. Since we are training, we want to register the train and validation sets. The validation set will allow us to evaluate our model at intermediate phases throughout training and perform model selection. Since we are using the val set to select our best model, it should **not** be the same dataset we ultimately evaluate our model on (that will be the test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "image_root = '/home/zack/datasets/manufacturer_identification/data/images'\n",
    "register_coco_instances('manu_id_train', {}, image_root=image_root, json_file='/home/zack/datasets/manufacturer_identification/data/coco/train.json')\n",
    "register_coco_instances('manu_id_val', {}, image_root=image_root, json_file='/home/zack/datasets/manufacturer_identification/data/coco/val.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Define the model weights we wish to use for initialization\n",
    "\n",
    "Initialization of object detectors is a massively explored sub-field of machine learning. In some cases, random initialization is okay to use, but utilizing pre-trained feature extractors (e.g. Resnet50) or full pre-trained networks (e.g. Faster-RCNN FPN) regularly provides superior performance to random initialization. There are several various pretraining regimes, of which the most populare are taking 1. Imagenet pretrained feature extractors (recognition task), 2. COCO/other detection dataset pretrained detectors (detection task), or 3. Self-supervised pretrained feature extractors (recognition task). Generally, the recognition pretrained feature extractors provide robust feature extraction capabilities and a strong starting point for optimization to take over. However, one challenge is that the remainder of the network (FPN Llteral connections, RPN, Box/Mask/Classifier heads) is still randomly initialized. Additionally, pre-training on recognition tasks often doesn't require scale information to the same level as an object detector, meaning some features may be useless/uninformative.\n",
    "\n",
    "Using a full pretrained network is in most ways superior to a simple pretrained feature extractor. Recent work has focused on unsupervised object detection training using Selective Search or properties of DETR, but these are still early techniques. For our application and for the sake of time/complexity, I will load the weights for a R50 Faster-RCNN FPN that was pretrained on the COCO dataset with Large Scale Jitter, which is a new augmentation and learning schedule that has shown large performance benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Large scale jitter likely won't help us here since most objects are at the same scale, but lets load the R50-FasterRCNN-FPN LSJ model weights as a good pretrained starting point.\n",
    "cfg.MODEL.WEIGHTS = get_checkpoint_url('new_baselines/mask_rcnn_R_50_FPN_400ep_LSJ.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Set the number of classes\n",
    "\n",
    "The problem description says there are 41 classes, but I only see 30 classes in the labels.txt file and in the unique values of the box_labels.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Define our training schedule\n",
    "\n",
    "Since this dataset isn't very large, we likely don't need to train for very long. I'm also pressed on time, so shorter training runs are better. 40,000 iterations seems like a reasonable place to start, but even that is likely too many iterations to train and will likely lead to overfitting. As mentioned above, one way to avoid overfitting is through utilizing a validation dataset. Detectron2 performs intermediate validation, but doesn't checkpoint based on the metric (unfortunately). After training, it will be up to us to determine the best model based on the logged metrics. Lets set the batch size to 6 (RTX 3090), and lets step the learning rate down at a schedule of 25000 and 35000 iterations. This will reduce the learning rate by some gamma (0.1) at these iterations, ideally model improving convergence as we approach a minima (hopefully the global minima and not a pesky local minima. CosineScheduler/Cyclic learning rates could help us get around that, but Detectron2 doesn't support those currently)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cfg.SOLVER.MAX_ITER = 40000\n",
    "cfg.SOLVER.STEPS = (25000, 35000)\n",
    "# I have to reduce the batch size here due to GPU memory constraints while I work on other things. This model will effectively see less data, but I think that's okay.\n",
    "cfg.SOLVER.IMS_PER_BATCH = 6\n",
    "# Checkpoint at a 1:1 ratio with evaluation\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 1000\n",
    "cfg.TEST.EVAL_PERIOD = 1000\n",
    "\n",
    "cfg.OUTPUT_DIR = './output_class_balanced'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train the model\n",
    "\n",
    "Train the model, making sure to load the weights we specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/21 23:41:28 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=31, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=120, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[04/21 23:41:28 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[04/21 23:41:28 d2.data.datasets.coco]: \u001b[0mLoaded 3334 images in COCO format from /home/zack/datasets/manufacturer_identification/data/coco/train.json\n",
      "\u001b[32m[04/21 23:41:28 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 3334 images left.\n",
      "\u001b[32m[04/21 23:41:28 d2.data.build]: \u001b[0mDistribution of instances among all 30 categories:\n",
      "\u001b[36m|   category    | #instances   |   category    | #instances   |   category    | #instances   |\n",
      "|:-------------:|:-------------|:-------------:|:-------------|:-------------:|:-------------|\n",
      "|      ATR      | 66           |    Airbus     | 434          |    Antonov    | 34           |\n",
      "|  Beechcraft   | 67           |    Boeing     | 733          | Bombardier .. | 33           |\n",
      "| British Aer.. | 133          |   Canadair    | 134          |    Cessna     | 133          |\n",
      "| Cirrus Airc.. | 33           | Dassault Av.. | 67           |    Dornier    | 34           |\n",
      "| Douglas Air.. | 133          |    Embraer    | 233          |  Eurofighter  | 33           |\n",
      "|   Fairchild   | 33           |    Fokker     | 100          | Gulfstream .. | 67           |\n",
      "|   Ilyushin    | 33           | Lockheed Co.. | 68           | Lockheed Ma.. | 34           |\n",
      "| McDonnell D.. | 232          |    Panavia    | 34           |     Piper     | 33           |\n",
      "|     Robin     | 33           |     Saab      | 67           |  Supermarine  | 33           |\n",
      "|    Tupolev    | 66           |   Yakovlev    | 34           | de Havilland  | 167          |\n",
      "|               |              |               |              |               |              |\n",
      "|     total     | 3334         |               |              |               |              |\u001b[0m\n",
      "\u001b[32m[04/21 23:41:28 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "Using training sampler WeightedRandomSampler\n",
      "Weighting draws by {28: 0.05323674151243055, 1: 0.00417062030281714, 21: 0.007801936256132064, 4: 0.0024693713661973244, 22: 0.05323674151243055, 13: 0.007768451551170123, 16: 0.018100492114226388, 29: 0.010838618032470892, 8: 0.01360939256708751, 23: 0.05484997610371633, 6: 0.01360939256708751, 10: 0.027015659871979684, 19: 0.026618370756215277, 0: 0.027424988051858164, 17: 0.027015659871979684, 5: 0.05484997610371633, 14: 0.05484997610371633, 25: 0.027015659871979684, 26: 0.05484997610371633, 27: 0.027424988051858164, 20: 0.05323674151243055, 7: 0.013507829935989842, 12: 0.01360939256708751, 24: 0.05484997610371633, 3: 0.027015659871979684, 18: 0.05484997610371633, 11: 0.05323674151243055, 9: 0.05484997610371633, 15: 0.05484997610371633, 2: 0.05323674151243055}\n",
      "\u001b[32m[04/21 23:41:28 d2.data.common]: \u001b[0mSerializing 3334 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[04/21 23:41:28 d2.data.common]: \u001b[0mSerialized dataset takes 0.85 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (31, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (31,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (120, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (120,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mbackbone.fpn_lateral2.bias\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral3.bias\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral4.bias\u001b[0m\n",
      "\u001b[34mbackbone.fpn_lateral5.bias\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output2.bias\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output3.bias\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output4.bias\u001b[0m\n",
      "\u001b[34mbackbone.fpn_output5.bias\u001b[0m\n",
      "\u001b[34mproposal_generator.rpn_head.conv.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_head.fc2.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n",
      "The checkpoint state_dict contains keys that are not used by the model:\n",
      "  \u001b[35mbackbone.fpn_lateral2.norm.{bias, num_batches_tracked, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.fpn_output2.norm.{bias, num_batches_tracked, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.fpn_lateral3.norm.{bias, num_batches_tracked, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.fpn_output3.norm.{bias, num_batches_tracked, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.fpn_lateral4.norm.{bias, num_batches_tracked, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.fpn_output4.norm.{bias, num_batches_tracked, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.fpn_lateral5.norm.{bias, num_batches_tracked, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.fpn_output5.norm.{bias, num_batches_tracked, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.stem.conv1.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.0.shortcut.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.0.conv1.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.0.conv2.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.0.conv3.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.1.conv1.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.1.conv2.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.1.conv3.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.2.conv1.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.2.conv2.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res2.2.conv3.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.0.shortcut.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.0.conv1.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.0.conv2.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.0.conv3.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.1.conv1.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.1.conv2.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.1.conv3.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.2.conv1.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.2.conv2.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.2.conv3.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.3.conv1.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.3.conv2.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res3.3.conv3.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.0.shortcut.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.0.conv1.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.0.conv2.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.0.conv3.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.1.conv1.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.1.conv2.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.1.conv3.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.2.conv1.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.2.conv2.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.2.conv3.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.3.conv1.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.3.conv2.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.3.conv3.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.4.conv1.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.4.conv2.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.4.conv3.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.5.conv1.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.5.conv2.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res4.5.conv3.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.0.shortcut.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.0.conv1.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.0.conv2.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.0.conv3.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.1.conv1.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.1.conv2.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.1.conv3.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.2.conv1.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.2.conv2.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mbackbone.bottom_up.res5.2.conv3.norm.num_batches_tracked\u001b[0m\n",
      "  \u001b[35mproposal_generator.rpn_head.conv.conv0.{bias, weight}\u001b[0m\n",
      "  \u001b[35mproposal_generator.rpn_head.conv.conv1.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.mask_fcn1.weight\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.mask_fcn1.norm.{bias, num_batches_tracked, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.mask_fcn2.weight\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.mask_fcn2.norm.{bias, num_batches_tracked, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.mask_fcn3.weight\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.mask_fcn3.norm.{bias, num_batches_tracked, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.mask_fcn4.weight\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.mask_fcn4.norm.{bias, num_batches_tracked, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.deconv.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.mask_head.predictor.{bias, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.conv1.weight\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.conv1.norm.{bias, num_batches_tracked, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.conv2.weight\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.conv2.norm.{bias, num_batches_tracked, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.conv3.weight\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.conv3.norm.{bias, num_batches_tracked, running_mean, running_var, weight}\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.conv4.weight\u001b[0m\n",
      "  \u001b[35mroi_heads.box_head.conv4.norm.{bias, num_batches_tracked, running_mean, running_var, weight}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/21 23:41:29 d2.engine.train_loop]: \u001b[0mStarting training from iteration 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zack/anaconda3/envs/riverside/lib/python3.8/site-packages/detectron2/structures/image_list.py:88: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  max_size = (max_size + (stride - 1)) // stride * stride\n",
      "/home/zack/anaconda3/envs/riverside/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/21 23:41:37 d2.utils.events]: \u001b[0m eta: 4:11:28  iter: 19  total_loss: 0.2932  loss_cls: 0.1164  loss_box_reg: 0.009293  loss_rpn_cls: 0.09114  loss_rpn_loc: 0.0523  time: 0.3779  data_time: 0.0262  lr: 0.00039962  max_mem: 5885M\n",
      "\u001b[32m[04/21 23:41:46 d2.utils.events]: \u001b[0m eta: 4:26:19  iter: 39  total_loss: 0.2718  loss_cls: 0.1389  loss_box_reg: 0.08177  loss_rpn_cls: 0.01241  loss_rpn_loc: 0.01527  time: 0.4034  data_time: 0.0060  lr: 0.00079922  max_mem: 5885M\n",
      "\u001b[32m[04/21 23:41:54 d2.utils.events]: \u001b[0m eta: 4:37:39  iter: 59  total_loss: 0.2232  loss_cls: 0.1148  loss_box_reg: 0.08254  loss_rpn_cls: 0.008943  loss_rpn_loc: 0.01049  time: 0.4116  data_time: 0.0060  lr: 0.0011988  max_mem: 6157M\n",
      "\u001b[32m[04/21 23:42:03 d2.utils.events]: \u001b[0m eta: 4:38:07  iter: 79  total_loss: 0.2075  loss_cls: 0.1072  loss_box_reg: 0.08508  loss_rpn_cls: 0.005604  loss_rpn_loc: 0.009233  time: 0.4144  data_time: 0.0061  lr: 0.0015984  max_mem: 6157M\n",
      "\u001b[32m[04/21 23:42:11 d2.utils.events]: \u001b[0m eta: 4:38:22  iter: 99  total_loss: 0.2186  loss_cls: 0.11  loss_box_reg: 0.08775  loss_rpn_cls: 0.005209  loss_rpn_loc: 0.01099  time: 0.4174  data_time: 0.0062  lr: 0.001998  max_mem: 6157M\n",
      "\u001b[32m[04/21 23:42:20 d2.utils.events]: \u001b[0m eta: 4:40:28  iter: 119  total_loss: 0.2215  loss_cls: 0.1156  loss_box_reg: 0.09651  loss_rpn_cls: 0.004657  loss_rpn_loc: 0.00823  time: 0.4177  data_time: 0.0061  lr: 0.0023976  max_mem: 6157M\n",
      "\u001b[32m[04/21 23:42:28 d2.utils.events]: \u001b[0m eta: 4:40:26  iter: 139  total_loss: 0.2156  loss_cls: 0.107  loss_box_reg: 0.0903  loss_rpn_cls: 0.004586  loss_rpn_loc: 0.01154  time: 0.4199  data_time: 0.0060  lr: 0.0027972  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:42:38 d2.utils.events]: \u001b[0m eta: 4:41:31  iter: 159  total_loss: 0.2045  loss_cls: 0.1008  loss_box_reg: 0.08525  loss_rpn_cls: 0.005158  loss_rpn_loc: 0.009311  time: 0.4243  data_time: 0.0058  lr: 0.0031968  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:42:46 d2.utils.events]: \u001b[0m eta: 4:40:47  iter: 179  total_loss: 0.1984  loss_cls: 0.1001  loss_box_reg: 0.0866  loss_rpn_cls: 0.002187  loss_rpn_loc: 0.008841  time: 0.4240  data_time: 0.0060  lr: 0.0035964  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:42:55 d2.utils.events]: \u001b[0m eta: 4:42:07  iter: 199  total_loss: 0.2071  loss_cls: 0.102  loss_box_reg: 0.08883  loss_rpn_cls: 0.003744  loss_rpn_loc: 0.01119  time: 0.4264  data_time: 0.0059  lr: 0.003996  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:43:04 d2.utils.events]: \u001b[0m eta: 4:42:28  iter: 219  total_loss: 0.2092  loss_cls: 0.09915  loss_box_reg: 0.0864  loss_rpn_cls: 0.002441  loss_rpn_loc: 0.008592  time: 0.4283  data_time: 0.0061  lr: 0.0043956  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:43:13 d2.utils.events]: \u001b[0m eta: 4:42:41  iter: 239  total_loss: 0.2177  loss_cls: 0.1086  loss_box_reg: 0.09854  loss_rpn_cls: 0.0032  loss_rpn_loc: 0.00783  time: 0.4293  data_time: 0.0058  lr: 0.0047952  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:43:21 d2.utils.events]: \u001b[0m eta: 4:42:32  iter: 259  total_loss: 0.2031  loss_cls: 0.1012  loss_box_reg: 0.09341  loss_rpn_cls: 0.002716  loss_rpn_loc: 0.007634  time: 0.4291  data_time: 0.0059  lr: 0.0051948  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:43:30 d2.utils.events]: \u001b[0m eta: 4:43:42  iter: 279  total_loss: 0.2034  loss_cls: 0.09958  loss_box_reg: 0.08949  loss_rpn_cls: 0.00207  loss_rpn_loc: 0.008656  time: 0.4312  data_time: 0.0062  lr: 0.0055944  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:43:39 d2.utils.events]: \u001b[0m eta: 4:43:39  iter: 299  total_loss: 0.2064  loss_cls: 0.1001  loss_box_reg: 0.09174  loss_rpn_cls: 0.002943  loss_rpn_loc: 0.007756  time: 0.4320  data_time: 0.0059  lr: 0.005994  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:43:48 d2.utils.events]: \u001b[0m eta: 4:44:21  iter: 319  total_loss: 0.215  loss_cls: 0.1074  loss_box_reg: 0.0971  loss_rpn_cls: 0.002389  loss_rpn_loc: 0.008574  time: 0.4334  data_time: 0.0058  lr: 0.0063936  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:43:57 d2.utils.events]: \u001b[0m eta: 4:43:59  iter: 339  total_loss: 0.1983  loss_cls: 0.1005  loss_box_reg: 0.09062  loss_rpn_cls: 0.002095  loss_rpn_loc: 0.007686  time: 0.4330  data_time: 0.0061  lr: 0.0067932  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:44:05 d2.utils.events]: \u001b[0m eta: 4:43:10  iter: 359  total_loss: 0.2139  loss_cls: 0.1033  loss_box_reg: 0.09846  loss_rpn_cls: 0.003077  loss_rpn_loc: 0.009308  time: 0.4321  data_time: 0.0057  lr: 0.0071928  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:44:14 d2.utils.events]: \u001b[0m eta: 4:42:50  iter: 379  total_loss: 0.2156  loss_cls: 0.1059  loss_box_reg: 0.09825  loss_rpn_cls: 0.001464  loss_rpn_loc: 0.008501  time: 0.4310  data_time: 0.0060  lr: 0.0075924  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:44:22 d2.utils.events]: \u001b[0m eta: 4:42:53  iter: 399  total_loss: 0.23  loss_cls: 0.1152  loss_box_reg: 0.1008  loss_rpn_cls: 0.002199  loss_rpn_loc: 0.007937  time: 0.4310  data_time: 0.0060  lr: 0.007992  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:44:31 d2.utils.events]: \u001b[0m eta: 4:42:40  iter: 419  total_loss: 0.2102  loss_cls: 0.1054  loss_box_reg: 0.09209  loss_rpn_cls: 0.002639  loss_rpn_loc: 0.007976  time: 0.4312  data_time: 0.0062  lr: 0.0083916  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:44:39 d2.utils.events]: \u001b[0m eta: 4:42:31  iter: 439  total_loss: 0.3033  loss_cls: 0.1498  loss_box_reg: 0.1335  loss_rpn_cls: 0.002565  loss_rpn_loc: 0.008586  time: 0.4310  data_time: 0.0058  lr: 0.0087912  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:44:48 d2.utils.events]: \u001b[0m eta: 4:41:44  iter: 459  total_loss: 0.2744  loss_cls: 0.1416  loss_box_reg: 0.1223  loss_rpn_cls: 0.002362  loss_rpn_loc: 0.009038  time: 0.4301  data_time: 0.0059  lr: 0.0091908  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:44:56 d2.utils.events]: \u001b[0m eta: 4:41:05  iter: 479  total_loss: 0.2072  loss_cls: 0.107  loss_box_reg: 0.09069  loss_rpn_cls: 0.001835  loss_rpn_loc: 0.007302  time: 0.4296  data_time: 0.0061  lr: 0.0095904  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:45:05 d2.utils.events]: \u001b[0m eta: 4:41:08  iter: 499  total_loss: 0.2555  loss_cls: 0.1344  loss_box_reg: 0.1106  loss_rpn_cls: 0.003189  loss_rpn_loc: 0.00693  time: 0.4300  data_time: 0.0060  lr: 0.00999  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:45:13 d2.utils.events]: \u001b[0m eta: 4:41:12  iter: 519  total_loss: 0.3083  loss_cls: 0.1666  loss_box_reg: 0.1364  loss_rpn_cls: 0.0007899  loss_rpn_loc: 0.006717  time: 0.4302  data_time: 0.0059  lr: 0.01039  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:45:23 d2.utils.events]: \u001b[0m eta: 4:41:44  iter: 539  total_loss: 0.2975  loss_cls: 0.1718  loss_box_reg: 0.127  loss_rpn_cls: 0.001101  loss_rpn_loc: 0.00745  time: 0.4310  data_time: 0.0061  lr: 0.010789  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:45:32 d2.utils.events]: \u001b[0m eta: 4:41:46  iter: 559  total_loss: 0.314  loss_cls: 0.184  loss_box_reg: 0.117  loss_rpn_cls: 0.001744  loss_rpn_loc: 0.007953  time: 0.4323  data_time: 0.0064  lr: 0.011189  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:45:41 d2.utils.events]: \u001b[0m eta: 4:41:55  iter: 579  total_loss: 0.2813  loss_cls: 0.1738  loss_box_reg: 0.09233  loss_rpn_cls: 0.001442  loss_rpn_loc: 0.008158  time: 0.4329  data_time: 0.0066  lr: 0.011588  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:45:50 d2.utils.events]: \u001b[0m eta: 4:41:36  iter: 599  total_loss: 0.2653  loss_cls: 0.1725  loss_box_reg: 0.08182  loss_rpn_cls: 0.001786  loss_rpn_loc: 0.009159  time: 0.4326  data_time: 0.0065  lr: 0.011988  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:45:58 d2.utils.events]: \u001b[0m eta: 4:41:59  iter: 619  total_loss: 0.2292  loss_cls: 0.1522  loss_box_reg: 0.06534  loss_rpn_cls: 0.001738  loss_rpn_loc: 0.007148  time: 0.4330  data_time: 0.0061  lr: 0.012388  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:46:07 d2.utils.events]: \u001b[0m eta: 4:42:03  iter: 639  total_loss: 0.2274  loss_cls: 0.156  loss_box_reg: 0.06074  loss_rpn_cls: 0.00115  loss_rpn_loc: 0.006226  time: 0.4331  data_time: 0.0060  lr: 0.012787  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:46:16 d2.utils.events]: \u001b[0m eta: 4:42:01  iter: 659  total_loss: 0.2215  loss_cls: 0.1532  loss_box_reg: 0.05846  loss_rpn_cls: 0.00169  loss_rpn_loc: 0.007791  time: 0.4333  data_time: 0.0065  lr: 0.013187  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:46:24 d2.utils.events]: \u001b[0m eta: 4:41:33  iter: 679  total_loss: 0.2459  loss_cls: 0.172  loss_box_reg: 0.064  loss_rpn_cls: 0.0008955  loss_rpn_loc: 0.007823  time: 0.4330  data_time: 0.0065  lr: 0.013586  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:46:33 d2.utils.events]: \u001b[0m eta: 4:41:17  iter: 699  total_loss: 0.196  loss_cls: 0.1352  loss_box_reg: 0.04723  loss_rpn_cls: 0.000603  loss_rpn_loc: 0.006905  time: 0.4325  data_time: 0.0063  lr: 0.013986  max_mem: 6293M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/21 23:46:41 d2.utils.events]: \u001b[0m eta: 4:40:51  iter: 719  total_loss: 0.1987  loss_cls: 0.1396  loss_box_reg: 0.04933  loss_rpn_cls: 0.002101  loss_rpn_loc: 0.007917  time: 0.4321  data_time: 0.0062  lr: 0.014386  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:46:49 d2.utils.events]: \u001b[0m eta: 4:40:25  iter: 739  total_loss: 0.1833  loss_cls: 0.1276  loss_box_reg: 0.04308  loss_rpn_cls: 0.001112  loss_rpn_loc: 0.006278  time: 0.4312  data_time: 0.0060  lr: 0.014785  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:46:57 d2.utils.events]: \u001b[0m eta: 4:39:50  iter: 759  total_loss: 0.1684  loss_cls: 0.1171  loss_box_reg: 0.04106  loss_rpn_cls: 0.001439  loss_rpn_loc: 0.006578  time: 0.4303  data_time: 0.0061  lr: 0.015185  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:47:05 d2.utils.events]: \u001b[0m eta: 4:39:41  iter: 779  total_loss: 0.1766  loss_cls: 0.1214  loss_box_reg: 0.04123  loss_rpn_cls: 0.001043  loss_rpn_loc: 0.007177  time: 0.4300  data_time: 0.0061  lr: 0.015584  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:47:14 d2.utils.events]: \u001b[0m eta: 4:39:04  iter: 799  total_loss: 0.1827  loss_cls: 0.1279  loss_box_reg: 0.04566  loss_rpn_cls: 0.001277  loss_rpn_loc: 0.006889  time: 0.4297  data_time: 0.0063  lr: 0.015984  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:47:22 d2.utils.events]: \u001b[0m eta: 4:38:52  iter: 819  total_loss: 0.1816  loss_cls: 0.1275  loss_box_reg: 0.03989  loss_rpn_cls: 0.001796  loss_rpn_loc: 0.007802  time: 0.4297  data_time: 0.0061  lr: 0.016384  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:47:31 d2.utils.events]: \u001b[0m eta: 4:38:47  iter: 839  total_loss: 0.218  loss_cls: 0.1523  loss_box_reg: 0.04992  loss_rpn_cls: 0.002985  loss_rpn_loc: 0.008271  time: 0.4299  data_time: 0.0063  lr: 0.016783  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:47:40 d2.utils.events]: \u001b[0m eta: 4:38:34  iter: 859  total_loss: 0.1604  loss_cls: 0.1084  loss_box_reg: 0.03969  loss_rpn_cls: 0.00182  loss_rpn_loc: 0.007665  time: 0.4298  data_time: 0.0062  lr: 0.017183  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:47:48 d2.utils.events]: \u001b[0m eta: 4:38:26  iter: 879  total_loss: 0.1353  loss_cls: 0.1006  loss_box_reg: 0.03114  loss_rpn_cls: 0.000837  loss_rpn_loc: 0.006851  time: 0.4299  data_time: 0.0063  lr: 0.017582  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:47:57 d2.utils.events]: \u001b[0m eta: 4:38:17  iter: 899  total_loss: 0.1384  loss_cls: 0.09907  loss_box_reg: 0.02908  loss_rpn_cls: 0.001158  loss_rpn_loc: 0.006521  time: 0.4298  data_time: 0.0060  lr: 0.017982  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:48:06 d2.utils.events]: \u001b[0m eta: 4:38:09  iter: 919  total_loss: 0.1673  loss_cls: 0.1153  loss_box_reg: 0.04162  loss_rpn_cls: 0.0007041  loss_rpn_loc: 0.007117  time: 0.4302  data_time: 0.0063  lr: 0.018382  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:48:15 d2.utils.events]: \u001b[0m eta: 4:38:00  iter: 939  total_loss: 0.1648  loss_cls: 0.1192  loss_box_reg: 0.04283  loss_rpn_cls: 0.001031  loss_rpn_loc: 0.005492  time: 0.4302  data_time: 0.0061  lr: 0.018781  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:48:23 d2.utils.events]: \u001b[0m eta: 4:37:51  iter: 959  total_loss: 0.1568  loss_cls: 0.1059  loss_box_reg: 0.04216  loss_rpn_cls: 0.001193  loss_rpn_loc: 0.006852  time: 0.4300  data_time: 0.0061  lr: 0.019181  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:48:32 d2.utils.events]: \u001b[0m eta: 4:37:47  iter: 979  total_loss: 0.1688  loss_cls: 0.1138  loss_box_reg: 0.03945  loss_rpn_cls: 0.001481  loss_rpn_loc: 0.008424  time: 0.4300  data_time: 0.0058  lr: 0.01958  max_mem: 6293M\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[04/21 23:48:41 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[04/21 23:48:41 d2.data.datasets.coco]: \u001b[0mLoaded 3333 images in COCO format from /home/zack/datasets/manufacturer_identification/data/coco/val.json\n",
      "\u001b[32m[04/21 23:48:41 d2.data.build]: \u001b[0mDistribution of instances among all 30 categories:\n",
      "\u001b[36m|   category    | #instances   |   category    | #instances   |   category    | #instances   |\n",
      "|:-------------:|:-------------|:-------------:|:-------------|:-------------:|:-------------|\n",
      "|      ATR      | 67           |    Airbus     | 433          |    Antonov    | 33           |\n",
      "|  Beechcraft   | 67           |    Boeing     | 733          | Bombardier .. | 34           |\n",
      "| British Aer.. | 134          |   Canadair    | 133          |    Cessna     | 133          |\n",
      "| Cirrus Airc.. | 33           | Dassault Av.. | 66           |    Dornier    | 33           |\n",
      "| Douglas Air.. | 134          |    Embraer    | 234          |  Eurofighter  | 33           |\n",
      "|   Fairchild   | 33           |    Fokker     | 100          | Gulfstream .. | 66           |\n",
      "|   Ilyushin    | 33           | Lockheed Co.. | 66           | Lockheed Ma.. | 33           |\n",
      "| McDonnell D.. | 235          |    Panavia    | 33           |     Piper     | 34           |\n",
      "|     Robin     | 33           |     Saab      | 67           |  Supermarine  | 33           |\n",
      "|    Tupolev    | 67           |   Yakovlev    | 33           | de Havilland  | 167          |\n",
      "|               |              |               |              |               |              |\n",
      "|     total     | 3333         |               |              |               |              |\u001b[0m\n",
      "\u001b[32m[04/21 23:48:41 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[04/21 23:48:41 d2.data.common]: \u001b[0mSerializing 3333 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[04/21 23:48:41 d2.data.common]: \u001b[0mSerialized dataset takes 0.85 MiB\n",
      "\u001b[32m[04/21 23:48:41 d2.evaluation.evaluator]: \u001b[0mStart inference on 3333 batches\n",
      "\u001b[32m[04/21 23:48:42 d2.evaluation.evaluator]: \u001b[0mInference done 11/3333. Dataloading: 0.0004 s/iter. Inference: 0.0334 s/iter. Eval: 0.0001 s/iter. Total: 0.0340 s/iter. ETA=0:01:52\n",
      "\u001b[32m[04/21 23:48:47 d2.evaluation.evaluator]: \u001b[0mInference done 164/3333. Dataloading: 0.0008 s/iter. Inference: 0.0319 s/iter. Eval: 0.0001 s/iter. Total: 0.0329 s/iter. ETA=0:01:44\n",
      "\u001b[32m[04/21 23:48:52 d2.evaluation.evaluator]: \u001b[0mInference done 311/3333. Dataloading: 0.0008 s/iter. Inference: 0.0324 s/iter. Eval: 0.0001 s/iter. Total: 0.0334 s/iter. ETA=0:01:41\n",
      "\u001b[32m[04/21 23:48:57 d2.evaluation.evaluator]: \u001b[0mInference done 470/3333. Dataloading: 0.0008 s/iter. Inference: 0.0318 s/iter. Eval: 0.0001 s/iter. Total: 0.0328 s/iter. ETA=0:01:33\n",
      "\u001b[32m[04/21 23:49:02 d2.evaluation.evaluator]: \u001b[0mInference done 620/3333. Dataloading: 0.0008 s/iter. Inference: 0.0320 s/iter. Eval: 0.0001 s/iter. Total: 0.0330 s/iter. ETA=0:01:29\n",
      "\u001b[32m[04/21 23:49:07 d2.evaluation.evaluator]: \u001b[0mInference done 760/3333. Dataloading: 0.0008 s/iter. Inference: 0.0325 s/iter. Eval: 0.0001 s/iter. Total: 0.0335 s/iter. ETA=0:01:26\n",
      "\u001b[32m[04/21 23:49:12 d2.evaluation.evaluator]: \u001b[0mInference done 906/3333. Dataloading: 0.0008 s/iter. Inference: 0.0327 s/iter. Eval: 0.0001 s/iter. Total: 0.0336 s/iter. ETA=0:01:21\n",
      "\u001b[32m[04/21 23:49:17 d2.evaluation.evaluator]: \u001b[0mInference done 1046/3333. Dataloading: 0.0008 s/iter. Inference: 0.0330 s/iter. Eval: 0.0001 s/iter. Total: 0.0339 s/iter. ETA=0:01:17\n",
      "\u001b[32m[04/21 23:49:22 d2.evaluation.evaluator]: \u001b[0mInference done 1192/3333. Dataloading: 0.0008 s/iter. Inference: 0.0330 s/iter. Eval: 0.0001 s/iter. Total: 0.0340 s/iter. ETA=0:01:12\n",
      "\u001b[32m[04/21 23:49:27 d2.evaluation.evaluator]: \u001b[0mInference done 1342/3333. Dataloading: 0.0008 s/iter. Inference: 0.0330 s/iter. Eval: 0.0001 s/iter. Total: 0.0339 s/iter. ETA=0:01:07\n",
      "\u001b[32m[04/21 23:49:32 d2.evaluation.evaluator]: \u001b[0mInference done 1489/3333. Dataloading: 0.0008 s/iter. Inference: 0.0330 s/iter. Eval: 0.0001 s/iter. Total: 0.0340 s/iter. ETA=0:01:02\n",
      "\u001b[32m[04/21 23:49:37 d2.evaluation.evaluator]: \u001b[0mInference done 1637/3333. Dataloading: 0.0008 s/iter. Inference: 0.0330 s/iter. Eval: 0.0001 s/iter. Total: 0.0340 s/iter. ETA=0:00:57\n",
      "\u001b[32m[04/21 23:49:42 d2.evaluation.evaluator]: \u001b[0mInference done 1762/3333. Dataloading: 0.0008 s/iter. Inference: 0.0334 s/iter. Eval: 0.0001 s/iter. Total: 0.0344 s/iter. ETA=0:00:54\n",
      "\u001b[32m[04/21 23:49:47 d2.evaluation.evaluator]: \u001b[0mInference done 1906/3333. Dataloading: 0.0008 s/iter. Inference: 0.0335 s/iter. Eval: 0.0001 s/iter. Total: 0.0344 s/iter. ETA=0:00:49\n",
      "\u001b[32m[04/21 23:49:52 d2.evaluation.evaluator]: \u001b[0mInference done 2059/3333. Dataloading: 0.0008 s/iter. Inference: 0.0334 s/iter. Eval: 0.0001 s/iter. Total: 0.0343 s/iter. ETA=0:00:43\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/21 23:49:57 d2.evaluation.evaluator]: \u001b[0mInference done 2206/3333. Dataloading: 0.0008 s/iter. Inference: 0.0333 s/iter. Eval: 0.0001 s/iter. Total: 0.0343 s/iter. ETA=0:00:38\n",
      "\u001b[32m[04/21 23:50:02 d2.evaluation.evaluator]: \u001b[0mInference done 2358/3333. Dataloading: 0.0008 s/iter. Inference: 0.0333 s/iter. Eval: 0.0001 s/iter. Total: 0.0342 s/iter. ETA=0:00:33\n",
      "\u001b[32m[04/21 23:50:07 d2.evaluation.evaluator]: \u001b[0mInference done 2503/3333. Dataloading: 0.0008 s/iter. Inference: 0.0332 s/iter. Eval: 0.0001 s/iter. Total: 0.0342 s/iter. ETA=0:00:28\n",
      "\u001b[32m[04/21 23:50:12 d2.evaluation.evaluator]: \u001b[0mInference done 2640/3333. Dataloading: 0.0008 s/iter. Inference: 0.0334 s/iter. Eval: 0.0001 s/iter. Total: 0.0344 s/iter. ETA=0:00:23\n",
      "\u001b[32m[04/21 23:50:17 d2.evaluation.evaluator]: \u001b[0mInference done 2786/3333. Dataloading: 0.0008 s/iter. Inference: 0.0334 s/iter. Eval: 0.0001 s/iter. Total: 0.0344 s/iter. ETA=0:00:18\n",
      "\u001b[32m[04/21 23:50:22 d2.evaluation.evaluator]: \u001b[0mInference done 2934/3333. Dataloading: 0.0008 s/iter. Inference: 0.0334 s/iter. Eval: 0.0001 s/iter. Total: 0.0343 s/iter. ETA=0:00:13\n",
      "\u001b[32m[04/21 23:50:27 d2.evaluation.evaluator]: \u001b[0mInference done 3085/3333. Dataloading: 0.0008 s/iter. Inference: 0.0333 s/iter. Eval: 0.0001 s/iter. Total: 0.0343 s/iter. ETA=0:00:08\n",
      "\u001b[32m[04/21 23:50:32 d2.evaluation.evaluator]: \u001b[0mInference done 3237/3333. Dataloading: 0.0008 s/iter. Inference: 0.0332 s/iter. Eval: 0.0001 s/iter. Total: 0.0342 s/iter. ETA=0:00:03\n",
      "\u001b[32m[04/21 23:50:36 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:01:53.986325 (0.034251 s / iter per device, on 1 devices)\n",
      "\u001b[32m[04/21 23:50:36 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:50 (0.033229 s / iter per device, on 1 devices)\n",
      "\u001b[32m[04/21 23:50:36 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[04/21 23:50:36 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output_class_balanced/inference/coco_instances_results.json\n",
      "\u001b[32m[04/21 23:50:36 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.12s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[04/21 23:50:36 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[04/21 23:50:37 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 1.42 seconds.\n",
      "\u001b[32m[04/21 23:50:37 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[04/21 23:50:38 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.21 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.074\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.104\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.090\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.074\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.494\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.496\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.496\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.496\n",
      "\u001b[32m[04/21 23:50:38 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|  AP   |  AP50  |  AP75  |  APs  |  APm  |  APl  |\n",
      "|:-----:|:------:|:------:|:-----:|:-----:|:-----:|\n",
      "| 7.359 | 10.428 | 9.024  |  nan  | 0.000 | 7.361 |\n",
      "\u001b[32m[04/21 23:50:38 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[04/21 23:50:38 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category                 | AP     | category             | AP     | category             | AP     |\n",
      "|:-------------------------|:-------|:---------------------|:-------|:---------------------|:-------|\n",
      "| ATR                      | 0.000  | Airbus               | 12.289 | Antonov              | 1.615  |\n",
      "| Beechcraft               | 2.762  | Boeing               | 27.477 | Bombardier Aerospace | 2.671  |\n",
      "| British Aerospace        | 3.125  | Canadair             | 7.892  | Cessna               | 13.377 |\n",
      "| Cirrus Aircraft          | 21.665 | Dassault Aviation    | 9.058  | Dornier              | 10.982 |\n",
      "| Douglas Aircraft Company | 0.000  | Embraer              | 8.037  | Eurofighter          | 5.143  |\n",
      "| Fairchild                | 4.160  | Fokker               | 2.274  | Gulfstream Aerospace | 0.000  |\n",
      "| Ilyushin                 | 0.000  | Lockheed Corporation | 0.517  | Lockheed Martin      | 7.033  |\n",
      "| McDonnell Douglas        | 9.378  | Panavia              | 4.927  | Piper                | 20.679 |\n",
      "| Robin                    | 22.891 | Saab                 | 4.424  | Supermarine          | 3.756  |\n",
      "| Tupolev                  | 4.865  | Yakovlev             | 1.937  | de Havilland         | 7.832  |\n",
      "\u001b[32m[04/21 23:50:38 d2.engine.defaults]: \u001b[0mEvaluation results for manu_id_val in csv format:\n",
      "\u001b[32m[04/21 23:50:38 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[04/21 23:50:38 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[04/21 23:50:38 d2.evaluation.testing]: \u001b[0mcopypaste: 7.3589,10.4277,9.0241,nan,0.0000,7.3609\n",
      "\u001b[32m[04/21 23:50:38 d2.utils.events]: \u001b[0m eta: 4:37:44  iter: 999  total_loss: 0.1522  loss_cls: 0.106  loss_box_reg: 0.03494  loss_rpn_cls: 0.001177  loss_rpn_loc: 0.007108  time: 0.4301  data_time: 0.0064  lr: 0.01998  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:50:47 d2.utils.events]: \u001b[0m eta: 4:38:03  iter: 1019  total_loss: 0.1502  loss_cls: 0.1051  loss_box_reg: 0.03813  loss_rpn_cls: 0.001001  loss_rpn_loc: 0.008015  time: 0.4302  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:50:56 d2.utils.events]: \u001b[0m eta: 4:38:14  iter: 1039  total_loss: 0.1629  loss_cls: 0.1079  loss_box_reg: 0.03612  loss_rpn_cls: 0.001317  loss_rpn_loc: 0.00814  time: 0.4305  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:51:04 d2.utils.events]: \u001b[0m eta: 4:38:06  iter: 1059  total_loss: 0.1355  loss_cls: 0.0943  loss_box_reg: 0.03319  loss_rpn_cls: 0.0009457  loss_rpn_loc: 0.00706  time: 0.4305  data_time: 0.0074  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:51:13 d2.utils.events]: \u001b[0m eta: 4:38:02  iter: 1079  total_loss: 0.1561  loss_cls: 0.1069  loss_box_reg: 0.03557  loss_rpn_cls: 0.001056  loss_rpn_loc: 0.007125  time: 0.4306  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:51:21 d2.utils.events]: \u001b[0m eta: 4:37:54  iter: 1099  total_loss: 0.1807  loss_cls: 0.1303  loss_box_reg: 0.04565  loss_rpn_cls: 0.0008911  loss_rpn_loc: 0.006407  time: 0.4304  data_time: 0.0058  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:51:30 d2.utils.events]: \u001b[0m eta: 4:37:49  iter: 1119  total_loss: 0.1793  loss_cls: 0.1257  loss_box_reg: 0.04472  loss_rpn_cls: 0.000658  loss_rpn_loc: 0.006549  time: 0.4305  data_time: 0.0066  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:51:39 d2.utils.events]: \u001b[0m eta: 4:37:50  iter: 1139  total_loss: 0.1569  loss_cls: 0.1045  loss_box_reg: 0.03739  loss_rpn_cls: 0.001133  loss_rpn_loc: 0.005509  time: 0.4307  data_time: 0.0065  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:51:48 d2.utils.events]: \u001b[0m eta: 4:37:39  iter: 1159  total_loss: 0.1552  loss_cls: 0.107  loss_box_reg: 0.03717  loss_rpn_cls: 0.0007079  loss_rpn_loc: 0.007668  time: 0.4310  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:51:57 d2.utils.events]: \u001b[0m eta: 4:38:02  iter: 1179  total_loss: 0.1672  loss_cls: 0.1173  loss_box_reg: 0.03803  loss_rpn_cls: 0.0007174  loss_rpn_loc: 0.006675  time: 0.4314  data_time: 0.0124  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:52:06 d2.utils.events]: \u001b[0m eta: 4:37:53  iter: 1199  total_loss: 0.1378  loss_cls: 0.09487  loss_box_reg: 0.03326  loss_rpn_cls: 0.001093  loss_rpn_loc: 0.005393  time: 0.4317  data_time: 0.0073  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:52:14 d2.utils.events]: \u001b[0m eta: 4:37:34  iter: 1219  total_loss: 0.1185  loss_cls: 0.08329  loss_box_reg: 0.02691  loss_rpn_cls: 0.0004744  loss_rpn_loc: 0.006258  time: 0.4314  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/21 23:52:24 d2.utils.events]: \u001b[0m eta: 4:37:38  iter: 1239  total_loss: 0.1191  loss_cls: 0.08362  loss_box_reg: 0.02577  loss_rpn_cls: 0.0008107  loss_rpn_loc: 0.006172  time: 0.4319  data_time: 0.0069  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:52:33 d2.utils.events]: \u001b[0m eta: 4:37:31  iter: 1259  total_loss: 0.1302  loss_cls: 0.08886  loss_box_reg: 0.03367  loss_rpn_cls: 0.001026  loss_rpn_loc: 0.007664  time: 0.4319  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:52:42 d2.utils.events]: \u001b[0m eta: 4:37:19  iter: 1279  total_loss: 0.1532  loss_cls: 0.1048  loss_box_reg: 0.0399  loss_rpn_cls: 0.001559  loss_rpn_loc: 0.006812  time: 0.4322  data_time: 0.0070  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:52:51 d2.utils.events]: \u001b[0m eta: 4:37:12  iter: 1299  total_loss: 0.1325  loss_cls: 0.09292  loss_box_reg: 0.03188  loss_rpn_cls: 0.0008813  loss_rpn_loc: 0.006295  time: 0.4324  data_time: 0.0073  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:53:00 d2.utils.events]: \u001b[0m eta: 4:37:03  iter: 1319  total_loss: 0.1311  loss_cls: 0.0858  loss_box_reg: 0.03398  loss_rpn_cls: 0.0005013  loss_rpn_loc: 0.006803  time: 0.4326  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:53:09 d2.utils.events]: \u001b[0m eta: 4:37:13  iter: 1339  total_loss: 0.1253  loss_cls: 0.08056  loss_box_reg: 0.0325  loss_rpn_cls: 0.0007632  loss_rpn_loc: 0.006385  time: 0.4328  data_time: 0.0067  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:53:18 d2.utils.events]: \u001b[0m eta: 4:37:58  iter: 1359  total_loss: 0.1424  loss_cls: 0.09964  loss_box_reg: 0.03564  loss_rpn_cls: 0.0006207  loss_rpn_loc: 0.006447  time: 0.4330  data_time: 0.0068  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:53:27 d2.utils.events]: \u001b[0m eta: 4:38:15  iter: 1379  total_loss: 0.1462  loss_cls: 0.1046  loss_box_reg: 0.03724  loss_rpn_cls: 0.0005593  loss_rpn_loc: 0.006755  time: 0.4332  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:53:36 d2.utils.events]: \u001b[0m eta: 4:37:46  iter: 1399  total_loss: 0.1313  loss_cls: 0.09212  loss_box_reg: 0.03107  loss_rpn_cls: 0.0004951  loss_rpn_loc: 0.005735  time: 0.4332  data_time: 0.0067  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:53:45 d2.utils.events]: \u001b[0m eta: 4:38:06  iter: 1419  total_loss: 0.1228  loss_cls: 0.08416  loss_box_reg: 0.02979  loss_rpn_cls: 0.000639  loss_rpn_loc: 0.006167  time: 0.4334  data_time: 0.0077  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:53:54 d2.utils.events]: \u001b[0m eta: 4:38:16  iter: 1439  total_loss: 0.1242  loss_cls: 0.08485  loss_box_reg: 0.02913  loss_rpn_cls: 0.0004319  loss_rpn_loc: 0.005518  time: 0.4338  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:54:02 d2.utils.events]: \u001b[0m eta: 4:38:10  iter: 1459  total_loss: 0.1127  loss_cls: 0.08126  loss_box_reg: 0.03028  loss_rpn_cls: 0.0004786  loss_rpn_loc: 0.0057  time: 0.4336  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:54:11 d2.utils.events]: \u001b[0m eta: 4:38:25  iter: 1479  total_loss: 0.1325  loss_cls: 0.08792  loss_box_reg: 0.03463  loss_rpn_cls: 0.0004932  loss_rpn_loc: 0.006837  time: 0.4336  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:54:20 d2.utils.events]: \u001b[0m eta: 4:38:18  iter: 1499  total_loss: 0.1176  loss_cls: 0.08052  loss_box_reg: 0.02863  loss_rpn_cls: 0.0009512  loss_rpn_loc: 0.007271  time: 0.4338  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:54:29 d2.utils.events]: \u001b[0m eta: 4:38:23  iter: 1519  total_loss: 0.1155  loss_cls: 0.07508  loss_box_reg: 0.03  loss_rpn_cls: 0.0008552  loss_rpn_loc: 0.006808  time: 0.4341  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:54:39 d2.utils.events]: \u001b[0m eta: 4:38:14  iter: 1539  total_loss: 0.117  loss_cls: 0.06988  loss_box_reg: 0.03598  loss_rpn_cls: 0.000447  loss_rpn_loc: 0.0059  time: 0.4346  data_time: 0.0059  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:54:48 d2.utils.events]: \u001b[0m eta: 4:38:08  iter: 1559  total_loss: 0.1187  loss_cls: 0.07548  loss_box_reg: 0.02923  loss_rpn_cls: 0.0003873  loss_rpn_loc: 0.006438  time: 0.4348  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:54:57 d2.utils.events]: \u001b[0m eta: 4:38:04  iter: 1579  total_loss: 0.1262  loss_cls: 0.08045  loss_box_reg: 0.03333  loss_rpn_cls: 0.0005646  loss_rpn_loc: 0.005971  time: 0.4350  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:55:06 d2.utils.events]: \u001b[0m eta: 4:38:08  iter: 1599  total_loss: 0.1309  loss_cls: 0.08586  loss_box_reg: 0.03505  loss_rpn_cls: 0.0007322  loss_rpn_loc: 0.007259  time: 0.4351  data_time: 0.0059  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:55:14 d2.utils.events]: \u001b[0m eta: 4:37:38  iter: 1619  total_loss: 0.1163  loss_cls: 0.07715  loss_box_reg: 0.03026  loss_rpn_cls: 0.0006791  loss_rpn_loc: 0.007328  time: 0.4348  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:55:23 d2.utils.events]: \u001b[0m eta: 4:37:33  iter: 1639  total_loss: 0.1185  loss_cls: 0.08086  loss_box_reg: 0.03035  loss_rpn_cls: 0.001034  loss_rpn_loc: 0.007458  time: 0.4349  data_time: 0.0060  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:55:32 d2.utils.events]: \u001b[0m eta: 4:37:21  iter: 1659  total_loss: 0.1069  loss_cls: 0.06818  loss_box_reg: 0.03069  loss_rpn_cls: 0.0008618  loss_rpn_loc: 0.00606  time: 0.4348  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:55:41 d2.utils.events]: \u001b[0m eta: 4:37:27  iter: 1679  total_loss: 0.1169  loss_cls: 0.07467  loss_box_reg: 0.02925  loss_rpn_cls: 0.0005144  loss_rpn_loc: 0.0076  time: 0.4350  data_time: 0.0065  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:55:50 d2.utils.events]: \u001b[0m eta: 4:37:53  iter: 1699  total_loss: 0.1235  loss_cls: 0.0796  loss_box_reg: 0.03463  loss_rpn_cls: 0.0009073  loss_rpn_loc: 0.007388  time: 0.4355  data_time: 0.0074  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:55:59 d2.utils.events]: \u001b[0m eta: 4:38:07  iter: 1719  total_loss: 0.1121  loss_cls: 0.07168  loss_box_reg: 0.03315  loss_rpn_cls: 0.0009183  loss_rpn_loc: 0.005787  time: 0.4354  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:56:08 d2.utils.events]: \u001b[0m eta: 4:38:51  iter: 1739  total_loss: 0.1131  loss_cls: 0.07066  loss_box_reg: 0.03283  loss_rpn_cls: 0.0006488  loss_rpn_loc: 0.007257  time: 0.4355  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:56:16 d2.utils.events]: \u001b[0m eta: 4:39:11  iter: 1759  total_loss: 0.1092  loss_cls: 0.07063  loss_box_reg: 0.03086  loss_rpn_cls: 0.0007881  loss_rpn_loc: 0.008551  time: 0.4355  data_time: 0.0076  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:56:25 d2.utils.events]: \u001b[0m eta: 4:39:06  iter: 1779  total_loss: 0.1127  loss_cls: 0.0693  loss_box_reg: 0.02979  loss_rpn_cls: 0.0005319  loss_rpn_loc: 0.007164  time: 0.4353  data_time: 0.0060  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:56:34 d2.utils.events]: \u001b[0m eta: 4:39:01  iter: 1799  total_loss: 0.1012  loss_cls: 0.0654  loss_box_reg: 0.031  loss_rpn_cls: 0.0006554  loss_rpn_loc: 0.005913  time: 0.4352  data_time: 0.0066  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:56:42 d2.utils.events]: \u001b[0m eta: 4:38:57  iter: 1819  total_loss: 0.1035  loss_cls: 0.06914  loss_box_reg: 0.02995  loss_rpn_cls: 0.0003973  loss_rpn_loc: 0.005568  time: 0.4350  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:56:51 d2.utils.events]: \u001b[0m eta: 4:38:40  iter: 1839  total_loss: 0.1249  loss_cls: 0.07911  loss_box_reg: 0.03489  loss_rpn_cls: 0.0005762  loss_rpn_loc: 0.006387  time: 0.4350  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:56:59 d2.utils.events]: \u001b[0m eta: 4:38:31  iter: 1859  total_loss: 0.1038  loss_cls: 0.06333  loss_box_reg: 0.03503  loss_rpn_cls: 0.0002386  loss_rpn_loc: 0.006101  time: 0.4347  data_time: 0.0060  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:57:07 d2.utils.events]: \u001b[0m eta: 4:38:23  iter: 1879  total_loss: 0.1152  loss_cls: 0.07314  loss_box_reg: 0.03612  loss_rpn_cls: 0.0004138  loss_rpn_loc: 0.005175  time: 0.4347  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:57:16 d2.utils.events]: \u001b[0m eta: 4:38:13  iter: 1899  total_loss: 0.1093  loss_cls: 0.07211  loss_box_reg: 0.03276  loss_rpn_cls: 0.0003667  loss_rpn_loc: 0.005795  time: 0.4345  data_time: 0.0066  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:57:25 d2.utils.events]: \u001b[0m eta: 4:38:05  iter: 1919  total_loss: 0.1045  loss_cls: 0.0637  loss_box_reg: 0.03307  loss_rpn_cls: 0.0006326  loss_rpn_loc: 0.006815  time: 0.4345  data_time: 0.0065  lr: 0.02  max_mem: 6293M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/21 23:57:34 d2.utils.events]: \u001b[0m eta: 4:38:04  iter: 1939  total_loss: 0.08775  loss_cls: 0.05513  loss_box_reg: 0.02703  loss_rpn_cls: 0.0005604  loss_rpn_loc: 0.005444  time: 0.4346  data_time: 0.0065  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:57:42 d2.utils.events]: \u001b[0m eta: 4:38:06  iter: 1959  total_loss: 0.1017  loss_cls: 0.06009  loss_box_reg: 0.03061  loss_rpn_cls: 0.0004905  loss_rpn_loc: 0.006911  time: 0.4346  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/21 23:57:52 d2.utils.events]: \u001b[0m eta: 4:37:49  iter: 1979  total_loss: 0.09166  loss_cls: 0.05517  loss_box_reg: 0.02744  loss_rpn_cls: 0.001497  loss_rpn_loc: 0.008883  time: 0.4346  data_time: 0.0074  lr: 0.02  max_mem: 6293M\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[04/21 23:58:02 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[04/21 23:58:02 d2.data.datasets.coco]: \u001b[0mLoaded 3333 images in COCO format from /home/zack/datasets/manufacturer_identification/data/coco/val.json\n",
      "\u001b[32m[04/21 23:58:02 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[04/21 23:58:02 d2.data.common]: \u001b[0mSerializing 3333 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[04/21 23:58:02 d2.data.common]: \u001b[0mSerialized dataset takes 0.85 MiB\n",
      "\u001b[32m[04/21 23:58:02 d2.evaluation.evaluator]: \u001b[0mStart inference on 3333 batches\n",
      "\u001b[32m[04/21 23:58:03 d2.evaluation.evaluator]: \u001b[0mInference done 11/3333. Dataloading: 0.0012 s/iter. Inference: 0.0318 s/iter. Eval: 0.0001 s/iter. Total: 0.0332 s/iter. ETA=0:01:50\n",
      "\u001b[32m[04/21 23:58:08 d2.evaluation.evaluator]: \u001b[0mInference done 162/3333. Dataloading: 0.0008 s/iter. Inference: 0.0321 s/iter. Eval: 0.0001 s/iter. Total: 0.0332 s/iter. ETA=0:01:45\n",
      "\u001b[32m[04/21 23:58:13 d2.evaluation.evaluator]: \u001b[0mInference done 311/3333. Dataloading: 0.0008 s/iter. Inference: 0.0324 s/iter. Eval: 0.0001 s/iter. Total: 0.0334 s/iter. ETA=0:01:40\n",
      "\u001b[32m[04/21 23:58:19 d2.evaluation.evaluator]: \u001b[0mInference done 457/3333. Dataloading: 0.0008 s/iter. Inference: 0.0327 s/iter. Eval: 0.0001 s/iter. Total: 0.0337 s/iter. ETA=0:01:36\n",
      "\u001b[32m[04/21 23:58:24 d2.evaluation.evaluator]: \u001b[0mInference done 609/3333. Dataloading: 0.0008 s/iter. Inference: 0.0325 s/iter. Eval: 0.0001 s/iter. Total: 0.0335 s/iter. ETA=0:01:31\n",
      "\u001b[32m[04/21 23:58:29 d2.evaluation.evaluator]: \u001b[0mInference done 762/3333. Dataloading: 0.0008 s/iter. Inference: 0.0324 s/iter. Eval: 0.0001 s/iter. Total: 0.0334 s/iter. ETA=0:01:25\n",
      "\u001b[32m[04/21 23:58:34 d2.evaluation.evaluator]: \u001b[0mInference done 911/3333. Dataloading: 0.0008 s/iter. Inference: 0.0325 s/iter. Eval: 0.0001 s/iter. Total: 0.0334 s/iter. ETA=0:01:20\n",
      "\u001b[32m[04/21 23:58:39 d2.evaluation.evaluator]: \u001b[0mInference done 1052/3333. Dataloading: 0.0008 s/iter. Inference: 0.0327 s/iter. Eval: 0.0001 s/iter. Total: 0.0337 s/iter. ETA=0:01:16\n",
      "\u001b[32m[04/21 23:58:44 d2.evaluation.evaluator]: \u001b[0mInference done 1196/3333. Dataloading: 0.0008 s/iter. Inference: 0.0329 s/iter. Eval: 0.0001 s/iter. Total: 0.0339 s/iter. ETA=0:01:12\n",
      "\u001b[32m[04/21 23:58:49 d2.evaluation.evaluator]: \u001b[0mInference done 1333/3333. Dataloading: 0.0008 s/iter. Inference: 0.0331 s/iter. Eval: 0.0001 s/iter. Total: 0.0341 s/iter. ETA=0:01:08\n",
      "\u001b[32m[04/21 23:58:54 d2.evaluation.evaluator]: \u001b[0mInference done 1481/3333. Dataloading: 0.0008 s/iter. Inference: 0.0331 s/iter. Eval: 0.0001 s/iter. Total: 0.0341 s/iter. ETA=0:01:03\n",
      "\u001b[32m[04/21 23:58:59 d2.evaluation.evaluator]: \u001b[0mInference done 1627/3333. Dataloading: 0.0008 s/iter. Inference: 0.0331 s/iter. Eval: 0.0001 s/iter. Total: 0.0341 s/iter. ETA=0:00:58\n",
      "\u001b[32m[04/21 23:59:04 d2.evaluation.evaluator]: \u001b[0mInference done 1777/3333. Dataloading: 0.0008 s/iter. Inference: 0.0331 s/iter. Eval: 0.0001 s/iter. Total: 0.0341 s/iter. ETA=0:00:52\n",
      "\u001b[32m[04/21 23:59:09 d2.evaluation.evaluator]: \u001b[0mInference done 1925/3333. Dataloading: 0.0008 s/iter. Inference: 0.0330 s/iter. Eval: 0.0001 s/iter. Total: 0.0340 s/iter. ETA=0:00:47\n",
      "\u001b[32m[04/21 23:59:14 d2.evaluation.evaluator]: \u001b[0mInference done 2073/3333. Dataloading: 0.0008 s/iter. Inference: 0.0330 s/iter. Eval: 0.0001 s/iter. Total: 0.0340 s/iter. ETA=0:00:42\n",
      "\u001b[32m[04/21 23:59:19 d2.evaluation.evaluator]: \u001b[0mInference done 2223/3333. Dataloading: 0.0008 s/iter. Inference: 0.0330 s/iter. Eval: 0.0001 s/iter. Total: 0.0340 s/iter. ETA=0:00:37\n",
      "\u001b[32m[04/21 23:59:24 d2.evaluation.evaluator]: \u001b[0mInference done 2374/3333. Dataloading: 0.0008 s/iter. Inference: 0.0329 s/iter. Eval: 0.0001 s/iter. Total: 0.0339 s/iter. ETA=0:00:32\n",
      "\u001b[32m[04/21 23:59:29 d2.evaluation.evaluator]: \u001b[0mInference done 2524/3333. Dataloading: 0.0008 s/iter. Inference: 0.0329 s/iter. Eval: 0.0001 s/iter. Total: 0.0339 s/iter. ETA=0:00:27\n",
      "\u001b[32m[04/21 23:59:34 d2.evaluation.evaluator]: \u001b[0mInference done 2674/3333. Dataloading: 0.0008 s/iter. Inference: 0.0329 s/iter. Eval: 0.0001 s/iter. Total: 0.0339 s/iter. ETA=0:00:22\n",
      "\u001b[32m[04/21 23:59:39 d2.evaluation.evaluator]: \u001b[0mInference done 2823/3333. Dataloading: 0.0008 s/iter. Inference: 0.0329 s/iter. Eval: 0.0001 s/iter. Total: 0.0339 s/iter. ETA=0:00:17\n",
      "\u001b[32m[04/21 23:59:44 d2.evaluation.evaluator]: \u001b[0mInference done 2974/3333. Dataloading: 0.0008 s/iter. Inference: 0.0328 s/iter. Eval: 0.0001 s/iter. Total: 0.0338 s/iter. ETA=0:00:12\n",
      "\u001b[32m[04/21 23:59:49 d2.evaluation.evaluator]: \u001b[0mInference done 3125/3333. Dataloading: 0.0008 s/iter. Inference: 0.0328 s/iter. Eval: 0.0001 s/iter. Total: 0.0338 s/iter. ETA=0:00:07\n",
      "\u001b[32m[04/21 23:59:54 d2.evaluation.evaluator]: \u001b[0mInference done 3273/3333. Dataloading: 0.0008 s/iter. Inference: 0.0328 s/iter. Eval: 0.0001 s/iter. Total: 0.0338 s/iter. ETA=0:00:02\n",
      "\u001b[32m[04/21 23:59:56 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:01:52.637943 (0.033846 s / iter per device, on 1 devices)\n",
      "\u001b[32m[04/21 23:59:56 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:49 (0.032803 s / iter per device, on 1 devices)\n",
      "\u001b[32m[04/21 23:59:56 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[04/21 23:59:56 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output_class_balanced/inference/coco_instances_results.json\n",
      "\u001b[32m[04/21 23:59:56 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.08s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[04/21 23:59:56 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[04/21 23:59:58 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 1.24 seconds.\n",
      "\u001b[32m[04/21 23:59:58 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[04/21 23:59:58 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.18 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.446\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.568\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.545\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.446\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.762\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.763\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.763\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.763\n",
      "\u001b[32m[04/21 23:59:58 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 44.551 | 56.752 | 54.504 |  nan  | 0.000 | 44.555 |\n",
      "\u001b[32m[04/21 23:59:58 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[04/21 23:59:58 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category                 | AP     | category             | AP     | category             | AP     |\n",
      "|:-------------------------|:-------|:---------------------|:-------|:---------------------|:-------|\n",
      "| ATR                      | 37.300 | Airbus               | 25.227 | Antonov              | 52.781 |\n",
      "| Beechcraft               | 49.140 | Boeing               | 48.508 | Bombardier Aerospace | 27.748 |\n",
      "| British Aerospace        | 33.760 | Canadair             | 35.168 | Cessna               | 47.947 |\n",
      "| Cirrus Aircraft          | 77.964 | Dassault Aviation    | 68.455 | Dornier              | 52.945 |\n",
      "| Douglas Aircraft Company | 27.765 | Embraer              | 31.120 | Eurofighter          | 67.695 |\n",
      "| Fairchild                | 58.050 | Fokker               | 7.917  | Gulfstream Aerospace | 46.771 |\n",
      "| Ilyushin                 | 53.327 | Lockheed Corporation | 26.119 | Lockheed Martin      | 71.047 |\n",
      "| McDonnell Douglas        | 28.171 | Panavia              | 60.610 | Piper                | 42.421 |\n",
      "| Robin                    | 77.965 | Saab                 | 43.275 | Supermarine          | 51.625 |\n",
      "| Tupolev                  | 27.687 | Yakovlev             | 21.927 | de Havilland         | 36.091 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/21 23:59:58 d2.engine.defaults]: \u001b[0mEvaluation results for manu_id_val in csv format:\n",
      "\u001b[32m[04/21 23:59:58 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[04/21 23:59:58 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[04/21 23:59:58 d2.evaluation.testing]: \u001b[0mcopypaste: 44.5509,56.7519,54.5036,nan,0.0000,44.5553\n",
      "\u001b[32m[04/21 23:59:58 d2.utils.events]: \u001b[0m eta: 4:37:31  iter: 1999  total_loss: 0.0934  loss_cls: 0.05469  loss_box_reg: 0.0301  loss_rpn_cls: 0.0007762  loss_rpn_loc: 0.006234  time: 0.4346  data_time: 0.0073  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:00:06 d2.utils.events]: \u001b[0m eta: 4:37:11  iter: 2019  total_loss: 0.09154  loss_cls: 0.05538  loss_box_reg: 0.02866  loss_rpn_cls: 0.0005805  loss_rpn_loc: 0.006182  time: 0.4344  data_time: 0.0060  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:00:15 d2.utils.events]: \u001b[0m eta: 4:36:54  iter: 2039  total_loss: 0.08754  loss_cls: 0.05652  loss_box_reg: 0.02482  loss_rpn_cls: 0.0005258  loss_rpn_loc: 0.005976  time: 0.4343  data_time: 0.0074  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:00:23 d2.utils.events]: \u001b[0m eta: 4:36:49  iter: 2059  total_loss: 0.08762  loss_cls: 0.05396  loss_box_reg: 0.02763  loss_rpn_cls: 0.0003595  loss_rpn_loc: 0.004908  time: 0.4341  data_time: 0.0068  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:00:32 d2.utils.events]: \u001b[0m eta: 4:36:40  iter: 2079  total_loss: 0.08474  loss_cls: 0.05041  loss_box_reg: 0.02864  loss_rpn_cls: 0.0004807  loss_rpn_loc: 0.005245  time: 0.4341  data_time: 0.0076  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:00:41 d2.utils.events]: \u001b[0m eta: 4:36:43  iter: 2099  total_loss: 0.09897  loss_cls: 0.0573  loss_box_reg: 0.03403  loss_rpn_cls: 0.0002099  loss_rpn_loc: 0.006111  time: 0.4343  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:00:50 d2.utils.events]: \u001b[0m eta: 4:36:31  iter: 2119  total_loss: 0.09668  loss_cls: 0.05438  loss_box_reg: 0.03314  loss_rpn_cls: 0.0003233  loss_rpn_loc: 0.005572  time: 0.4343  data_time: 0.0065  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:00:59 d2.utils.events]: \u001b[0m eta: 4:36:20  iter: 2139  total_loss: 0.1051  loss_cls: 0.06512  loss_box_reg: 0.03056  loss_rpn_cls: 0.0005532  loss_rpn_loc: 0.006778  time: 0.4344  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:01:08 d2.utils.events]: \u001b[0m eta: 4:36:14  iter: 2159  total_loss: 0.09672  loss_cls: 0.06119  loss_box_reg: 0.03084  loss_rpn_cls: 0.001029  loss_rpn_loc: 0.008295  time: 0.4344  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:01:16 d2.utils.events]: \u001b[0m eta: 4:36:05  iter: 2179  total_loss: 0.1066  loss_cls: 0.05723  loss_box_reg: 0.03923  loss_rpn_cls: 0.0005702  loss_rpn_loc: 0.008687  time: 0.4344  data_time: 0.0065  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:01:25 d2.utils.events]: \u001b[0m eta: 4:35:53  iter: 2199  total_loss: 0.09374  loss_cls: 0.05661  loss_box_reg: 0.02946  loss_rpn_cls: 0.000501  loss_rpn_loc: 0.005667  time: 0.4344  data_time: 0.0069  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:01:34 d2.utils.events]: \u001b[0m eta: 4:35:59  iter: 2219  total_loss: 0.1066  loss_cls: 0.06207  loss_box_reg: 0.03435  loss_rpn_cls: 0.0006339  loss_rpn_loc: 0.006566  time: 0.4344  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:01:43 d2.utils.events]: \u001b[0m eta: 4:35:46  iter: 2239  total_loss: 0.1047  loss_cls: 0.06269  loss_box_reg: 0.03324  loss_rpn_cls: 0.0004256  loss_rpn_loc: 0.007464  time: 0.4344  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:01:52 d2.utils.events]: \u001b[0m eta: 4:35:42  iter: 2259  total_loss: 0.08597  loss_cls: 0.0515  loss_box_reg: 0.03164  loss_rpn_cls: 0.000228  loss_rpn_loc: 0.005767  time: 0.4345  data_time: 0.0067  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:02:00 d2.utils.events]: \u001b[0m eta: 4:35:28  iter: 2279  total_loss: 0.0853  loss_cls: 0.05165  loss_box_reg: 0.02964  loss_rpn_cls: 0.0003589  loss_rpn_loc: 0.005587  time: 0.4346  data_time: 0.0069  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:02:09 d2.utils.events]: \u001b[0m eta: 4:35:16  iter: 2299  total_loss: 0.101  loss_cls: 0.05923  loss_box_reg: 0.03339  loss_rpn_cls: 0.0002396  loss_rpn_loc: 0.005139  time: 0.4345  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:02:18 d2.utils.events]: \u001b[0m eta: 4:35:01  iter: 2319  total_loss: 0.08681  loss_cls: 0.05201  loss_box_reg: 0.03071  loss_rpn_cls: 0.0007308  loss_rpn_loc: 0.005526  time: 0.4346  data_time: 0.0065  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:02:27 d2.utils.events]: \u001b[0m eta: 4:34:52  iter: 2339  total_loss: 0.09148  loss_cls: 0.04267  loss_box_reg: 0.03255  loss_rpn_cls: 0.0005766  loss_rpn_loc: 0.006646  time: 0.4346  data_time: 0.0067  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:02:36 d2.utils.events]: \u001b[0m eta: 4:34:32  iter: 2359  total_loss: 0.09747  loss_cls: 0.05951  loss_box_reg: 0.02987  loss_rpn_cls: 0.0004417  loss_rpn_loc: 0.00664  time: 0.4347  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:02:45 d2.utils.events]: \u001b[0m eta: 4:34:24  iter: 2379  total_loss: 0.09212  loss_cls: 0.04976  loss_box_reg: 0.02912  loss_rpn_cls: 0.0005538  loss_rpn_loc: 0.006968  time: 0.4348  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:02:53 d2.utils.events]: \u001b[0m eta: 4:34:19  iter: 2399  total_loss: 0.08103  loss_cls: 0.04866  loss_box_reg: 0.02742  loss_rpn_cls: 0.0003661  loss_rpn_loc: 0.005326  time: 0.4348  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:03:02 d2.utils.events]: \u001b[0m eta: 4:34:06  iter: 2419  total_loss: 0.08947  loss_cls: 0.0483  loss_box_reg: 0.03089  loss_rpn_cls: 0.0002787  loss_rpn_loc: 0.006527  time: 0.4348  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:03:11 d2.utils.events]: \u001b[0m eta: 4:33:31  iter: 2439  total_loss: 0.08262  loss_cls: 0.04536  loss_box_reg: 0.03164  loss_rpn_cls: 0.0003321  loss_rpn_loc: 0.005381  time: 0.4348  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:03:20 d2.utils.events]: \u001b[0m eta: 4:33:49  iter: 2459  total_loss: 0.08849  loss_cls: 0.05305  loss_box_reg: 0.02775  loss_rpn_cls: 0.0004144  loss_rpn_loc: 0.005774  time: 0.4348  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:03:28 d2.utils.events]: \u001b[0m eta: 4:33:41  iter: 2479  total_loss: 0.0923  loss_cls: 0.04709  loss_box_reg: 0.03341  loss_rpn_cls: 0.0001614  loss_rpn_loc: 0.00595  time: 0.4348  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:03:37 d2.utils.events]: \u001b[0m eta: 4:33:20  iter: 2499  total_loss: 0.08732  loss_cls: 0.04196  loss_box_reg: 0.0339  loss_rpn_cls: 0.0004196  loss_rpn_loc: 0.006463  time: 0.4348  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:03:45 d2.utils.events]: \u001b[0m eta: 4:32:47  iter: 2519  total_loss: 0.08762  loss_cls: 0.04844  loss_box_reg: 0.02874  loss_rpn_cls: 0.0007619  loss_rpn_loc: 0.006556  time: 0.4347  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:03:54 d2.utils.events]: \u001b[0m eta: 4:32:09  iter: 2539  total_loss: 0.07301  loss_cls: 0.03749  loss_box_reg: 0.02765  loss_rpn_cls: 0.0003876  loss_rpn_loc: 0.005124  time: 0.4347  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:04:03 d2.utils.events]: \u001b[0m eta: 4:31:43  iter: 2559  total_loss: 0.07764  loss_cls: 0.04291  loss_box_reg: 0.03132  loss_rpn_cls: 0.00039  loss_rpn_loc: 0.005439  time: 0.4346  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:04:11 d2.utils.events]: \u001b[0m eta: 4:31:06  iter: 2579  total_loss: 0.08759  loss_cls: 0.04562  loss_box_reg: 0.02909  loss_rpn_cls: 0.0005086  loss_rpn_loc: 0.007125  time: 0.4345  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:04:19 d2.utils.events]: \u001b[0m eta: 4:30:24  iter: 2599  total_loss: 0.07932  loss_cls: 0.04759  loss_box_reg: 0.02651  loss_rpn_cls: 0.0003736  loss_rpn_loc: 0.005954  time: 0.4343  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:04:28 d2.utils.events]: \u001b[0m eta: 4:30:15  iter: 2619  total_loss: 0.08133  loss_cls: 0.04429  loss_box_reg: 0.02777  loss_rpn_cls: 0.0004239  loss_rpn_loc: 0.0049  time: 0.4343  data_time: 0.0065  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:04:37 d2.utils.events]: \u001b[0m eta: 4:30:06  iter: 2639  total_loss: 0.07416  loss_cls: 0.03389  loss_box_reg: 0.03273  loss_rpn_cls: 0.000383  loss_rpn_loc: 0.005468  time: 0.4344  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/22 00:04:46 d2.utils.events]: \u001b[0m eta: 4:29:58  iter: 2659  total_loss: 0.0726  loss_cls: 0.03936  loss_box_reg: 0.03152  loss_rpn_cls: 0.0005355  loss_rpn_loc: 0.006098  time: 0.4344  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:04:55 d2.utils.events]: \u001b[0m eta: 4:30:11  iter: 2679  total_loss: 0.07492  loss_cls: 0.04262  loss_box_reg: 0.02656  loss_rpn_cls: 0.0001917  loss_rpn_loc: 0.00603  time: 0.4346  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:05:04 d2.utils.events]: \u001b[0m eta: 4:29:38  iter: 2699  total_loss: 0.07113  loss_cls: 0.03649  loss_box_reg: 0.02711  loss_rpn_cls: 0.0002083  loss_rpn_loc: 0.00516  time: 0.4347  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:05:12 d2.utils.events]: \u001b[0m eta: 4:29:13  iter: 2719  total_loss: 0.07523  loss_cls: 0.03771  loss_box_reg: 0.02796  loss_rpn_cls: 0.0005268  loss_rpn_loc: 0.006775  time: 0.4346  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:05:21 d2.utils.events]: \u001b[0m eta: 4:28:48  iter: 2739  total_loss: 0.07089  loss_cls: 0.03529  loss_box_reg: 0.02916  loss_rpn_cls: 0.000732  loss_rpn_loc: 0.006533  time: 0.4345  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:05:29 d2.utils.events]: \u001b[0m eta: 4:28:09  iter: 2759  total_loss: 0.07859  loss_cls: 0.03783  loss_box_reg: 0.02653  loss_rpn_cls: 0.0003623  loss_rpn_loc: 0.004929  time: 0.4344  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:05:38 d2.utils.events]: \u001b[0m eta: 4:28:00  iter: 2779  total_loss: 0.07667  loss_cls: 0.0397  loss_box_reg: 0.02857  loss_rpn_cls: 0.000643  loss_rpn_loc: 0.006339  time: 0.4344  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:05:47 d2.utils.events]: \u001b[0m eta: 4:28:08  iter: 2799  total_loss: 0.07993  loss_cls: 0.04345  loss_box_reg: 0.02886  loss_rpn_cls: 0.0004618  loss_rpn_loc: 0.005851  time: 0.4345  data_time: 0.0066  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:05:56 d2.utils.events]: \u001b[0m eta: 4:28:10  iter: 2819  total_loss: 0.0725  loss_cls: 0.03963  loss_box_reg: 0.02943  loss_rpn_cls: 0.0002854  loss_rpn_loc: 0.005917  time: 0.4345  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:06:04 d2.utils.events]: \u001b[0m eta: 4:28:04  iter: 2839  total_loss: 0.07712  loss_cls: 0.03623  loss_box_reg: 0.029  loss_rpn_cls: 0.0003308  loss_rpn_loc: 0.005105  time: 0.4345  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:06:13 d2.utils.events]: \u001b[0m eta: 4:27:59  iter: 2859  total_loss: 0.07176  loss_cls: 0.03493  loss_box_reg: 0.03017  loss_rpn_cls: 0.0003198  loss_rpn_loc: 0.005401  time: 0.4345  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:06:21 d2.utils.events]: \u001b[0m eta: 4:27:41  iter: 2879  total_loss: 0.07459  loss_cls: 0.03813  loss_box_reg: 0.02643  loss_rpn_cls: 0.0002422  loss_rpn_loc: 0.006484  time: 0.4343  data_time: 0.0060  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:06:29 d2.utils.events]: \u001b[0m eta: 4:27:24  iter: 2899  total_loss: 0.05503  loss_cls: 0.02718  loss_box_reg: 0.0235  loss_rpn_cls: 0.0004349  loss_rpn_loc: 0.006473  time: 0.4341  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:06:38 d2.utils.events]: \u001b[0m eta: 4:26:42  iter: 2919  total_loss: 0.06255  loss_cls: 0.02952  loss_box_reg: 0.02355  loss_rpn_cls: 0.0003524  loss_rpn_loc: 0.005901  time: 0.4340  data_time: 0.0060  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:06:47 d2.utils.events]: \u001b[0m eta: 4:26:33  iter: 2939  total_loss: 0.0627  loss_cls: 0.03005  loss_box_reg: 0.02518  loss_rpn_cls: 0.0004028  loss_rpn_loc: 0.006796  time: 0.4341  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:06:55 d2.utils.events]: \u001b[0m eta: 4:26:06  iter: 2959  total_loss: 0.06919  loss_cls: 0.03332  loss_box_reg: 0.02732  loss_rpn_cls: 0.0003328  loss_rpn_loc: 0.006432  time: 0.4339  data_time: 0.0072  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:07:03 d2.utils.events]: \u001b[0m eta: 4:26:00  iter: 2979  total_loss: 0.06046  loss_cls: 0.03005  loss_box_reg: 0.02283  loss_rpn_cls: 0.0001874  loss_rpn_loc: 0.006086  time: 0.4338  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[04/22 00:07:13 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[04/22 00:07:13 d2.data.datasets.coco]: \u001b[0mLoaded 3333 images in COCO format from /home/zack/datasets/manufacturer_identification/data/coco/val.json\n",
      "\u001b[32m[04/22 00:07:13 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[04/22 00:07:13 d2.data.common]: \u001b[0mSerializing 3333 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[04/22 00:07:13 d2.data.common]: \u001b[0mSerialized dataset takes 0.85 MiB\n",
      "\u001b[32m[04/22 00:07:13 d2.evaluation.evaluator]: \u001b[0mStart inference on 3333 batches\n",
      "\u001b[32m[04/22 00:07:13 d2.evaluation.evaluator]: \u001b[0mInference done 11/3333. Dataloading: 0.0007 s/iter. Inference: 0.0376 s/iter. Eval: 0.0001 s/iter. Total: 0.0384 s/iter. ETA=0:02:07\n",
      "\u001b[32m[04/22 00:07:18 d2.evaluation.evaluator]: \u001b[0mInference done 161/3333. Dataloading: 0.0007 s/iter. Inference: 0.0326 s/iter. Eval: 0.0001 s/iter. Total: 0.0336 s/iter. ETA=0:01:46\n",
      "\u001b[32m[04/22 00:07:23 d2.evaluation.evaluator]: \u001b[0mInference done 310/3333. Dataloading: 0.0008 s/iter. Inference: 0.0326 s/iter. Eval: 0.0001 s/iter. Total: 0.0336 s/iter. ETA=0:01:41\n",
      "\u001b[32m[04/22 00:07:28 d2.evaluation.evaluator]: \u001b[0mInference done 464/3333. Dataloading: 0.0008 s/iter. Inference: 0.0323 s/iter. Eval: 0.0001 s/iter. Total: 0.0332 s/iter. ETA=0:01:35\n",
      "\u001b[32m[04/22 00:07:33 d2.evaluation.evaluator]: \u001b[0mInference done 615/3333. Dataloading: 0.0008 s/iter. Inference: 0.0323 s/iter. Eval: 0.0001 s/iter. Total: 0.0332 s/iter. ETA=0:01:30\n",
      "\u001b[32m[04/22 00:07:38 d2.evaluation.evaluator]: \u001b[0mInference done 756/3333. Dataloading: 0.0008 s/iter. Inference: 0.0327 s/iter. Eval: 0.0001 s/iter. Total: 0.0336 s/iter. ETA=0:01:26\n",
      "\u001b[32m[04/22 00:07:44 d2.evaluation.evaluator]: \u001b[0mInference done 910/3333. Dataloading: 0.0008 s/iter. Inference: 0.0325 s/iter. Eval: 0.0001 s/iter. Total: 0.0335 s/iter. ETA=0:01:21\n",
      "\u001b[32m[04/22 00:07:49 d2.evaluation.evaluator]: \u001b[0mInference done 1070/3333. Dataloading: 0.0008 s/iter. Inference: 0.0322 s/iter. Eval: 0.0001 s/iter. Total: 0.0331 s/iter. ETA=0:01:14\n",
      "\u001b[32m[04/22 00:07:54 d2.evaluation.evaluator]: \u001b[0mInference done 1228/3333. Dataloading: 0.0008 s/iter. Inference: 0.0320 s/iter. Eval: 0.0001 s/iter. Total: 0.0330 s/iter. ETA=0:01:09\n",
      "\u001b[32m[04/22 00:07:59 d2.evaluation.evaluator]: \u001b[0mInference done 1383/3333. Dataloading: 0.0008 s/iter. Inference: 0.0320 s/iter. Eval: 0.0001 s/iter. Total: 0.0329 s/iter. ETA=0:01:04\n",
      "\u001b[32m[04/22 00:08:04 d2.evaluation.evaluator]: \u001b[0mInference done 1541/3333. Dataloading: 0.0008 s/iter. Inference: 0.0318 s/iter. Eval: 0.0001 s/iter. Total: 0.0328 s/iter. ETA=0:00:58\n",
      "\u001b[32m[04/22 00:08:09 d2.evaluation.evaluator]: \u001b[0mInference done 1696/3333. Dataloading: 0.0008 s/iter. Inference: 0.0318 s/iter. Eval: 0.0001 s/iter. Total: 0.0327 s/iter. ETA=0:00:53\n",
      "\u001b[32m[04/22 00:08:14 d2.evaluation.evaluator]: \u001b[0mInference done 1849/3333. Dataloading: 0.0008 s/iter. Inference: 0.0318 s/iter. Eval: 0.0001 s/iter. Total: 0.0327 s/iter. ETA=0:00:48\n",
      "\u001b[32m[04/22 00:08:19 d2.evaluation.evaluator]: \u001b[0mInference done 2010/3333. Dataloading: 0.0008 s/iter. Inference: 0.0317 s/iter. Eval: 0.0001 s/iter. Total: 0.0326 s/iter. ETA=0:00:43\n",
      "\u001b[32m[04/22 00:08:24 d2.evaluation.evaluator]: \u001b[0mInference done 2168/3333. Dataloading: 0.0008 s/iter. Inference: 0.0316 s/iter. Eval: 0.0001 s/iter. Total: 0.0326 s/iter. ETA=0:00:37\n",
      "\u001b[32m[04/22 00:08:29 d2.evaluation.evaluator]: \u001b[0mInference done 2326/3333. Dataloading: 0.0008 s/iter. Inference: 0.0316 s/iter. Eval: 0.0001 s/iter. Total: 0.0325 s/iter. ETA=0:00:32\n",
      "\u001b[32m[04/22 00:08:34 d2.evaluation.evaluator]: \u001b[0mInference done 2483/3333. Dataloading: 0.0008 s/iter. Inference: 0.0316 s/iter. Eval: 0.0001 s/iter. Total: 0.0325 s/iter. ETA=0:00:27\n",
      "\u001b[32m[04/22 00:08:39 d2.evaluation.evaluator]: \u001b[0mInference done 2640/3333. Dataloading: 0.0008 s/iter. Inference: 0.0315 s/iter. Eval: 0.0001 s/iter. Total: 0.0325 s/iter. ETA=0:00:22\n",
      "\u001b[32m[04/22 00:08:44 d2.evaluation.evaluator]: \u001b[0mInference done 2783/3333. Dataloading: 0.0008 s/iter. Inference: 0.0317 s/iter. Eval: 0.0001 s/iter. Total: 0.0326 s/iter. ETA=0:00:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/22 00:08:49 d2.evaluation.evaluator]: \u001b[0mInference done 2934/3333. Dataloading: 0.0008 s/iter. Inference: 0.0317 s/iter. Eval: 0.0001 s/iter. Total: 0.0326 s/iter. ETA=0:00:13\n",
      "\u001b[32m[04/22 00:08:54 d2.evaluation.evaluator]: \u001b[0mInference done 3084/3333. Dataloading: 0.0008 s/iter. Inference: 0.0317 s/iter. Eval: 0.0001 s/iter. Total: 0.0327 s/iter. ETA=0:00:08\n",
      "\u001b[32m[04/22 00:08:59 d2.evaluation.evaluator]: \u001b[0mInference done 3239/3333. Dataloading: 0.0008 s/iter. Inference: 0.0317 s/iter. Eval: 0.0001 s/iter. Total: 0.0326 s/iter. ETA=0:00:03\n",
      "\u001b[32m[04/22 00:09:02 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:01:48.998620 (0.032752 s / iter per device, on 1 devices)\n",
      "\u001b[32m[04/22 00:09:02 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:45 (0.031797 s / iter per device, on 1 devices)\n",
      "\u001b[32m[04/22 00:09:02 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[04/22 00:09:02 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output_class_balanced/inference/coco_instances_results.json\n",
      "\u001b[32m[04/22 00:09:02 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.02s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[04/22 00:09:02 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[04/22 00:09:04 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 1.20 seconds.\n",
      "\u001b[32m[04/22 00:09:04 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[04/22 00:09:04 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.16 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.555\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.719\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.689\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.555\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.768\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.769\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.769\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.769\n",
      "\u001b[32m[04/22 00:09:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 55.499 | 71.928 | 68.853 |  nan  | 0.000 | 55.501 |\n",
      "\u001b[32m[04/22 00:09:04 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[04/22 00:09:04 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category                 | AP     | category             | AP     | category             | AP     |\n",
      "|:-------------------------|:-------|:---------------------|:-------|:---------------------|:-------|\n",
      "| ATR                      | 75.761 | Airbus               | 38.359 | Antonov              | 72.561 |\n",
      "| Beechcraft               | 61.180 | Boeing               | 53.241 | Bombardier Aerospace | 35.437 |\n",
      "| British Aerospace        | 66.417 | Canadair             | 40.226 | Cessna               | 51.687 |\n",
      "| Cirrus Aircraft          | 68.497 | Dassault Aviation    | 68.874 | Dornier              | 59.837 |\n",
      "| Douglas Aircraft Company | 39.614 | Embraer              | 32.239 | Eurofighter          | 64.840 |\n",
      "| Fairchild                | 53.325 | Fokker               | 12.026 | Gulfstream Aerospace | 64.556 |\n",
      "| Ilyushin                 | 63.660 | Lockheed Corporation | 54.676 | Lockheed Martin      | 65.895 |\n",
      "| McDonnell Douglas        | 22.246 | Panavia              | 74.938 | Piper                | 59.580 |\n",
      "| Robin                    | 76.814 | Saab                 | 43.231 | Supermarine          | 60.122 |\n",
      "| Tupolev                  | 63.806 | Yakovlev             | 63.638 | de Havilland         | 57.694 |\n",
      "\u001b[32m[04/22 00:09:04 d2.engine.defaults]: \u001b[0mEvaluation results for manu_id_val in csv format:\n",
      "\u001b[32m[04/22 00:09:04 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[04/22 00:09:04 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[04/22 00:09:04 d2.evaluation.testing]: \u001b[0mcopypaste: 55.4991,71.9283,68.8530,nan,0.0000,55.5014\n",
      "\u001b[32m[04/22 00:09:04 d2.utils.events]: \u001b[0m eta: 4:25:47  iter: 2999  total_loss: 0.06945  loss_cls: 0.03535  loss_box_reg: 0.02775  loss_rpn_cls: 0.0005336  loss_rpn_loc: 0.00678  time: 0.4338  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:09:13 d2.utils.events]: \u001b[0m eta: 4:25:59  iter: 3019  total_loss: 0.0645  loss_cls: 0.02959  loss_box_reg: 0.02911  loss_rpn_cls: 0.0005588  loss_rpn_loc: 0.006112  time: 0.4338  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:09:21 d2.utils.events]: \u001b[0m eta: 4:25:51  iter: 3039  total_loss: 0.06578  loss_cls: 0.03428  loss_box_reg: 0.02462  loss_rpn_cls: 0.0003139  loss_rpn_loc: 0.005288  time: 0.4338  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:09:30 d2.utils.events]: \u001b[0m eta: 4:25:42  iter: 3059  total_loss: 0.06493  loss_cls: 0.031  loss_box_reg: 0.02542  loss_rpn_cls: 0.0006582  loss_rpn_loc: 0.005561  time: 0.4337  data_time: 0.0060  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:09:39 d2.utils.events]: \u001b[0m eta: 4:25:36  iter: 3079  total_loss: 0.07242  loss_cls: 0.03213  loss_box_reg: 0.03119  loss_rpn_cls: 0.0006612  loss_rpn_loc: 0.005614  time: 0.4338  data_time: 0.0065  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:09:47 d2.utils.events]: \u001b[0m eta: 4:25:10  iter: 3099  total_loss: 0.09716  loss_cls: 0.05129  loss_box_reg: 0.03867  loss_rpn_cls: 0.0004573  loss_rpn_loc: 0.006345  time: 0.4338  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:09:56 d2.utils.events]: \u001b[0m eta: 4:24:43  iter: 3119  total_loss: 0.0883  loss_cls: 0.04316  loss_box_reg: 0.03559  loss_rpn_cls: 0.0008527  loss_rpn_loc: 0.006356  time: 0.4337  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:10:04 d2.utils.events]: \u001b[0m eta: 4:24:21  iter: 3139  total_loss: 0.08632  loss_cls: 0.04381  loss_box_reg: 0.03012  loss_rpn_cls: 0.0004084  loss_rpn_loc: 0.006196  time: 0.4337  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:10:13 d2.utils.events]: \u001b[0m eta: 4:23:54  iter: 3159  total_loss: 0.08037  loss_cls: 0.04187  loss_box_reg: 0.03216  loss_rpn_cls: 0.0003117  loss_rpn_loc: 0.005203  time: 0.4336  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:10:22 d2.utils.events]: \u001b[0m eta: 4:23:37  iter: 3179  total_loss: 0.06892  loss_cls: 0.03294  loss_box_reg: 0.02909  loss_rpn_cls: 0.000179  loss_rpn_loc: 0.005583  time: 0.4337  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:10:30 d2.utils.events]: \u001b[0m eta: 4:23:05  iter: 3199  total_loss: 0.08037  loss_cls: 0.04069  loss_box_reg: 0.02956  loss_rpn_cls: 0.0003245  loss_rpn_loc: 0.006363  time: 0.4337  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:10:39 d2.utils.events]: \u001b[0m eta: 4:22:45  iter: 3219  total_loss: 0.07722  loss_cls: 0.03894  loss_box_reg: 0.02997  loss_rpn_cls: 0.0003842  loss_rpn_loc: 0.006296  time: 0.4336  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:10:47 d2.utils.events]: \u001b[0m eta: 4:22:28  iter: 3239  total_loss: 0.07379  loss_cls: 0.03513  loss_box_reg: 0.03165  loss_rpn_cls: 0.0003242  loss_rpn_loc: 0.00666  time: 0.4335  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:10:56 d2.utils.events]: \u001b[0m eta: 4:22:20  iter: 3259  total_loss: 0.08557  loss_cls: 0.03901  loss_box_reg: 0.03345  loss_rpn_cls: 0.0004437  loss_rpn_loc: 0.007089  time: 0.4335  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:11:04 d2.utils.events]: \u001b[0m eta: 4:22:12  iter: 3279  total_loss: 0.07122  loss_cls: 0.03321  loss_box_reg: 0.03019  loss_rpn_cls: 0.0003155  loss_rpn_loc: 0.005098  time: 0.4335  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/22 00:11:13 d2.utils.events]: \u001b[0m eta: 4:22:04  iter: 3299  total_loss: 0.08774  loss_cls: 0.04264  loss_box_reg: 0.03673  loss_rpn_cls: 0.0003483  loss_rpn_loc: 0.005478  time: 0.4335  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:11:21 d2.utils.events]: \u001b[0m eta: 4:21:51  iter: 3319  total_loss: 0.08143  loss_cls: 0.03986  loss_box_reg: 0.03386  loss_rpn_cls: 0.0004121  loss_rpn_loc: 0.006002  time: 0.4334  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:11:30 d2.utils.events]: \u001b[0m eta: 4:21:34  iter: 3339  total_loss: 0.07378  loss_cls: 0.03576  loss_box_reg: 0.02949  loss_rpn_cls: 0.000354  loss_rpn_loc: 0.005849  time: 0.4334  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:11:39 d2.utils.events]: \u001b[0m eta: 4:21:16  iter: 3359  total_loss: 0.07246  loss_cls: 0.03501  loss_box_reg: 0.02983  loss_rpn_cls: 0.0002384  loss_rpn_loc: 0.006743  time: 0.4334  data_time: 0.0060  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:11:47 d2.utils.events]: \u001b[0m eta: 4:21:00  iter: 3379  total_loss: 0.06699  loss_cls: 0.03488  loss_box_reg: 0.02728  loss_rpn_cls: 0.0002834  loss_rpn_loc: 0.004752  time: 0.4334  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:11:55 d2.utils.events]: \u001b[0m eta: 4:20:27  iter: 3399  total_loss: 0.06523  loss_cls: 0.02893  loss_box_reg: 0.03113  loss_rpn_cls: 0.0002491  loss_rpn_loc: 0.004662  time: 0.4332  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:12:04 d2.utils.events]: \u001b[0m eta: 4:20:11  iter: 3419  total_loss: 0.07344  loss_cls: 0.03474  loss_box_reg: 0.02928  loss_rpn_cls: 0.0005536  loss_rpn_loc: 0.006239  time: 0.4331  data_time: 0.0065  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:12:12 d2.utils.events]: \u001b[0m eta: 4:19:49  iter: 3439  total_loss: 0.06409  loss_cls: 0.02831  loss_box_reg: 0.0287  loss_rpn_cls: 0.0002882  loss_rpn_loc: 0.005418  time: 0.4329  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:12:20 d2.utils.events]: \u001b[0m eta: 4:19:09  iter: 3459  total_loss: 0.06735  loss_cls: 0.03246  loss_box_reg: 0.029  loss_rpn_cls: 0.0004075  loss_rpn_loc: 0.006356  time: 0.4329  data_time: 0.0069  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:12:29 d2.utils.events]: \u001b[0m eta: 4:18:32  iter: 3479  total_loss: 0.07055  loss_cls: 0.03484  loss_box_reg: 0.02812  loss_rpn_cls: 0.0001764  loss_rpn_loc: 0.005535  time: 0.4327  data_time: 0.0060  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:12:37 d2.utils.events]: \u001b[0m eta: 4:18:00  iter: 3499  total_loss: 0.06513  loss_cls: 0.03114  loss_box_reg: 0.02722  loss_rpn_cls: 0.000247  loss_rpn_loc: 0.004648  time: 0.4326  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:12:45 d2.utils.events]: \u001b[0m eta: 4:17:25  iter: 3519  total_loss: 0.06839  loss_cls: 0.02915  loss_box_reg: 0.03019  loss_rpn_cls: 0.0003138  loss_rpn_loc: 0.004878  time: 0.4325  data_time: 0.0059  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:12:53 d2.utils.events]: \u001b[0m eta: 4:17:05  iter: 3539  total_loss: 0.06208  loss_cls: 0.02699  loss_box_reg: 0.03052  loss_rpn_cls: 0.0003233  loss_rpn_loc: 0.005632  time: 0.4324  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:13:02 d2.utils.events]: \u001b[0m eta: 4:16:46  iter: 3559  total_loss: 0.06719  loss_cls: 0.03001  loss_box_reg: 0.02801  loss_rpn_cls: 0.0001651  loss_rpn_loc: 0.00598  time: 0.4323  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:13:10 d2.utils.events]: \u001b[0m eta: 4:16:31  iter: 3579  total_loss: 0.05804  loss_cls: 0.02449  loss_box_reg: 0.02868  loss_rpn_cls: 0.0002586  loss_rpn_loc: 0.004813  time: 0.4322  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:13:19 d2.utils.events]: \u001b[0m eta: 4:16:23  iter: 3599  total_loss: 0.06507  loss_cls: 0.03033  loss_box_reg: 0.0283  loss_rpn_cls: 0.000309  loss_rpn_loc: 0.00538  time: 0.4322  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:13:27 d2.utils.events]: \u001b[0m eta: 4:16:03  iter: 3619  total_loss: 0.05832  loss_cls: 0.02395  loss_box_reg: 0.02766  loss_rpn_cls: 0.0003875  loss_rpn_loc: 0.00478  time: 0.4321  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:13:36 d2.utils.events]: \u001b[0m eta: 4:15:45  iter: 3639  total_loss: 0.06636  loss_cls: 0.02565  loss_box_reg: 0.0294  loss_rpn_cls: 0.0004081  loss_rpn_loc: 0.005316  time: 0.4321  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:13:44 d2.utils.events]: \u001b[0m eta: 4:15:20  iter: 3659  total_loss: 0.06555  loss_cls: 0.02988  loss_box_reg: 0.02771  loss_rpn_cls: 0.000392  loss_rpn_loc: 0.005816  time: 0.4321  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:13:52 d2.utils.events]: \u001b[0m eta: 4:14:35  iter: 3679  total_loss: 0.06636  loss_cls: 0.02802  loss_box_reg: 0.02866  loss_rpn_cls: 0.0002283  loss_rpn_loc: 0.005451  time: 0.4320  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:14:01 d2.utils.events]: \u001b[0m eta: 4:14:22  iter: 3699  total_loss: 0.06104  loss_cls: 0.02453  loss_box_reg: 0.03  loss_rpn_cls: 0.0003799  loss_rpn_loc: 0.004836  time: 0.4320  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:14:10 d2.utils.events]: \u001b[0m eta: 4:14:27  iter: 3719  total_loss: 0.05684  loss_cls: 0.0236  loss_box_reg: 0.02545  loss_rpn_cls: 0.0002621  loss_rpn_loc: 0.006047  time: 0.4321  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:14:19 d2.utils.events]: \u001b[0m eta: 4:14:34  iter: 3739  total_loss: 0.05934  loss_cls: 0.02721  loss_box_reg: 0.02722  loss_rpn_cls: 0.0002512  loss_rpn_loc: 0.005302  time: 0.4321  data_time: 0.0059  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:14:28 d2.utils.events]: \u001b[0m eta: 4:14:49  iter: 3759  total_loss: 0.05738  loss_cls: 0.02153  loss_box_reg: 0.02933  loss_rpn_cls: 0.000421  loss_rpn_loc: 0.005545  time: 0.4321  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:14:36 d2.utils.events]: \u001b[0m eta: 4:14:34  iter: 3779  total_loss: 0.05905  loss_cls: 0.02197  loss_box_reg: 0.02824  loss_rpn_cls: 0.0002134  loss_rpn_loc: 0.004763  time: 0.4321  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:14:45 d2.utils.events]: \u001b[0m eta: 4:14:01  iter: 3799  total_loss: 0.06019  loss_cls: 0.02641  loss_box_reg: 0.02685  loss_rpn_cls: 0.0002845  loss_rpn_loc: 0.005703  time: 0.4320  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:14:53 d2.utils.events]: \u001b[0m eta: 4:13:41  iter: 3819  total_loss: 0.05923  loss_cls: 0.02679  loss_box_reg: 0.02488  loss_rpn_cls: 0.0003084  loss_rpn_loc: 0.005054  time: 0.4319  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:15:01 d2.utils.events]: \u001b[0m eta: 4:13:33  iter: 3839  total_loss: 0.06657  loss_cls: 0.02585  loss_box_reg: 0.03036  loss_rpn_cls: 0.000211  loss_rpn_loc: 0.006682  time: 0.4318  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:15:09 d2.utils.events]: \u001b[0m eta: 4:13:13  iter: 3859  total_loss: 0.05893  loss_cls: 0.02464  loss_box_reg: 0.02862  loss_rpn_cls: 0.0004006  loss_rpn_loc: 0.005916  time: 0.4317  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:15:18 d2.utils.events]: \u001b[0m eta: 4:13:06  iter: 3879  total_loss: 0.0583  loss_cls: 0.02305  loss_box_reg: 0.02574  loss_rpn_cls: 0.0002392  loss_rpn_loc: 0.006123  time: 0.4317  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:15:27 d2.utils.events]: \u001b[0m eta: 4:13:09  iter: 3899  total_loss: 0.05866  loss_cls: 0.02618  loss_box_reg: 0.02807  loss_rpn_cls: 0.0002162  loss_rpn_loc: 0.005731  time: 0.4317  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:15:35 d2.utils.events]: \u001b[0m eta: 4:13:00  iter: 3919  total_loss: 0.05002  loss_cls: 0.02086  loss_box_reg: 0.02121  loss_rpn_cls: 0.0003467  loss_rpn_loc: 0.00586  time: 0.4316  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:15:43 d2.utils.events]: \u001b[0m eta: 4:12:47  iter: 3939  total_loss: 0.05155  loss_cls: 0.02488  loss_box_reg: 0.02284  loss_rpn_cls: 0.0004685  loss_rpn_loc: 0.004774  time: 0.4315  data_time: 0.0067  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:15:52 d2.utils.events]: \u001b[0m eta: 4:12:57  iter: 3959  total_loss: 0.05909  loss_cls: 0.02733  loss_box_reg: 0.02486  loss_rpn_cls: 0.0004767  loss_rpn_loc: 0.006559  time: 0.4316  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:16:01 d2.utils.events]: \u001b[0m eta: 4:13:12  iter: 3979  total_loss: 0.05808  loss_cls: 0.02422  loss_box_reg: 0.02614  loss_rpn_cls: 0.0002301  loss_rpn_loc: 0.006079  time: 0.4317  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[04/22 00:16:10 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[04/22 00:16:10 d2.data.datasets.coco]: \u001b[0mLoaded 3333 images in COCO format from /home/zack/datasets/manufacturer_identification/data/coco/val.json\n",
      "\u001b[32m[04/22 00:16:10 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[04/22 00:16:10 d2.data.common]: \u001b[0mSerializing 3333 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[04/22 00:16:10 d2.data.common]: \u001b[0mSerialized dataset takes 0.85 MiB\n",
      "\u001b[32m[04/22 00:16:10 d2.evaluation.evaluator]: \u001b[0mStart inference on 3333 batches\n",
      "\u001b[32m[04/22 00:16:11 d2.evaluation.evaluator]: \u001b[0mInference done 11/3333. Dataloading: 0.0008 s/iter. Inference: 0.0322 s/iter. Eval: 0.0001 s/iter. Total: 0.0331 s/iter. ETA=0:01:50\n",
      "\u001b[32m[04/22 00:16:16 d2.evaluation.evaluator]: \u001b[0mInference done 157/3333. Dataloading: 0.0007 s/iter. Inference: 0.0333 s/iter. Eval: 0.0001 s/iter. Total: 0.0342 s/iter. ETA=0:01:48\n",
      "\u001b[32m[04/22 00:16:21 d2.evaluation.evaluator]: \u001b[0mInference done 303/3333. Dataloading: 0.0008 s/iter. Inference: 0.0333 s/iter. Eval: 0.0001 s/iter. Total: 0.0343 s/iter. ETA=0:01:43\n",
      "\u001b[32m[04/22 00:16:26 d2.evaluation.evaluator]: \u001b[0mInference done 437/3333. Dataloading: 0.0008 s/iter. Inference: 0.0343 s/iter. Eval: 0.0001 s/iter. Total: 0.0353 s/iter. ETA=0:01:42\n",
      "\u001b[32m[04/22 00:16:31 d2.evaluation.evaluator]: \u001b[0mInference done 588/3333. Dataloading: 0.0008 s/iter. Inference: 0.0338 s/iter. Eval: 0.0001 s/iter. Total: 0.0348 s/iter. ETA=0:01:35\n",
      "\u001b[32m[04/22 00:16:36 d2.evaluation.evaluator]: \u001b[0mInference done 742/3333. Dataloading: 0.0008 s/iter. Inference: 0.0334 s/iter. Eval: 0.0001 s/iter. Total: 0.0343 s/iter. ETA=0:01:28\n",
      "\u001b[32m[04/22 00:16:41 d2.evaluation.evaluator]: \u001b[0mInference done 899/3333. Dataloading: 0.0008 s/iter. Inference: 0.0330 s/iter. Eval: 0.0001 s/iter. Total: 0.0339 s/iter. ETA=0:01:22\n",
      "\u001b[32m[04/22 00:16:46 d2.evaluation.evaluator]: \u001b[0mInference done 1053/3333. Dataloading: 0.0008 s/iter. Inference: 0.0328 s/iter. Eval: 0.0001 s/iter. Total: 0.0337 s/iter. ETA=0:01:16\n",
      "\u001b[32m[04/22 00:16:51 d2.evaluation.evaluator]: \u001b[0mInference done 1210/3333. Dataloading: 0.0008 s/iter. Inference: 0.0325 s/iter. Eval: 0.0001 s/iter. Total: 0.0335 s/iter. ETA=0:01:11\n",
      "\u001b[32m[04/22 00:16:56 d2.evaluation.evaluator]: \u001b[0mInference done 1362/3333. Dataloading: 0.0008 s/iter. Inference: 0.0325 s/iter. Eval: 0.0001 s/iter. Total: 0.0334 s/iter. ETA=0:01:05\n",
      "\u001b[32m[04/22 00:17:01 d2.evaluation.evaluator]: \u001b[0mInference done 1518/3333. Dataloading: 0.0008 s/iter. Inference: 0.0324 s/iter. Eval: 0.0001 s/iter. Total: 0.0333 s/iter. ETA=0:01:00\n",
      "\u001b[32m[04/22 00:17:06 d2.evaluation.evaluator]: \u001b[0mInference done 1664/3333. Dataloading: 0.0008 s/iter. Inference: 0.0325 s/iter. Eval: 0.0001 s/iter. Total: 0.0334 s/iter. ETA=0:00:55\n",
      "\u001b[32m[04/22 00:17:11 d2.evaluation.evaluator]: \u001b[0mInference done 1822/3333. Dataloading: 0.0008 s/iter. Inference: 0.0323 s/iter. Eval: 0.0001 s/iter. Total: 0.0332 s/iter. ETA=0:00:50\n",
      "\u001b[32m[04/22 00:17:16 d2.evaluation.evaluator]: \u001b[0mInference done 1969/3333. Dataloading: 0.0008 s/iter. Inference: 0.0324 s/iter. Eval: 0.0001 s/iter. Total: 0.0333 s/iter. ETA=0:00:45\n",
      "\u001b[32m[04/22 00:17:21 d2.evaluation.evaluator]: \u001b[0mInference done 2121/3333. Dataloading: 0.0008 s/iter. Inference: 0.0323 s/iter. Eval: 0.0001 s/iter. Total: 0.0333 s/iter. ETA=0:00:40\n",
      "\u001b[32m[04/22 00:17:26 d2.evaluation.evaluator]: \u001b[0mInference done 2273/3333. Dataloading: 0.0008 s/iter. Inference: 0.0323 s/iter. Eval: 0.0001 s/iter. Total: 0.0333 s/iter. ETA=0:00:35\n",
      "\u001b[32m[04/22 00:17:31 d2.evaluation.evaluator]: \u001b[0mInference done 2410/3333. Dataloading: 0.0008 s/iter. Inference: 0.0325 s/iter. Eval: 0.0001 s/iter. Total: 0.0335 s/iter. ETA=0:00:30\n",
      "\u001b[32m[04/22 00:17:36 d2.evaluation.evaluator]: \u001b[0mInference done 2548/3333. Dataloading: 0.0008 s/iter. Inference: 0.0327 s/iter. Eval: 0.0001 s/iter. Total: 0.0336 s/iter. ETA=0:00:26\n",
      "\u001b[32m[04/22 00:17:41 d2.evaluation.evaluator]: \u001b[0mInference done 2691/3333. Dataloading: 0.0008 s/iter. Inference: 0.0327 s/iter. Eval: 0.0001 s/iter. Total: 0.0337 s/iter. ETA=0:00:21\n",
      "\u001b[32m[04/22 00:17:46 d2.evaluation.evaluator]: \u001b[0mInference done 2836/3333. Dataloading: 0.0008 s/iter. Inference: 0.0328 s/iter. Eval: 0.0001 s/iter. Total: 0.0338 s/iter. ETA=0:00:16\n",
      "\u001b[32m[04/22 00:17:51 d2.evaluation.evaluator]: \u001b[0mInference done 2982/3333. Dataloading: 0.0008 s/iter. Inference: 0.0328 s/iter. Eval: 0.0001 s/iter. Total: 0.0338 s/iter. ETA=0:00:11\n",
      "\u001b[32m[04/22 00:17:57 d2.evaluation.evaluator]: \u001b[0mInference done 3123/3333. Dataloading: 0.0008 s/iter. Inference: 0.0329 s/iter. Eval: 0.0001 s/iter. Total: 0.0339 s/iter. ETA=0:00:07\n",
      "\u001b[32m[04/22 00:18:02 d2.evaluation.evaluator]: \u001b[0mInference done 3272/3333. Dataloading: 0.0008 s/iter. Inference: 0.0329 s/iter. Eval: 0.0001 s/iter. Total: 0.0339 s/iter. ETA=0:00:02\n",
      "\u001b[32m[04/22 00:18:04 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:01:52.953175 (0.033940 s / iter per device, on 1 devices)\n",
      "\u001b[32m[04/22 00:18:04 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:49 (0.032937 s / iter per device, on 1 devices)\n",
      "\u001b[32m[04/22 00:18:04 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[04/22 00:18:04 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output_class_balanced/inference/coco_instances_results.json\n",
      "\u001b[32m[04/22 00:18:04 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.08s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[04/22 00:18:04 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[04/22 00:18:05 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 1.13 seconds.\n",
      "\u001b[32m[04/22 00:18:05 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[04/22 00:18:05 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.15 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.701\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.867\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.843\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.701\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.814\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.815\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.815\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.815\n",
      "\u001b[32m[04/22 00:18:05 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 70.080 | 86.740 | 84.334 |  nan  | 0.000 | 70.085 |\n",
      "\u001b[32m[04/22 00:18:05 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[04/22 00:18:05 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category                 | AP     | category             | AP     | category             | AP     |\n",
      "|:-------------------------|:-------|:---------------------|:-------|:---------------------|:-------|\n",
      "| ATR                      | 62.156 | Airbus               | 46.072 | Antonov              | 82.478 |\n",
      "| Beechcraft               | 73.770 | Boeing               | 60.065 | Bombardier Aerospace | 65.157 |\n",
      "| British Aerospace        | 79.290 | Canadair             | 70.347 | Cessna               | 71.474 |\n",
      "| Cirrus Aircraft          | 78.188 | Dassault Aviation    | 83.236 | Dornier              | 81.785 |\n",
      "| Douglas Aircraft Company | 58.792 | Embraer              | 64.364 | Eurofighter          | 68.228 |\n",
      "| Fairchild                | 68.627 | Fokker               | 55.970 | Gulfstream Aerospace | 78.762 |\n",
      "| Ilyushin                 | 58.567 | Lockheed Corporation | 62.160 | Lockheed Martin      | 82.726 |\n",
      "| McDonnell Douglas        | 41.220 | Panavia              | 76.391 | Piper                | 77.700 |\n",
      "| Robin                    | 63.086 | Saab                 | 77.857 | Supermarine          | 78.100 |\n",
      "| Tupolev                  | 81.869 | Yakovlev             | 79.127 | de Havilland         | 74.832 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/22 00:18:05 d2.engine.defaults]: \u001b[0mEvaluation results for manu_id_val in csv format:\n",
      "\u001b[32m[04/22 00:18:05 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[04/22 00:18:05 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[04/22 00:18:05 d2.evaluation.testing]: \u001b[0mcopypaste: 70.0798,86.7400,84.3338,nan,0.0000,70.0847\n",
      "\u001b[32m[04/22 00:18:05 d2.utils.events]: \u001b[0m eta: 4:13:07  iter: 3999  total_loss: 0.04782  loss_cls: 0.02239  loss_box_reg: 0.02287  loss_rpn_cls: 0.000186  loss_rpn_loc: 0.004336  time: 0.4317  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:18:14 d2.utils.events]: \u001b[0m eta: 4:12:57  iter: 4019  total_loss: 0.05185  loss_cls: 0.02238  loss_box_reg: 0.02243  loss_rpn_cls: 0.0001685  loss_rpn_loc: 0.004839  time: 0.4317  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:18:23 d2.utils.events]: \u001b[0m eta: 4:12:53  iter: 4039  total_loss: 0.04769  loss_cls: 0.02076  loss_box_reg: 0.02266  loss_rpn_cls: 0.000422  loss_rpn_loc: 0.004965  time: 0.4317  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:18:31 d2.utils.events]: \u001b[0m eta: 4:12:38  iter: 4059  total_loss: 0.05276  loss_cls: 0.02365  loss_box_reg: 0.02384  loss_rpn_cls: 0.0002223  loss_rpn_loc: 0.005902  time: 0.4316  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:18:39 d2.utils.events]: \u001b[0m eta: 4:11:53  iter: 4079  total_loss: 0.05132  loss_cls: 0.02176  loss_box_reg: 0.02436  loss_rpn_cls: 0.0002063  loss_rpn_loc: 0.005528  time: 0.4315  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:18:47 d2.utils.events]: \u001b[0m eta: 4:11:36  iter: 4099  total_loss: 0.04509  loss_cls: 0.01797  loss_box_reg: 0.02009  loss_rpn_cls: 0.0002871  loss_rpn_loc: 0.005154  time: 0.4313  data_time: 0.0058  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:18:55 d2.utils.events]: \u001b[0m eta: 4:11:20  iter: 4119  total_loss: 0.04251  loss_cls: 0.01747  loss_box_reg: 0.02071  loss_rpn_cls: 0.0004431  loss_rpn_loc: 0.005254  time: 0.4311  data_time: 0.0059  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:19:04 d2.utils.events]: \u001b[0m eta: 4:11:05  iter: 4139  total_loss: 0.05795  loss_cls: 0.02296  loss_box_reg: 0.02459  loss_rpn_cls: 0.0003238  loss_rpn_loc: 0.006066  time: 0.4311  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:19:12 d2.utils.events]: \u001b[0m eta: 4:11:00  iter: 4159  total_loss: 0.04864  loss_cls: 0.01964  loss_box_reg: 0.02156  loss_rpn_cls: 0.0001724  loss_rpn_loc: 0.005133  time: 0.4311  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:19:21 d2.utils.events]: \u001b[0m eta: 4:10:51  iter: 4179  total_loss: 0.04936  loss_cls: 0.02016  loss_box_reg: 0.02126  loss_rpn_cls: 0.0001886  loss_rpn_loc: 0.004858  time: 0.4312  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:19:30 d2.utils.events]: \u001b[0m eta: 4:10:46  iter: 4199  total_loss: 0.05241  loss_cls: 0.0219  loss_box_reg: 0.02325  loss_rpn_cls: 0.0004081  loss_rpn_loc: 0.005491  time: 0.4312  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:19:39 d2.utils.events]: \u001b[0m eta: 4:10:50  iter: 4219  total_loss: 0.04881  loss_cls: 0.0185  loss_box_reg: 0.02403  loss_rpn_cls: 0.0002969  loss_rpn_loc: 0.005474  time: 0.4313  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:19:48 d2.utils.events]: \u001b[0m eta: 4:10:55  iter: 4239  total_loss: 0.0566  loss_cls: 0.02366  loss_box_reg: 0.02432  loss_rpn_cls: 0.000134  loss_rpn_loc: 0.005114  time: 0.4314  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:19:57 d2.utils.events]: \u001b[0m eta: 4:10:47  iter: 4259  total_loss: 0.05388  loss_cls: 0.02466  loss_box_reg: 0.02453  loss_rpn_cls: 0.0004082  loss_rpn_loc: 0.005237  time: 0.4315  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:20:06 d2.utils.events]: \u001b[0m eta: 4:10:38  iter: 4279  total_loss: 0.04928  loss_cls: 0.01805  loss_box_reg: 0.02255  loss_rpn_cls: 0.0002989  loss_rpn_loc: 0.005894  time: 0.4315  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:20:15 d2.utils.events]: \u001b[0m eta: 4:10:33  iter: 4299  total_loss: 0.05512  loss_cls: 0.02242  loss_box_reg: 0.02582  loss_rpn_cls: 0.0003881  loss_rpn_loc: 0.006275  time: 0.4316  data_time: 0.0065  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:20:24 d2.utils.events]: \u001b[0m eta: 4:10:36  iter: 4319  total_loss: 0.0541  loss_cls: 0.02061  loss_box_reg: 0.0237  loss_rpn_cls: 0.0002525  loss_rpn_loc: 0.005905  time: 0.4316  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:20:33 d2.utils.events]: \u001b[0m eta: 4:10:40  iter: 4339  total_loss: 0.05553  loss_cls: 0.02289  loss_box_reg: 0.02402  loss_rpn_cls: 0.0001743  loss_rpn_loc: 0.005257  time: 0.4317  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:20:42 d2.utils.events]: \u001b[0m eta: 4:10:37  iter: 4359  total_loss: 0.05417  loss_cls: 0.0218  loss_box_reg: 0.02362  loss_rpn_cls: 0.0002007  loss_rpn_loc: 0.004399  time: 0.4318  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:20:50 d2.utils.events]: \u001b[0m eta: 4:10:33  iter: 4379  total_loss: 0.05776  loss_cls: 0.02666  loss_box_reg: 0.02394  loss_rpn_cls: 0.0002439  loss_rpn_loc: 0.005667  time: 0.4318  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:20:59 d2.utils.events]: \u001b[0m eta: 4:11:19  iter: 4399  total_loss: 0.0535  loss_cls: 0.02355  loss_box_reg: 0.02359  loss_rpn_cls: 0.0001823  loss_rpn_loc: 0.004808  time: 0.4319  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:21:08 d2.utils.events]: \u001b[0m eta: 4:11:23  iter: 4419  total_loss: 0.05426  loss_cls: 0.02055  loss_box_reg: 0.02583  loss_rpn_cls: 0.0002517  loss_rpn_loc: 0.005397  time: 0.4319  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:21:17 d2.utils.events]: \u001b[0m eta: 4:11:37  iter: 4439  total_loss: 0.05045  loss_cls: 0.01717  loss_box_reg: 0.02682  loss_rpn_cls: 0.0001994  loss_rpn_loc: 0.005249  time: 0.4321  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:21:27 d2.utils.events]: \u001b[0m eta: 4:11:49  iter: 4459  total_loss: 0.05054  loss_cls: 0.02085  loss_box_reg: 0.02263  loss_rpn_cls: 0.0002476  loss_rpn_loc: 0.00606  time: 0.4322  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:21:36 d2.utils.events]: \u001b[0m eta: 4:12:17  iter: 4479  total_loss: 0.05732  loss_cls: 0.02228  loss_box_reg: 0.02493  loss_rpn_cls: 0.0005524  loss_rpn_loc: 0.005446  time: 0.4323  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:21:44 d2.utils.events]: \u001b[0m eta: 4:12:15  iter: 4499  total_loss: 0.04954  loss_cls: 0.01869  loss_box_reg: 0.02242  loss_rpn_cls: 0.0004538  loss_rpn_loc: 0.005417  time: 0.4323  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:21:53 d2.utils.events]: \u001b[0m eta: 4:12:47  iter: 4519  total_loss: 0.05003  loss_cls: 0.02214  loss_box_reg: 0.02447  loss_rpn_cls: 0.0001773  loss_rpn_loc: 0.005842  time: 0.4323  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:22:02 d2.utils.events]: \u001b[0m eta: 4:12:59  iter: 4539  total_loss: 0.05081  loss_cls: 0.01913  loss_box_reg: 0.02487  loss_rpn_cls: 0.0002597  loss_rpn_loc: 0.005308  time: 0.4324  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:22:10 d2.utils.events]: \u001b[0m eta: 4:12:50  iter: 4559  total_loss: 0.04809  loss_cls: 0.01926  loss_box_reg: 0.02255  loss_rpn_cls: 0.0001471  loss_rpn_loc: 0.004445  time: 0.4323  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:22:19 d2.utils.events]: \u001b[0m eta: 4:12:59  iter: 4579  total_loss: 0.05107  loss_cls: 0.02226  loss_box_reg: 0.02303  loss_rpn_cls: 0.0001146  loss_rpn_loc: 0.004605  time: 0.4323  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:22:28 d2.utils.events]: \u001b[0m eta: 4:12:45  iter: 4599  total_loss: 0.05318  loss_cls: 0.02181  loss_box_reg: 0.02396  loss_rpn_cls: 0.0001833  loss_rpn_loc: 0.005343  time: 0.4323  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:22:36 d2.utils.events]: \u001b[0m eta: 4:13:03  iter: 4619  total_loss: 0.04461  loss_cls: 0.01802  loss_box_reg: 0.0201  loss_rpn_cls: 0.0002846  loss_rpn_loc: 0.00489  time: 0.4323  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:22:45 d2.utils.events]: \u001b[0m eta: 4:12:55  iter: 4639  total_loss: 0.05404  loss_cls: 0.02417  loss_box_reg: 0.02311  loss_rpn_cls: 0.0002084  loss_rpn_loc: 0.005235  time: 0.4324  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/22 00:22:54 d2.utils.events]: \u001b[0m eta: 4:13:10  iter: 4659  total_loss: 0.04494  loss_cls: 0.01772  loss_box_reg: 0.02392  loss_rpn_cls: 0.0003118  loss_rpn_loc: 0.005681  time: 0.4324  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:23:02 d2.utils.events]: \u001b[0m eta: 4:13:11  iter: 4679  total_loss: 0.04218  loss_cls: 0.01716  loss_box_reg: 0.02075  loss_rpn_cls: 0.0002039  loss_rpn_loc: 0.004771  time: 0.4323  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:23:11 d2.utils.events]: \u001b[0m eta: 4:12:56  iter: 4699  total_loss: 0.04221  loss_cls: 0.01522  loss_box_reg: 0.02158  loss_rpn_cls: 0.0001643  loss_rpn_loc: 0.00499  time: 0.4323  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:23:20 d2.utils.events]: \u001b[0m eta: 4:12:39  iter: 4719  total_loss: 0.04722  loss_cls: 0.0191  loss_box_reg: 0.02197  loss_rpn_cls: 0.0001854  loss_rpn_loc: 0.005183  time: 0.4323  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:23:28 d2.utils.events]: \u001b[0m eta: 4:12:31  iter: 4739  total_loss: 0.04355  loss_cls: 0.01568  loss_box_reg: 0.02098  loss_rpn_cls: 0.0002143  loss_rpn_loc: 0.005666  time: 0.4323  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:23:37 d2.utils.events]: \u001b[0m eta: 4:12:27  iter: 4759  total_loss: 0.04924  loss_cls: 0.01822  loss_box_reg: 0.0216  loss_rpn_cls: 0.0002778  loss_rpn_loc: 0.005132  time: 0.4323  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:23:46 d2.utils.events]: \u001b[0m eta: 4:12:52  iter: 4779  total_loss: 0.05067  loss_cls: 0.01994  loss_box_reg: 0.02269  loss_rpn_cls: 0.0002463  loss_rpn_loc: 0.005182  time: 0.4324  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:23:55 d2.utils.events]: \u001b[0m eta: 4:13:16  iter: 4799  total_loss: 0.04335  loss_cls: 0.01725  loss_box_reg: 0.02149  loss_rpn_cls: 0.0001807  loss_rpn_loc: 0.005107  time: 0.4325  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:24:04 d2.utils.events]: \u001b[0m eta: 4:13:25  iter: 4819  total_loss: 0.03808  loss_cls: 0.01522  loss_box_reg: 0.01812  loss_rpn_cls: 0.0002681  loss_rpn_loc: 0.004616  time: 0.4325  data_time: 0.0070  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:24:12 d2.utils.events]: \u001b[0m eta: 4:13:31  iter: 4839  total_loss: 0.04631  loss_cls: 0.01949  loss_box_reg: 0.01976  loss_rpn_cls: 0.0002823  loss_rpn_loc: 0.005252  time: 0.4325  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:24:21 d2.utils.events]: \u001b[0m eta: 4:13:44  iter: 4859  total_loss: 0.04082  loss_cls: 0.01723  loss_box_reg: 0.02007  loss_rpn_cls: 0.0002839  loss_rpn_loc: 0.00454  time: 0.4325  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:24:30 d2.utils.events]: \u001b[0m eta: 4:13:35  iter: 4879  total_loss: 0.04335  loss_cls: 0.01563  loss_box_reg: 0.0201  loss_rpn_cls: 0.0002446  loss_rpn_loc: 0.006149  time: 0.4325  data_time: 0.0060  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:24:39 d2.utils.events]: \u001b[0m eta: 4:13:48  iter: 4899  total_loss: 0.04309  loss_cls: 0.01629  loss_box_reg: 0.02082  loss_rpn_cls: 0.0002588  loss_rpn_loc: 0.004578  time: 0.4325  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:24:47 d2.utils.events]: \u001b[0m eta: 4:13:51  iter: 4919  total_loss: 0.04516  loss_cls: 0.0177  loss_box_reg: 0.02223  loss_rpn_cls: 0.0002515  loss_rpn_loc: 0.005256  time: 0.4325  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:24:56 d2.utils.events]: \u001b[0m eta: 4:14:05  iter: 4939  total_loss: 0.04237  loss_cls: 0.01263  loss_box_reg: 0.0226  loss_rpn_cls: 0.0001406  loss_rpn_loc: 0.004661  time: 0.4325  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:25:04 d2.utils.events]: \u001b[0m eta: 4:13:43  iter: 4959  total_loss: 0.04954  loss_cls: 0.02009  loss_box_reg: 0.02273  loss_rpn_cls: 0.0002818  loss_rpn_loc: 0.004934  time: 0.4324  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:25:13 d2.utils.events]: \u001b[0m eta: 4:13:07  iter: 4979  total_loss: 0.03982  loss_cls: 0.01483  loss_box_reg: 0.01831  loss_rpn_cls: 0.0002196  loss_rpn_loc: 0.004763  time: 0.4324  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[04/22 00:25:22 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[04/22 00:25:22 d2.data.datasets.coco]: \u001b[0mLoaded 3333 images in COCO format from /home/zack/datasets/manufacturer_identification/data/coco/val.json\n",
      "\u001b[32m[04/22 00:25:22 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[04/22 00:25:22 d2.data.common]: \u001b[0mSerializing 3333 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[04/22 00:25:22 d2.data.common]: \u001b[0mSerialized dataset takes 0.85 MiB\n",
      "\u001b[32m[04/22 00:25:22 d2.evaluation.evaluator]: \u001b[0mStart inference on 3333 batches\n",
      "\u001b[32m[04/22 00:25:22 d2.evaluation.evaluator]: \u001b[0mInference done 11/3333. Dataloading: 0.0006 s/iter. Inference: 0.0305 s/iter. Eval: 0.0001 s/iter. Total: 0.0313 s/iter. ETA=0:01:44\n",
      "\u001b[32m[04/22 00:25:28 d2.evaluation.evaluator]: \u001b[0mInference done 168/3333. Dataloading: 0.0008 s/iter. Inference: 0.0310 s/iter. Eval: 0.0001 s/iter. Total: 0.0320 s/iter. ETA=0:01:41\n",
      "\u001b[32m[04/22 00:25:33 d2.evaluation.evaluator]: \u001b[0mInference done 323/3333. Dataloading: 0.0008 s/iter. Inference: 0.0312 s/iter. Eval: 0.0001 s/iter. Total: 0.0322 s/iter. ETA=0:01:36\n",
      "\u001b[32m[04/22 00:25:38 d2.evaluation.evaluator]: \u001b[0mInference done 480/3333. Dataloading: 0.0008 s/iter. Inference: 0.0312 s/iter. Eval: 0.0001 s/iter. Total: 0.0321 s/iter. ETA=0:01:31\n",
      "\u001b[32m[04/22 00:25:43 d2.evaluation.evaluator]: \u001b[0mInference done 634/3333. Dataloading: 0.0008 s/iter. Inference: 0.0313 s/iter. Eval: 0.0001 s/iter. Total: 0.0322 s/iter. ETA=0:01:26\n",
      "\u001b[32m[04/22 00:25:48 d2.evaluation.evaluator]: \u001b[0mInference done 789/3333. Dataloading: 0.0008 s/iter. Inference: 0.0313 s/iter. Eval: 0.0001 s/iter. Total: 0.0322 s/iter. ETA=0:01:22\n",
      "\u001b[32m[04/22 00:25:53 d2.evaluation.evaluator]: \u001b[0mInference done 941/3333. Dataloading: 0.0008 s/iter. Inference: 0.0314 s/iter. Eval: 0.0001 s/iter. Total: 0.0324 s/iter. ETA=0:01:17\n",
      "\u001b[32m[04/22 00:25:58 d2.evaluation.evaluator]: \u001b[0mInference done 1093/3333. Dataloading: 0.0008 s/iter. Inference: 0.0315 s/iter. Eval: 0.0001 s/iter. Total: 0.0325 s/iter. ETA=0:01:12\n",
      "\u001b[32m[04/22 00:26:03 d2.evaluation.evaluator]: \u001b[0mInference done 1245/3333. Dataloading: 0.0008 s/iter. Inference: 0.0316 s/iter. Eval: 0.0001 s/iter. Total: 0.0325 s/iter. ETA=0:01:07\n",
      "\u001b[32m[04/22 00:26:08 d2.evaluation.evaluator]: \u001b[0mInference done 1398/3333. Dataloading: 0.0008 s/iter. Inference: 0.0316 s/iter. Eval: 0.0001 s/iter. Total: 0.0326 s/iter. ETA=0:01:03\n",
      "\u001b[32m[04/22 00:26:13 d2.evaluation.evaluator]: \u001b[0mInference done 1548/3333. Dataloading: 0.0008 s/iter. Inference: 0.0317 s/iter. Eval: 0.0001 s/iter. Total: 0.0326 s/iter. ETA=0:00:58\n",
      "\u001b[32m[04/22 00:26:18 d2.evaluation.evaluator]: \u001b[0mInference done 1698/3333. Dataloading: 0.0008 s/iter. Inference: 0.0318 s/iter. Eval: 0.0001 s/iter. Total: 0.0327 s/iter. ETA=0:00:53\n",
      "\u001b[32m[04/22 00:26:23 d2.evaluation.evaluator]: \u001b[0mInference done 1850/3333. Dataloading: 0.0008 s/iter. Inference: 0.0318 s/iter. Eval: 0.0001 s/iter. Total: 0.0327 s/iter. ETA=0:00:48\n",
      "\u001b[32m[04/22 00:26:28 d2.evaluation.evaluator]: \u001b[0mInference done 2004/3333. Dataloading: 0.0008 s/iter. Inference: 0.0318 s/iter. Eval: 0.0001 s/iter. Total: 0.0327 s/iter. ETA=0:00:43\n",
      "\u001b[32m[04/22 00:26:33 d2.evaluation.evaluator]: \u001b[0mInference done 2158/3333. Dataloading: 0.0008 s/iter. Inference: 0.0318 s/iter. Eval: 0.0001 s/iter. Total: 0.0327 s/iter. ETA=0:00:38\n",
      "\u001b[32m[04/22 00:26:38 d2.evaluation.evaluator]: \u001b[0mInference done 2315/3333. Dataloading: 0.0008 s/iter. Inference: 0.0317 s/iter. Eval: 0.0001 s/iter. Total: 0.0327 s/iter. ETA=0:00:33\n",
      "\u001b[32m[04/22 00:26:43 d2.evaluation.evaluator]: \u001b[0mInference done 2472/3333. Dataloading: 0.0008 s/iter. Inference: 0.0317 s/iter. Eval: 0.0001 s/iter. Total: 0.0326 s/iter. ETA=0:00:28\n",
      "\u001b[32m[04/22 00:26:48 d2.evaluation.evaluator]: \u001b[0mInference done 2629/3333. Dataloading: 0.0008 s/iter. Inference: 0.0316 s/iter. Eval: 0.0001 s/iter. Total: 0.0326 s/iter. ETA=0:00:22\n",
      "\u001b[32m[04/22 00:26:53 d2.evaluation.evaluator]: \u001b[0mInference done 2784/3333. Dataloading: 0.0008 s/iter. Inference: 0.0316 s/iter. Eval: 0.0001 s/iter. Total: 0.0326 s/iter. ETA=0:00:17\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/22 00:26:58 d2.evaluation.evaluator]: \u001b[0mInference done 2936/3333. Dataloading: 0.0008 s/iter. Inference: 0.0316 s/iter. Eval: 0.0001 s/iter. Total: 0.0326 s/iter. ETA=0:00:12\n",
      "\u001b[32m[04/22 00:27:03 d2.evaluation.evaluator]: \u001b[0mInference done 3086/3333. Dataloading: 0.0008 s/iter. Inference: 0.0317 s/iter. Eval: 0.0001 s/iter. Total: 0.0326 s/iter. ETA=0:00:08\n",
      "\u001b[32m[04/22 00:27:08 d2.evaluation.evaluator]: \u001b[0mInference done 3236/3333. Dataloading: 0.0008 s/iter. Inference: 0.0317 s/iter. Eval: 0.0001 s/iter. Total: 0.0326 s/iter. ETA=0:00:03\n",
      "\u001b[32m[04/22 00:27:11 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:01:48.772802 (0.032684 s / iter per device, on 1 devices)\n",
      "\u001b[32m[04/22 00:27:11 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:45 (0.031703 s / iter per device, on 1 devices)\n",
      "\u001b[32m[04/22 00:27:11 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[04/22 00:27:11 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output_class_balanced/inference/coco_instances_results.json\n",
      "\u001b[32m[04/22 00:27:11 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[04/22 00:27:11 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[04/22 00:27:12 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 1.06 seconds.\n",
      "\u001b[32m[04/22 00:27:12 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[04/22 00:27:12 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.15 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.726\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.863\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.849\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.726\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.847\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.848\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.848\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.848\n",
      "\u001b[32m[04/22 00:27:12 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 72.590 | 86.329 | 84.917 |  nan  | 0.000 | 72.595 |\n",
      "\u001b[32m[04/22 00:27:12 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[04/22 00:27:12 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category                 | AP     | category             | AP     | category             | AP     |\n",
      "|:-------------------------|:-------|:---------------------|:-------|:---------------------|:-------|\n",
      "| ATR                      | 83.958 | Airbus               | 52.537 | Antonov              | 76.951 |\n",
      "| Beechcraft               | 78.032 | Boeing               | 60.441 | Bombardier Aerospace | 74.214 |\n",
      "| British Aerospace        | 75.323 | Canadair             | 74.969 | Cessna               | 74.728 |\n",
      "| Cirrus Aircraft          | 82.692 | Dassault Aviation    | 86.836 | Dornier              | 87.164 |\n",
      "| Douglas Aircraft Company | 62.186 | Embraer              | 59.343 | Eurofighter          | 71.394 |\n",
      "| Fairchild                | 74.530 | Fokker               | 43.301 | Gulfstream Aerospace | 79.860 |\n",
      "| Ilyushin                 | 68.498 | Lockheed Corporation | 65.445 | Lockheed Martin      | 77.929 |\n",
      "| McDonnell Douglas        | 44.556 | Panavia              | 70.749 | Piper                | 75.280 |\n",
      "| Robin                    | 79.269 | Saab                 | 82.877 | Supermarine          | 72.866 |\n",
      "| Tupolev                  | 84.031 | Yakovlev             | 86.848 | de Havilland         | 70.882 |\n",
      "\u001b[32m[04/22 00:27:12 d2.engine.defaults]: \u001b[0mEvaluation results for manu_id_val in csv format:\n",
      "\u001b[32m[04/22 00:27:12 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[04/22 00:27:12 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[04/22 00:27:12 d2.evaluation.testing]: \u001b[0mcopypaste: 72.5896,86.3294,84.9168,nan,0.0000,72.5952\n",
      "\u001b[32m[04/22 00:27:12 d2.utils.events]: \u001b[0m eta: 4:12:34  iter: 4999  total_loss: 0.04402  loss_cls: 0.01573  loss_box_reg: 0.02133  loss_rpn_cls: 0.0002377  loss_rpn_loc: 0.00469  time: 0.4324  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:27:21 d2.utils.events]: \u001b[0m eta: 4:12:25  iter: 5019  total_loss: 0.04838  loss_cls: 0.02242  loss_box_reg: 0.01912  loss_rpn_cls: 0.0002169  loss_rpn_loc: 0.005108  time: 0.4324  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:27:30 d2.utils.events]: \u001b[0m eta: 4:12:35  iter: 5039  total_loss: 0.05361  loss_cls: 0.0211  loss_box_reg: 0.02082  loss_rpn_cls: 0.0002928  loss_rpn_loc: 0.005955  time: 0.4324  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:27:38 d2.utils.events]: \u001b[0m eta: 4:12:59  iter: 5059  total_loss: 0.05242  loss_cls: 0.02441  loss_box_reg: 0.0222  loss_rpn_cls: 0.0004404  loss_rpn_loc: 0.005077  time: 0.4323  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:27:47 d2.utils.events]: \u001b[0m eta: 4:13:21  iter: 5079  total_loss: 0.04776  loss_cls: 0.01995  loss_box_reg: 0.02313  loss_rpn_cls: 0.0001696  loss_rpn_loc: 0.0047  time: 0.4323  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:27:55 d2.utils.events]: \u001b[0m eta: 4:13:19  iter: 5099  total_loss: 0.04429  loss_cls: 0.01776  loss_box_reg: 0.02133  loss_rpn_cls: 0.0001697  loss_rpn_loc: 0.004649  time: 0.4323  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:28:04 d2.utils.events]: \u001b[0m eta: 4:13:18  iter: 5119  total_loss: 0.04351  loss_cls: 0.01769  loss_box_reg: 0.02055  loss_rpn_cls: 0.0003588  loss_rpn_loc: 0.005576  time: 0.4323  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:28:12 d2.utils.events]: \u001b[0m eta: 4:13:13  iter: 5139  total_loss: 0.05283  loss_cls: 0.02108  loss_box_reg: 0.02243  loss_rpn_cls: 0.0003285  loss_rpn_loc: 0.005039  time: 0.4322  data_time: 0.0060  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:28:21 d2.utils.events]: \u001b[0m eta: 4:13:04  iter: 5159  total_loss: 0.04387  loss_cls: 0.01815  loss_box_reg: 0.0206  loss_rpn_cls: 0.0002817  loss_rpn_loc: 0.00622  time: 0.4323  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:28:29 d2.utils.events]: \u001b[0m eta: 4:12:46  iter: 5179  total_loss: 0.04246  loss_cls: 0.01485  loss_box_reg: 0.02208  loss_rpn_cls: 0.0002374  loss_rpn_loc: 0.004425  time: 0.4322  data_time: 0.0060  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:28:38 d2.utils.events]: \u001b[0m eta: 4:12:34  iter: 5199  total_loss: 0.04514  loss_cls: 0.0149  loss_box_reg: 0.02119  loss_rpn_cls: 0.0001907  loss_rpn_loc: 0.004542  time: 0.4321  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:28:46 d2.utils.events]: \u001b[0m eta: 4:12:20  iter: 5219  total_loss: 0.04483  loss_cls: 0.0161  loss_box_reg: 0.02093  loss_rpn_cls: 0.000511  loss_rpn_loc: 0.00586  time: 0.4321  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:28:55 d2.utils.events]: \u001b[0m eta: 4:11:56  iter: 5239  total_loss: 0.04083  loss_cls: 0.01509  loss_box_reg: 0.02094  loss_rpn_cls: 0.0002003  loss_rpn_loc: 0.004963  time: 0.4321  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:29:03 d2.utils.events]: \u001b[0m eta: 4:11:46  iter: 5259  total_loss: 0.04058  loss_cls: 0.01539  loss_box_reg: 0.01978  loss_rpn_cls: 0.0002775  loss_rpn_loc: 0.004901  time: 0.4321  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:29:12 d2.utils.events]: \u001b[0m eta: 4:11:38  iter: 5279  total_loss: 0.04229  loss_cls: 0.01415  loss_box_reg: 0.02025  loss_rpn_cls: 0.0002851  loss_rpn_loc: 0.006124  time: 0.4320  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/22 00:29:20 d2.utils.events]: \u001b[0m eta: 4:11:18  iter: 5299  total_loss: 0.03893  loss_cls: 0.01253  loss_box_reg: 0.01898  loss_rpn_cls: 0.0003951  loss_rpn_loc: 0.005084  time: 0.4320  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:29:29 d2.utils.events]: \u001b[0m eta: 4:11:04  iter: 5319  total_loss: 0.03931  loss_cls: 0.01356  loss_box_reg: 0.01809  loss_rpn_cls: 0.0001628  loss_rpn_loc: 0.005102  time: 0.4320  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:29:38 d2.utils.events]: \u001b[0m eta: 4:10:41  iter: 5339  total_loss: 0.04333  loss_cls: 0.01644  loss_box_reg: 0.02088  loss_rpn_cls: 0.0003589  loss_rpn_loc: 0.00508  time: 0.4320  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:29:46 d2.utils.events]: \u001b[0m eta: 4:10:16  iter: 5359  total_loss: 0.04323  loss_cls: 0.01471  loss_box_reg: 0.01962  loss_rpn_cls: 0.0002353  loss_rpn_loc: 0.004888  time: 0.4320  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:29:55 d2.utils.events]: \u001b[0m eta: 4:10:06  iter: 5379  total_loss: 0.03899  loss_cls: 0.01191  loss_box_reg: 0.02006  loss_rpn_cls: 0.0003192  loss_rpn_loc: 0.004399  time: 0.4321  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:30:04 d2.utils.events]: \u001b[0m eta: 4:09:59  iter: 5399  total_loss: 0.03808  loss_cls: 0.01553  loss_box_reg: 0.01783  loss_rpn_cls: 0.0001596  loss_rpn_loc: 0.004054  time: 0.4321  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:30:13 d2.utils.events]: \u001b[0m eta: 4:09:54  iter: 5419  total_loss: 0.03972  loss_cls: 0.01568  loss_box_reg: 0.02024  loss_rpn_cls: 0.0001433  loss_rpn_loc: 0.003933  time: 0.4321  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:30:21 d2.utils.events]: \u001b[0m eta: 4:09:10  iter: 5439  total_loss: 0.04284  loss_cls: 0.01468  loss_box_reg: 0.02267  loss_rpn_cls: 0.0002685  loss_rpn_loc: 0.005609  time: 0.4321  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:30:30 d2.utils.events]: \u001b[0m eta: 4:08:51  iter: 5459  total_loss: 0.04331  loss_cls: 0.01914  loss_box_reg: 0.01871  loss_rpn_cls: 0.0001951  loss_rpn_loc: 0.00488  time: 0.4322  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:30:40 d2.utils.events]: \u001b[0m eta: 4:08:45  iter: 5479  total_loss: 0.04284  loss_cls: 0.01656  loss_box_reg: 0.01823  loss_rpn_cls: 0.0003357  loss_rpn_loc: 0.005691  time: 0.4323  data_time: 0.0065  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:30:49 d2.utils.events]: \u001b[0m eta: 4:08:50  iter: 5499  total_loss: 0.03903  loss_cls: 0.01544  loss_box_reg: 0.01919  loss_rpn_cls: 0.0002538  loss_rpn_loc: 0.004813  time: 0.4324  data_time: 0.0065  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:30:58 d2.utils.events]: \u001b[0m eta: 4:09:07  iter: 5519  total_loss: 0.044  loss_cls: 0.01816  loss_box_reg: 0.0194  loss_rpn_cls: 0.000354  loss_rpn_loc: 0.005491  time: 0.4324  data_time: 0.0065  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:31:07 d2.utils.events]: \u001b[0m eta: 4:09:08  iter: 5539  total_loss: 0.03617  loss_cls: 0.01485  loss_box_reg: 0.01881  loss_rpn_cls: 9.448e-05  loss_rpn_loc: 0.004065  time: 0.4325  data_time: 0.0066  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:31:15 d2.utils.events]: \u001b[0m eta: 4:09:23  iter: 5559  total_loss: 0.03822  loss_cls: 0.01445  loss_box_reg: 0.01782  loss_rpn_cls: 0.0002544  loss_rpn_loc: 0.005586  time: 0.4325  data_time: 0.0066  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:31:24 d2.utils.events]: \u001b[0m eta: 4:09:27  iter: 5579  total_loss: 0.03714  loss_cls: 0.0128  loss_box_reg: 0.01838  loss_rpn_cls: 0.0002434  loss_rpn_loc: 0.004611  time: 0.4326  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:31:33 d2.utils.events]: \u001b[0m eta: 4:09:09  iter: 5599  total_loss: 0.04223  loss_cls: 0.01406  loss_box_reg: 0.0204  loss_rpn_cls: 0.0003637  loss_rpn_loc: 0.005196  time: 0.4325  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:31:41 d2.utils.events]: \u001b[0m eta: 4:08:24  iter: 5619  total_loss: 0.04197  loss_cls: 0.01417  loss_box_reg: 0.01981  loss_rpn_cls: 8.905e-05  loss_rpn_loc: 0.004934  time: 0.4324  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:31:50 d2.utils.events]: \u001b[0m eta: 4:08:19  iter: 5639  total_loss: 0.04482  loss_cls: 0.01737  loss_box_reg: 0.01918  loss_rpn_cls: 0.0001669  loss_rpn_loc: 0.00452  time: 0.4325  data_time: 0.0065  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:31:59 d2.utils.events]: \u001b[0m eta: 4:08:25  iter: 5659  total_loss: 0.04521  loss_cls: 0.01461  loss_box_reg: 0.02243  loss_rpn_cls: 0.0003448  loss_rpn_loc: 0.005596  time: 0.4325  data_time: 0.0066  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:32:07 d2.utils.events]: \u001b[0m eta: 4:08:16  iter: 5679  total_loss: 0.03678  loss_cls: 0.01294  loss_box_reg: 0.01905  loss_rpn_cls: 0.0001883  loss_rpn_loc: 0.005534  time: 0.4325  data_time: 0.0060  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:32:16 d2.utils.events]: \u001b[0m eta: 4:08:16  iter: 5699  total_loss: 0.03522  loss_cls: 0.01333  loss_box_reg: 0.01679  loss_rpn_cls: 0.0002898  loss_rpn_loc: 0.004807  time: 0.4325  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:32:25 d2.utils.events]: \u001b[0m eta: 4:08:07  iter: 5719  total_loss: 0.03845  loss_cls: 0.01253  loss_box_reg: 0.01981  loss_rpn_cls: 0.0003889  loss_rpn_loc: 0.005777  time: 0.4325  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:32:33 d2.utils.events]: \u001b[0m eta: 4:07:32  iter: 5739  total_loss: 0.04357  loss_cls: 0.01505  loss_box_reg: 0.01874  loss_rpn_cls: 0.0002182  loss_rpn_loc: 0.005047  time: 0.4325  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:32:42 d2.utils.events]: \u001b[0m eta: 4:07:05  iter: 5759  total_loss: 0.03852  loss_cls: 0.01292  loss_box_reg: 0.01834  loss_rpn_cls: 0.0002365  loss_rpn_loc: 0.004402  time: 0.4325  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:32:50 d2.utils.events]: \u001b[0m eta: 4:06:48  iter: 5779  total_loss: 0.03882  loss_cls: 0.01543  loss_box_reg: 0.01785  loss_rpn_cls: 0.0002788  loss_rpn_loc: 0.004804  time: 0.4325  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:32:59 d2.utils.events]: \u001b[0m eta: 4:06:24  iter: 5799  total_loss: 0.03565  loss_cls: 0.01222  loss_box_reg: 0.01826  loss_rpn_cls: 0.0003152  loss_rpn_loc: 0.004705  time: 0.4325  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:33:08 d2.utils.events]: \u001b[0m eta: 4:06:12  iter: 5819  total_loss: 0.03558  loss_cls: 0.0137  loss_box_reg: 0.0182  loss_rpn_cls: 0.0001211  loss_rpn_loc: 0.0043  time: 0.4325  data_time: 0.0060  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:33:16 d2.utils.events]: \u001b[0m eta: 4:05:45  iter: 5839  total_loss: 0.03804  loss_cls: 0.01348  loss_box_reg: 0.0202  loss_rpn_cls: 0.0001133  loss_rpn_loc: 0.003981  time: 0.4324  data_time: 0.0069  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:33:24 d2.utils.events]: \u001b[0m eta: 4:05:31  iter: 5859  total_loss: 0.03905  loss_cls: 0.01475  loss_box_reg: 0.01964  loss_rpn_cls: 0.0001263  loss_rpn_loc: 0.004254  time: 0.4323  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:33:33 d2.utils.events]: \u001b[0m eta: 4:05:06  iter: 5879  total_loss: 0.04236  loss_cls: 0.01427  loss_box_reg: 0.02156  loss_rpn_cls: 0.0001718  loss_rpn_loc: 0.00494  time: 0.4323  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:33:42 d2.utils.events]: \u001b[0m eta: 4:05:14  iter: 5899  total_loss: 0.03981  loss_cls: 0.01495  loss_box_reg: 0.0204  loss_rpn_cls: 0.0001412  loss_rpn_loc: 0.004887  time: 0.4323  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:33:51 d2.utils.events]: \u001b[0m eta: 4:05:19  iter: 5919  total_loss: 0.03733  loss_cls: 0.01249  loss_box_reg: 0.0189  loss_rpn_cls: 0.00016  loss_rpn_loc: 0.004118  time: 0.4324  data_time: 0.0067  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:34:00 d2.utils.events]: \u001b[0m eta: 4:05:26  iter: 5939  total_loss: 0.03971  loss_cls: 0.01391  loss_box_reg: 0.01798  loss_rpn_cls: 0.0002205  loss_rpn_loc: 0.00518  time: 0.4325  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:34:09 d2.utils.events]: \u001b[0m eta: 4:05:30  iter: 5959  total_loss: 0.03814  loss_cls: 0.01359  loss_box_reg: 0.01931  loss_rpn_cls: 0.0001449  loss_rpn_loc: 0.004712  time: 0.4325  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:34:17 d2.utils.events]: \u001b[0m eta: 4:05:16  iter: 5979  total_loss: 0.0403  loss_cls: 0.01626  loss_box_reg: 0.01854  loss_rpn_cls: 0.0001543  loss_rpn_loc: 0.004219  time: 0.4325  data_time: 0.0060  lr: 0.02  max_mem: 6293M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[5m\u001b[31mWARNING\u001b[0m \u001b[32m[04/22 00:34:26 d2.data.datasets.coco]: \u001b[0m\n",
      "Category ids in annotations are not in [1, #categories]! We'll apply a mapping for you.\n",
      "\n",
      "\u001b[32m[04/22 00:34:26 d2.data.datasets.coco]: \u001b[0mLoaded 3333 images in COCO format from /home/zack/datasets/manufacturer_identification/data/coco/val.json\n",
      "\u001b[32m[04/22 00:34:26 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in inference: [ResizeShortestEdge(short_edge_length=(800, 800), max_size=1333, sample_style='choice')]\n",
      "\u001b[32m[04/22 00:34:26 d2.data.common]: \u001b[0mSerializing 3333 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[04/22 00:34:26 d2.data.common]: \u001b[0mSerialized dataset takes 0.85 MiB\n",
      "\u001b[32m[04/22 00:34:26 d2.evaluation.evaluator]: \u001b[0mStart inference on 3333 batches\n",
      "\u001b[32m[04/22 00:34:27 d2.evaluation.evaluator]: \u001b[0mInference done 11/3333. Dataloading: 0.0006 s/iter. Inference: 0.0303 s/iter. Eval: 0.0001 s/iter. Total: 0.0311 s/iter. ETA=0:01:43\n",
      "\u001b[32m[04/22 00:34:32 d2.evaluation.evaluator]: \u001b[0mInference done 164/3333. Dataloading: 0.0008 s/iter. Inference: 0.0317 s/iter. Eval: 0.0001 s/iter. Total: 0.0327 s/iter. ETA=0:01:43\n",
      "\u001b[32m[04/22 00:34:37 d2.evaluation.evaluator]: \u001b[0mInference done 302/3333. Dataloading: 0.0008 s/iter. Inference: 0.0335 s/iter. Eval: 0.0001 s/iter. Total: 0.0344 s/iter. ETA=0:01:44\n",
      "\u001b[32m[04/22 00:34:42 d2.evaluation.evaluator]: \u001b[0mInference done 460/3333. Dataloading: 0.0008 s/iter. Inference: 0.0325 s/iter. Eval: 0.0001 s/iter. Total: 0.0335 s/iter. ETA=0:01:36\n",
      "\u001b[32m[04/22 00:34:47 d2.evaluation.evaluator]: \u001b[0mInference done 614/3333. Dataloading: 0.0008 s/iter. Inference: 0.0323 s/iter. Eval: 0.0001 s/iter. Total: 0.0332 s/iter. ETA=0:01:30\n",
      "\u001b[32m[04/22 00:34:52 d2.evaluation.evaluator]: \u001b[0mInference done 761/3333. Dataloading: 0.0008 s/iter. Inference: 0.0324 s/iter. Eval: 0.0001 s/iter. Total: 0.0334 s/iter. ETA=0:01:25\n",
      "\u001b[32m[04/22 00:34:57 d2.evaluation.evaluator]: \u001b[0mInference done 903/3333. Dataloading: 0.0008 s/iter. Inference: 0.0328 s/iter. Eval: 0.0001 s/iter. Total: 0.0337 s/iter. ETA=0:01:21\n",
      "\u001b[32m[04/22 00:35:02 d2.evaluation.evaluator]: \u001b[0mInference done 1049/3333. Dataloading: 0.0008 s/iter. Inference: 0.0328 s/iter. Eval: 0.0001 s/iter. Total: 0.0338 s/iter. ETA=0:01:17\n",
      "\u001b[32m[04/22 00:35:07 d2.evaluation.evaluator]: \u001b[0mInference done 1195/3333. Dataloading: 0.0008 s/iter. Inference: 0.0329 s/iter. Eval: 0.0001 s/iter. Total: 0.0339 s/iter. ETA=0:01:12\n",
      "\u001b[32m[04/22 00:35:12 d2.evaluation.evaluator]: \u001b[0mInference done 1351/3333. Dataloading: 0.0008 s/iter. Inference: 0.0327 s/iter. Eval: 0.0001 s/iter. Total: 0.0337 s/iter. ETA=0:01:06\n",
      "\u001b[32m[04/22 00:35:17 d2.evaluation.evaluator]: \u001b[0mInference done 1512/3333. Dataloading: 0.0008 s/iter. Inference: 0.0324 s/iter. Eval: 0.0001 s/iter. Total: 0.0334 s/iter. ETA=0:01:00\n",
      "\u001b[32m[04/22 00:35:22 d2.evaluation.evaluator]: \u001b[0mInference done 1648/3333. Dataloading: 0.0008 s/iter. Inference: 0.0327 s/iter. Eval: 0.0001 s/iter. Total: 0.0337 s/iter. ETA=0:00:56\n",
      "\u001b[32m[04/22 00:35:27 d2.evaluation.evaluator]: \u001b[0mInference done 1804/3333. Dataloading: 0.0008 s/iter. Inference: 0.0326 s/iter. Eval: 0.0001 s/iter. Total: 0.0336 s/iter. ETA=0:00:51\n",
      "\u001b[32m[04/22 00:35:32 d2.evaluation.evaluator]: \u001b[0mInference done 1956/3333. Dataloading: 0.0008 s/iter. Inference: 0.0326 s/iter. Eval: 0.0001 s/iter. Total: 0.0335 s/iter. ETA=0:00:46\n",
      "\u001b[32m[04/22 00:35:37 d2.evaluation.evaluator]: \u001b[0mInference done 2107/3333. Dataloading: 0.0008 s/iter. Inference: 0.0325 s/iter. Eval: 0.0001 s/iter. Total: 0.0335 s/iter. ETA=0:00:41\n",
      "\u001b[32m[04/22 00:35:42 d2.evaluation.evaluator]: \u001b[0mInference done 2260/3333. Dataloading: 0.0008 s/iter. Inference: 0.0325 s/iter. Eval: 0.0001 s/iter. Total: 0.0334 s/iter. ETA=0:00:35\n",
      "\u001b[32m[04/22 00:35:47 d2.evaluation.evaluator]: \u001b[0mInference done 2413/3333. Dataloading: 0.0008 s/iter. Inference: 0.0324 s/iter. Eval: 0.0001 s/iter. Total: 0.0334 s/iter. ETA=0:00:30\n",
      "\u001b[32m[04/22 00:35:52 d2.evaluation.evaluator]: \u001b[0mInference done 2562/3333. Dataloading: 0.0008 s/iter. Inference: 0.0325 s/iter. Eval: 0.0001 s/iter. Total: 0.0334 s/iter. ETA=0:00:25\n",
      "\u001b[32m[04/22 00:35:57 d2.evaluation.evaluator]: \u001b[0mInference done 2718/3333. Dataloading: 0.0008 s/iter. Inference: 0.0324 s/iter. Eval: 0.0001 s/iter. Total: 0.0334 s/iter. ETA=0:00:20\n",
      "\u001b[32m[04/22 00:36:02 d2.evaluation.evaluator]: \u001b[0mInference done 2876/3333. Dataloading: 0.0008 s/iter. Inference: 0.0323 s/iter. Eval: 0.0001 s/iter. Total: 0.0333 s/iter. ETA=0:00:15\n",
      "\u001b[32m[04/22 00:36:07 d2.evaluation.evaluator]: \u001b[0mInference done 3008/3333. Dataloading: 0.0008 s/iter. Inference: 0.0325 s/iter. Eval: 0.0001 s/iter. Total: 0.0335 s/iter. ETA=0:00:10\n",
      "\u001b[32m[04/22 00:36:12 d2.evaluation.evaluator]: \u001b[0mInference done 3145/3333. Dataloading: 0.0008 s/iter. Inference: 0.0326 s/iter. Eval: 0.0001 s/iter. Total: 0.0336 s/iter. ETA=0:00:06\n",
      "\u001b[32m[04/22 00:36:17 d2.evaluation.evaluator]: \u001b[0mInference done 3270/3333. Dataloading: 0.0008 s/iter. Inference: 0.0329 s/iter. Eval: 0.0001 s/iter. Total: 0.0339 s/iter. ETA=0:00:02\n",
      "\u001b[32m[04/22 00:36:20 d2.evaluation.evaluator]: \u001b[0mTotal inference time: 0:01:53.240940 (0.034027 s / iter per device, on 1 devices)\n",
      "\u001b[32m[04/22 00:36:20 d2.evaluation.evaluator]: \u001b[0mTotal inference pure compute time: 0:01:49 (0.033018 s / iter per device, on 1 devices)\n",
      "\u001b[32m[04/22 00:36:20 d2.evaluation.coco_evaluation]: \u001b[0mPreparing results for COCO format ...\n",
      "\u001b[32m[04/22 00:36:20 d2.evaluation.coco_evaluation]: \u001b[0mSaving results to ./output_class_balanced/inference/coco_instances_results.json\n",
      "\u001b[32m[04/22 00:36:20 d2.evaluation.coco_evaluation]: \u001b[0mEvaluating predictions with unofficial COCO API...\n",
      "Loading and preparing results...\n",
      "DONE (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "\u001b[32m[04/22 00:36:20 d2.evaluation.fast_eval_api]: \u001b[0mEvaluate annotation type *bbox*\n",
      "\u001b[32m[04/22 00:36:21 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.evaluate() finished in 1.01 seconds.\n",
      "\u001b[32m[04/22 00:36:21 d2.evaluation.fast_eval_api]: \u001b[0mAccumulating evaluation results...\n",
      "\u001b[32m[04/22 00:36:21 d2.evaluation.fast_eval_api]: \u001b[0mCOCOeval_opt.accumulate() finished in 0.14 seconds.\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.743\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.899\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.876\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.743\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.828\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.829\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.829\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.829\n",
      "\u001b[32m[04/22 00:36:21 d2.evaluation.coco_evaluation]: \u001b[0mEvaluation results for bbox: \n",
      "|   AP   |  AP50  |  AP75  |  APs  |  APm  |  APl   |\n",
      "|:------:|:------:|:------:|:-----:|:-----:|:------:|\n",
      "| 74.291 | 89.892 | 87.647 |  nan  | 0.000 | 74.300 |\n",
      "\u001b[32m[04/22 00:36:21 d2.evaluation.coco_evaluation]: \u001b[0mSome metrics cannot be computed and is shown as NaN.\n",
      "\u001b[32m[04/22 00:36:21 d2.evaluation.coco_evaluation]: \u001b[0mPer-category bbox AP: \n",
      "| category                 | AP     | category             | AP     | category             | AP     |\n",
      "|:-------------------------|:-------|:---------------------|:-------|:---------------------|:-------|\n",
      "| ATR                      | 88.976 | Airbus               | 60.795 | Antonov              | 85.406 |\n",
      "| Beechcraft               | 85.834 | Boeing               | 65.155 | Bombardier Aerospace | 78.326 |\n",
      "| British Aerospace        | 79.742 | Canadair             | 75.014 | Cessna               | 73.115 |\n",
      "| Cirrus Aircraft          | 79.334 | Dassault Aviation    | 79.341 | Dornier              | 84.727 |\n",
      "| Douglas Aircraft Company | 60.456 | Embraer              | 80.431 | Eurofighter          | 57.943 |\n",
      "| Fairchild                | 81.378 | Fokker               | 67.691 | Gulfstream Aerospace | 73.456 |\n",
      "| Ilyushin                 | 71.784 | Lockheed Corporation | 61.383 | Lockheed Martin      | 69.796 |\n",
      "| McDonnell Douglas        | 61.809 | Panavia              | 71.101 | Piper                | 75.406 |\n",
      "| Robin                    | 76.387 | Saab                 | 74.401 | Supermarine          | 66.014 |\n",
      "| Tupolev                  | 88.040 | Yakovlev             | 84.886 | de Havilland         | 70.599 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/22 00:36:21 d2.engine.defaults]: \u001b[0mEvaluation results for manu_id_val in csv format:\n",
      "\u001b[32m[04/22 00:36:21 d2.evaluation.testing]: \u001b[0mcopypaste: Task: bbox\n",
      "\u001b[32m[04/22 00:36:21 d2.evaluation.testing]: \u001b[0mcopypaste: AP,AP50,AP75,APs,APm,APl\n",
      "\u001b[32m[04/22 00:36:21 d2.evaluation.testing]: \u001b[0mcopypaste: 74.2909,89.8920,87.6466,nan,0.0000,74.3002\n",
      "\u001b[32m[04/22 00:36:21 d2.utils.events]: \u001b[0m eta: 4:05:06  iter: 5999  total_loss: 0.03806  loss_cls: 0.01483  loss_box_reg: 0.01774  loss_rpn_cls: 0.0001935  loss_rpn_loc: 0.004904  time: 0.4324  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:36:30 d2.utils.events]: \u001b[0m eta: 4:05:04  iter: 6019  total_loss: 0.03803  loss_cls: 0.01206  loss_box_reg: 0.0179  loss_rpn_cls: 0.0002233  loss_rpn_loc: 0.00436  time: 0.4325  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:36:40 d2.utils.events]: \u001b[0m eta: 4:04:58  iter: 6039  total_loss: 0.03877  loss_cls: 0.01209  loss_box_reg: 0.01867  loss_rpn_cls: 0.0001689  loss_rpn_loc: 0.004394  time: 0.4327  data_time: 0.0066  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:36:49 d2.utils.events]: \u001b[0m eta: 4:04:52  iter: 6059  total_loss: 0.03877  loss_cls: 0.01241  loss_box_reg: 0.01744  loss_rpn_cls: 0.0003985  loss_rpn_loc: 0.006748  time: 0.4328  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:36:58 d2.utils.events]: \u001b[0m eta: 4:04:31  iter: 6079  total_loss: 0.03417  loss_cls: 0.01357  loss_box_reg: 0.01646  loss_rpn_cls: 0.0001568  loss_rpn_loc: 0.004724  time: 0.4327  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:37:07 d2.utils.events]: \u001b[0m eta: 4:04:25  iter: 6099  total_loss: 0.03583  loss_cls: 0.01305  loss_box_reg: 0.01703  loss_rpn_cls: 0.0002627  loss_rpn_loc: 0.004602  time: 0.4328  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:37:15 d2.utils.events]: \u001b[0m eta: 4:04:16  iter: 6119  total_loss: 0.04217  loss_cls: 0.01499  loss_box_reg: 0.01968  loss_rpn_cls: 0.0002309  loss_rpn_loc: 0.005965  time: 0.4327  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:37:24 d2.utils.events]: \u001b[0m eta: 4:04:12  iter: 6139  total_loss: 0.03744  loss_cls: 0.01332  loss_box_reg: 0.01887  loss_rpn_cls: 0.0001722  loss_rpn_loc: 0.004527  time: 0.4328  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:37:33 d2.utils.events]: \u001b[0m eta: 4:04:05  iter: 6159  total_loss: 0.03869  loss_cls: 0.01379  loss_box_reg: 0.01923  loss_rpn_cls: 0.0002736  loss_rpn_loc: 0.004987  time: 0.4328  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:37:41 d2.utils.events]: \u001b[0m eta: 4:04:01  iter: 6179  total_loss: 0.04017  loss_cls: 0.01609  loss_box_reg: 0.02087  loss_rpn_cls: 0.0002448  loss_rpn_loc: 0.004518  time: 0.4327  data_time: 0.0067  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:37:49 d2.utils.events]: \u001b[0m eta: 4:03:41  iter: 6199  total_loss: 0.04637  loss_cls: 0.02044  loss_box_reg: 0.02148  loss_rpn_cls: 0.0001371  loss_rpn_loc: 0.004782  time: 0.4326  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:37:57 d2.utils.events]: \u001b[0m eta: 4:03:08  iter: 6219  total_loss: 0.03751  loss_cls: 0.01356  loss_box_reg: 0.01864  loss_rpn_cls: 0.0001213  loss_rpn_loc: 0.004662  time: 0.4325  data_time: 0.0059  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:38:06 d2.utils.events]: \u001b[0m eta: 4:02:55  iter: 6239  total_loss: 0.04081  loss_cls: 0.01486  loss_box_reg: 0.02056  loss_rpn_cls: 9.881e-05  loss_rpn_loc: 0.004854  time: 0.4326  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:38:17 d2.utils.events]: \u001b[0m eta: 4:03:06  iter: 6259  total_loss: 0.03974  loss_cls: 0.0148  loss_box_reg: 0.01933  loss_rpn_cls: 0.0001807  loss_rpn_loc: 0.004179  time: 0.4329  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:38:25 d2.utils.events]: \u001b[0m eta: 4:02:46  iter: 6279  total_loss: 0.03683  loss_cls: 0.01254  loss_box_reg: 0.01967  loss_rpn_cls: 0.0003273  loss_rpn_loc: 0.004297  time: 0.4329  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:38:33 d2.utils.events]: \u001b[0m eta: 4:02:23  iter: 6299  total_loss: 0.03764  loss_cls: 0.01335  loss_box_reg: 0.01832  loss_rpn_cls: 0.0001578  loss_rpn_loc: 0.005087  time: 0.4328  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:38:42 d2.utils.events]: \u001b[0m eta: 4:01:51  iter: 6319  total_loss: 0.03986  loss_cls: 0.01578  loss_box_reg: 0.01906  loss_rpn_cls: 0.0004089  loss_rpn_loc: 0.005193  time: 0.4327  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:38:50 d2.utils.events]: \u001b[0m eta: 4:01:18  iter: 6339  total_loss: 0.04179  loss_cls: 0.01558  loss_box_reg: 0.02011  loss_rpn_cls: 0.0001175  loss_rpn_loc: 0.004692  time: 0.4327  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:38:59 d2.utils.events]: \u001b[0m eta: 4:01:05  iter: 6359  total_loss: 0.03857  loss_cls: 0.01329  loss_box_reg: 0.0197  loss_rpn_cls: 0.0001731  loss_rpn_loc: 0.004445  time: 0.4327  data_time: 0.0066  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:39:07 d2.utils.events]: \u001b[0m eta: 4:00:32  iter: 6379  total_loss: 0.03932  loss_cls: 0.013  loss_box_reg: 0.0199  loss_rpn_cls: 0.0001921  loss_rpn_loc: 0.0059  time: 0.4326  data_time: 0.0060  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:39:15 d2.utils.events]: \u001b[0m eta: 3:59:54  iter: 6399  total_loss: 0.03652  loss_cls: 0.01415  loss_box_reg: 0.01914  loss_rpn_cls: 0.0001553  loss_rpn_loc: 0.005112  time: 0.4326  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:39:24 d2.utils.events]: \u001b[0m eta: 3:59:23  iter: 6419  total_loss: 0.03631  loss_cls: 0.01356  loss_box_reg: 0.01844  loss_rpn_cls: 0.0001768  loss_rpn_loc: 0.006491  time: 0.4326  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:39:32 d2.utils.events]: \u001b[0m eta: 3:58:54  iter: 6439  total_loss: 0.04393  loss_cls: 0.01448  loss_box_reg: 0.02416  loss_rpn_cls: 0.0003062  loss_rpn_loc: 0.004321  time: 0.4325  data_time: 0.0059  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:39:40 d2.utils.events]: \u001b[0m eta: 3:58:27  iter: 6459  total_loss: 0.03459  loss_cls: 0.01243  loss_box_reg: 0.01774  loss_rpn_cls: 0.0003931  loss_rpn_loc: 0.005134  time: 0.4324  data_time: 0.0070  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:39:49 d2.utils.events]: \u001b[0m eta: 3:57:52  iter: 6479  total_loss: 0.04129  loss_cls: 0.01477  loss_box_reg: 0.01982  loss_rpn_cls: 0.0001531  loss_rpn_loc: 0.005106  time: 0.4324  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:39:57 d2.utils.events]: \u001b[0m eta: 3:57:07  iter: 6499  total_loss: 0.03931  loss_cls: 0.01348  loss_box_reg: 0.01923  loss_rpn_cls: 0.000181  loss_rpn_loc: 0.004451  time: 0.4323  data_time: 0.0060  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:40:05 d2.utils.events]: \u001b[0m eta: 3:56:39  iter: 6519  total_loss: 0.0426  loss_cls: 0.01627  loss_box_reg: 0.0204  loss_rpn_cls: 0.0001932  loss_rpn_loc: 0.004968  time: 0.4323  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:40:14 d2.utils.events]: \u001b[0m eta: 3:56:18  iter: 6539  total_loss: 0.03765  loss_cls: 0.01558  loss_box_reg: 0.01671  loss_rpn_cls: 0.0001769  loss_rpn_loc: 0.005629  time: 0.4322  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:40:22 d2.utils.events]: \u001b[0m eta: 3:55:56  iter: 6559  total_loss: 0.0361  loss_cls: 0.01531  loss_box_reg: 0.01821  loss_rpn_cls: 0.0001288  loss_rpn_loc: 0.003649  time: 0.4321  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:40:30 d2.utils.events]: \u001b[0m eta: 3:55:18  iter: 6579  total_loss: 0.03976  loss_cls: 0.01266  loss_box_reg: 0.01931  loss_rpn_cls: 0.0002593  loss_rpn_loc: 0.004553  time: 0.4321  data_time: 0.0060  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:40:38 d2.utils.events]: \u001b[0m eta: 3:55:11  iter: 6599  total_loss: 0.03364  loss_cls: 0.01091  loss_box_reg: 0.01747  loss_rpn_cls: 0.0001434  loss_rpn_loc: 0.004196  time: 0.4320  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:40:46 d2.utils.events]: \u001b[0m eta: 3:55:15  iter: 6619  total_loss: 0.03346  loss_cls: 0.01194  loss_box_reg: 0.01671  loss_rpn_cls: 0.000124  loss_rpn_loc: 0.004087  time: 0.4319  data_time: 0.0061  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:40:54 d2.utils.events]: \u001b[0m eta: 3:54:44  iter: 6639  total_loss: 0.03236  loss_cls: 0.01204  loss_box_reg: 0.01552  loss_rpn_cls: 0.0001193  loss_rpn_loc: 0.003664  time: 0.4318  data_time: 0.0060  lr: 0.02  max_mem: 6293M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[04/22 00:41:03 d2.utils.events]: \u001b[0m eta: 3:54:23  iter: 6659  total_loss: 0.03622  loss_cls: 0.01208  loss_box_reg: 0.01769  loss_rpn_cls: 0.00023  loss_rpn_loc: 0.005556  time: 0.4318  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:41:11 d2.utils.events]: \u001b[0m eta: 3:54:10  iter: 6679  total_loss: 0.03272  loss_cls: 0.01128  loss_box_reg: 0.01609  loss_rpn_cls: 0.0001373  loss_rpn_loc: 0.005361  time: 0.4317  data_time: 0.0064  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:41:19 d2.utils.events]: \u001b[0m eta: 3:53:57  iter: 6699  total_loss: 0.0355  loss_cls: 0.01132  loss_box_reg: 0.01646  loss_rpn_cls: 0.0003391  loss_rpn_loc: 0.006676  time: 0.4317  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:41:28 d2.utils.events]: \u001b[0m eta: 3:53:29  iter: 6719  total_loss: 0.03334  loss_cls: 0.01007  loss_box_reg: 0.01657  loss_rpn_cls: 0.0002427  loss_rpn_loc: 0.004166  time: 0.4317  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:41:37 d2.utils.events]: \u001b[0m eta: 3:53:41  iter: 6739  total_loss: 0.04053  loss_cls: 0.01469  loss_box_reg: 0.01875  loss_rpn_cls: 0.0002004  loss_rpn_loc: 0.004676  time: 0.4318  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:41:46 d2.utils.events]: \u001b[0m eta: 3:53:42  iter: 6759  total_loss: 0.04454  loss_cls: 0.01507  loss_box_reg: 0.02172  loss_rpn_cls: 0.0002272  loss_rpn_loc: 0.006369  time: 0.4318  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:41:54 d2.utils.events]: \u001b[0m eta: 3:53:13  iter: 6779  total_loss: 0.04473  loss_cls: 0.01772  loss_box_reg: 0.02073  loss_rpn_cls: 0.0001961  loss_rpn_loc: 0.004741  time: 0.4317  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:42:02 d2.utils.events]: \u001b[0m eta: 3:52:41  iter: 6799  total_loss: 0.03913  loss_cls: 0.0127  loss_box_reg: 0.01974  loss_rpn_cls: 0.000243  loss_rpn_loc: 0.005571  time: 0.4317  data_time: 0.0063  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:42:11 d2.utils.events]: \u001b[0m eta: 3:52:24  iter: 6819  total_loss: 0.03742  loss_cls: 0.01137  loss_box_reg: 0.01856  loss_rpn_cls: 0.0002407  loss_rpn_loc: 0.004223  time: 0.4316  data_time: 0.0062  lr: 0.02  max_mem: 6293M\n",
      "\u001b[32m[04/22 00:42:19 d2.utils.events]: \u001b[0m eta: 3:52:19  iter: 6839  total_loss: 0.03982  loss_cls: 0.01469  loss_box_reg: 0.02018  loss_rpn_cls: 0.0002519  loss_rpn_loc: 0.004876  time: 0.4315  data_time: 0.0065  lr: 0.02  max_mem: 6293M\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "If you'd like to do anything fancier than the standard training logic,\n",
    "consider writing your own training loop (see plain_train_net.py) or\n",
    "subclassing the trainer.\n",
    "\"\"\"\n",
    "trainer = Trainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "if cfg.TEST.AUG.ENABLED:\n",
    "    trainer.register_hooks(\n",
    "        [hooks.EvalHook(0, lambda: trainer.test_with_TTA(cfg, trainer.model))]\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Training Discussion and Testing\n",
    "\n",
    "The model appears to have converged which is a good sign, and AP looks reasonable at 90.716 at final validation. AP is Average Precision (out of all the calls we made, how many were actually our target) and is evaluated over a sweeping range of Intersection-over-Union thresholds from 0.5 -> 0.95 at 0.05 increments. This gives us a pretty good idea that our boxes and classifications are fairly accurate.  For real-world imagery (specifically overhead imagery), we are often more concerned with AP50 (Average Precision at IoU=0.5) since it's difficult to get humans to produce perfect annotations, and we reach an astounding 96.178 AP50 here. However, AP50 reaching this high metric is no surprise considering how easy this data really is.\n",
    "\n",
    "Now for the fun part- testing of our model. Lets load up the test set and evaluate model performance at the last iteration of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cfg.DATASETS.TEST = ('manu_id_test',)\n",
    "image_root = '/home/zack/datasets/manufacturer_identification/data/images'\n",
    "register_coco_instances('manu_id_test', {}, image_root=image_root, json_file='/home/zack/datasets/manufacturer_identification/data/coco/test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.test(cfg=cfg, model=trainer.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Testing Discussion - Class Balanced\n",
    "\n",
    "The results are better than the validation dataset- Excellent! However, I believe we can do better. Taking the model at the last iteration is likely using an overfitted model. Let's load the best model (according to the validation dataset) and evaluate its performance. Assuming there is no covariate shift between the val/test sets, we should se better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with open('output_class_balanced/metrics.json', 'r') as f:\n",
    "    metrics = [eval(x.replace('\\n', '').replace('NaN', '0')) for x in f.readlines()]\n",
    "metrics = {x['iteration'] : x for i, x in enumerate(metrics) if 'bbox/AP' in x}\n",
    "bbox_ap = {k : v['bbox/AP'] for k, v in metrics.items()}\n",
    "best_ap_iter = max(bbox_ap, key= lambda x: bbox_ap[x])\n",
    "print(f'Best bbox/AP was achieved at iteration {best_ap_iter} with a value of {bbox_ap[best_ap_iter]}')\n",
    "print(f\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from detectron2.modeling import build_model\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "\n",
    "model = build_model(cfg)\n",
    "DetectionCheckpointer(model).load(f'output_class_balanced/model_{best_ap_iter:07}.pth')\n",
    "\n",
    "trainer.test(cfg=cfg, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Visualize the results\n",
    "\n",
    "Lets load up a simple test image and verify our model can predict correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data import transforms as T\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "\n",
    "class DefaultPredictor:\n",
    "    \"\"\"\n",
    "    Create a simple end-to-end predictor with the given config that runs on\n",
    "    single device for a single input image.\n",
    "    Compared to using the model directly, this class does the following additions:\n",
    "    1. Load checkpoint from `cfg.MODEL.WEIGHTS`.\n",
    "    2. Always take BGR image as the input and apply conversion defined by `cfg.INPUT.FORMAT`.\n",
    "    3. Apply resizing defined by `cfg.INPUT.{MIN,MAX}_SIZE_TEST`.\n",
    "    4. Take one input image and produce a single output, instead of a batch.\n",
    "    This is meant for simple demo purposes, so it does the above steps automatically.\n",
    "    This is not meant for benchmarks or running complicated inference logic.\n",
    "    If you'd like to do anything more complicated, please refer to its source code as\n",
    "    examples to build and use the model manually.\n",
    "    Attributes:\n",
    "        metadata (Metadata): the metadata of the underlying dataset, obtained from\n",
    "            cfg.DATASETS.TEST.\n",
    "    Examples:\n",
    "    ::\n",
    "        pred = DefaultPredictor(cfg)\n",
    "        inputs = cv2.imread(\"input.jpg\")\n",
    "        outputs = pred(inputs)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, cfg, model):\n",
    "        self.cfg = cfg.clone()  # cfg can be modified by model\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        if len(cfg.DATASETS.TEST):\n",
    "            self.metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n",
    "\n",
    "        self.aug = T.ResizeShortestEdge(\n",
    "            [cfg.INPUT.MIN_SIZE_TEST, cfg.INPUT.MIN_SIZE_TEST], cfg.INPUT.MAX_SIZE_TEST\n",
    "        )\n",
    "\n",
    "        self.input_format = cfg.INPUT.FORMAT\n",
    "        assert self.input_format in [\"RGB\", \"BGR\"], self.input_format\n",
    "\n",
    "    def __call__(self, original_image):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            original_image (np.ndarray): an image of shape (H, W, C) (in BGR order).\n",
    "        Returns:\n",
    "            predictions (dict):\n",
    "                the output of the model for one image only.\n",
    "                See :doc:`/tutorials/models` for details about the format.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():  # https://github.com/sphinx-doc/sphinx/issues/4258\n",
    "            # Apply pre-processing to image.\n",
    "            if self.input_format == \"RGB\":\n",
    "                # whether the model expects BGR inputs or RGB\n",
    "                original_image = original_image[:, :, ::-1]\n",
    "            height, width = original_image.shape[:2]\n",
    "            image = self.aug.get_transform(original_image).apply_image(original_image)\n",
    "            image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n",
    "\n",
    "            inputs = {\"image\": image, \"height\": height, \"width\": width}\n",
    "            predictions = self.model([inputs])[0]\n",
    "            return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/zack/datasets/manufacturer_identification/data/coco/test.json', 'r') as f:\n",
    "    test_data = json.load(f)\n",
    "    random_idx = np.random.choice(len(test_data['images']), 1)[0]\n",
    "    test_image = test_data['images'][random_idx]\n",
    "    test_image_file = os.path.join('/home/zack/datasets/manufacturer_identification/data/images', test_image['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.imread(test_image_file)\n",
    "print(im.shape)\n",
    "plt.figure(figsize=(15,7.5))\n",
    "plt.imshow(im[..., ::-1])   \n",
    "\n",
    "predictor = DefaultPredictor(cfg, model)\n",
    "\n",
    "outputs = predictor(im[..., ::-1])\n",
    "        \n",
    "        \n",
    "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TEST[0]), scale=1.5)\n",
    "out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(out.get_image()[..., ::-1][..., ::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
